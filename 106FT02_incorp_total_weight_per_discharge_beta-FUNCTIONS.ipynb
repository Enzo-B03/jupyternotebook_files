{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT THE LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exc1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from dateutil.parser import parse\n",
    "from itertools import compress\n",
    "from collections import OrderedDict\n",
    "from datetime import date\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import re\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD THE PI DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sheet From Pi\n",
    "#Step1: Load the Data for the month as a dataframe in pandas\n",
    "df_JAN2019Pi = pd.read_excel(r'C:\\Users\\v.t.flores\\Documents\\Pressure Filter Inv_2020.xlsx', sheet_name='PiData_PressureFilter', index_col=False)\n",
    "df_totalizer = pd.read_excel(r'C:\\Users\\v.t.flores\\Documents\\Pressure Filter Inv_2020.xlsx', sheet_name='Weigher', index_col=False)\n",
    "\n",
    "#Drop the row[1]\n",
    "df_JAN2019Pi = df_JAN2019Pi.drop(df_JAN2019Pi.index[0])\n",
    "df_totalizer = df_totalizer.drop(df_totalizer.index[0])\n",
    "#Step2: Parse the DATETIME column\n",
    "df_JAN2019Pi['DATETIME'] = df_JAN2019Pi['DATETIME'].apply(lambda x: parse(str(x)))\n",
    "df_totalizer['DATETIME'] = df_totalizer['DATETIME'].apply(lambda x: parse(str(x)))\n",
    "\n",
    "df_JAN2019Pi.set_index('DATETIME', inplace=True)\n",
    "df_comb = df_JAN2019Pi\n",
    "df_totalizer.set_index('DATETIME', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-70f443dd906c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_comb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\matrix.py\u001b[0m in \u001b[0;36mheatmap\u001b[1;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[0;32m    515\u001b[0m     plotter = _HeatMapper(data, vmin, vmax, cmap, center, robust, annot, fmt,\n\u001b[0;32m    516\u001b[0m                           \u001b[0mannot_kws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbar_kws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m                           yticklabels, mask)\n\u001b[0m\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m     \u001b[1;31m# Add the pcolormesh kwargs here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\matrix.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0mplot_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplot_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;31m# Validate the mask and convet to DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                 \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[1;31m# For data is list-like, or Iterable (will consume into list)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[1;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;31m# by definition an array here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mprep_ndarray\u001b[1;34m(values, copy)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Must pass 2-d input\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Must pass 2-d input"
     ]
    }
   ],
   "source": [
    "sns.heatmap(type(df_comb) == str, cbar = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DECLARE THE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---fcn defn--start\n",
    "#This function is used to replace the 'No good/ not enough data for calculation' value in the dataframe for data from PiData\n",
    "\n",
    "def det_no_data(value):\n",
    "    if type(value) == str:\n",
    "        reg = re.search(r'\\bCalculation\\b', value)\n",
    "        return reg[0]\n",
    "    else:\n",
    "        return value\n",
    "#------fcn defn end\n",
    "      \n",
    "#used in the laboratory analysis results data\n",
    "\n",
    "def filter_detrmin(cell_addrs):\n",
    "    PF_regex2 = re.compile(r'[a-z0-8]', re.IGNORECASE| re.VERBOSE|re.DOTALL)\n",
    "    filtername2 = PF_regex2.findall(str(cell_addrs))\n",
    "    for i in range(0, len(filtername2)):\n",
    "        if filtername2[i] != filtername2[-1]:\n",
    "            if filtername2[i]+filtername2[i+1] == '2A':\n",
    "                PFname = 'A'\n",
    "                return PFname\n",
    "            if filtername2[i]+filtername2[i+1] == '2B':\n",
    "                PFname = 'B'\n",
    "                return PFname      \n",
    "#-----------------end of function defn-----------        \n",
    "\n",
    "#fcn dfn\n",
    "#used in the analysis results data. cleans up the sampling time entry\n",
    "def time_cleaner(x):\n",
    "    time_regex = re.compile(r'[0-8a-zA-Z:]', re.IGNORECASE| re.VERBOSE|re.DOTALL)\n",
    "    timeact = time_regex.findall(x)\n",
    "    newtime = ''.join(timeact)\n",
    "    t = parse(x)\n",
    "    new_str_time = str(t.hour)+':'+str(t.minute)+':'+str(t.second)\n",
    "    return new_str_time\n",
    "\n",
    "#------------------\n",
    "\n",
    "def remove_no_good_data_N2Dry_Feed(df): #removes the string value and fills it with the last good value\n",
    "    \n",
    "    df = pd.to_numeric(df, errors='coerce')\n",
    "    \n",
    "    for x in range(0, len(df)):\n",
    "        if type(df[x]) == str:\n",
    "            df[x] = df[x-1]\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    return df\n",
    "#-----------------------\n",
    "def replace_no_good_data_with_LastGoodValues(df_col): #removes the string value and fills it with the last good value\n",
    "    \n",
    "    df_col = pd.to_numeric(df_col, errors='coerce')\n",
    "    for x in range(0, len(df_col)):\n",
    "        if type(df_col[x])== str:\n",
    "            if det_no_data(df_col[x]) == 'Calculation':\n",
    "                df_col[x] = np.nan\n",
    "\n",
    "    df_col.fillna(method='ffill', inplace= True)\n",
    "    return df_col\n",
    "\n",
    "#-----------------------\n",
    "def replace_strCalc_with_LastGoodVal(df_col):\n",
    "    regex_string = re.compile('[Calculation]', re.IGNORECASE| re.VERBOSE|re.DOTALL)\n",
    "    df_col = pd.to_numeric(df_col, errors='coerce')\n",
    "    for i in range(0, len(df_col)):\n",
    "        if type(df_col[i]) == str:\n",
    "            if bool(regex_string.match(df_col[i])) == True:\n",
    "                df_col[i] = np.nan\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    df_col.fillna(method='ffill', inplace=True)\n",
    "    return df_col\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------\n",
    "def replace_no_good_data_with_Zero(df_col): #removes the string value and fills it with zeroes\n",
    "\n",
    "    df_col = pd.to_numeric(df_col, errors='coerce')\n",
    "    for x in range(0, len(df_col)):\n",
    "        if type(df_col[x]) == str:\n",
    "            if det_no_data(df_col[x]) == 'Calculation':\n",
    "                df_col[x] = 0\n",
    "    df_col.fillna(value=0, inplace=True)\n",
    "    \n",
    "    return df_col\n",
    "#--------------------------------------------\n",
    "print('all functions declared', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df_comb['A_t_CWSH1'][-1]))\n",
    "print(df_comb['A_t_CWSH1'][-1])\n",
    "print(df_comb['A_t_CWSH1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cakewash1_clean = replace_strCalc_with_LastGoodVal(df_comb['A_t_CWSH1'])\n",
    "print(df_cakewash1_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb['A_t_CWSH1'].to_excel(r'C:\\Users\\v.t.flores\\Documents\\cakewash_withoutFunc_test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cakewash1_clean.to_excel(r'C:\\Users\\v.t.flores\\Documents\\cakewash_newFunc_test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the data. Remove all the Nan values using the two functions\n",
    "df_comb['A_t_FEED'] = remove_no_good_data_N2Dry_Feed(df_comb['A_t_FEED'])\n",
    "df_comb['A_t_DRY'] = remove_no_good_data_N2Dry_Feed(df_comb['A_t_DRY'])\n",
    "df_comb['A_t_CWSH1'] = replace_no_good_data_with_LastGoodValues(df_comb['A_t_CWSH1'])\n",
    "df_comb['A_t_CWSH2'] = replace_no_good_data_with_LastGoodValues(df_comb['A_t_CWSH2'])\n",
    "df_comb['A_t_PRESS1'] = replace_no_good_data_with_LastGoodValues(df_comb['A_t_PRESS1'])\n",
    "df_comb['A_t_PRESS2'] = replace_no_good_data_with_LastGoodValues(df_comb['A_t_PRESS2'])\n",
    "\n",
    "df_comb['106FT02A_MANFP'] = replace_no_good_data_with_Zero(df_comb['106FT02A_MANFP'])\n",
    "df_comb['106FT02A_CWP'] = replace_no_good_data_with_Zero(df_comb['106FT02A_CWP'])\n",
    "df_comb['106PU26A_CUR'] = replace_no_good_data_with_Zero(df_comb['106PU26A_CUR'])\n",
    "df_comb['106PU27A_CUR'] = replace_no_good_data_with_Zero(df_comb['106PU27A_CUR'])\n",
    "df_comb['PU26A_OP'] = replace_no_good_data_with_Zero(df_comb['PU26A_OP'])\n",
    "df_comb['PU27A_OP'] = replace_no_good_data_with_Zero(df_comb['PU27A_OP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECT THE ALPHA PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding peaks FT for A_filtration time\n",
    "#---function start---------\n",
    "def getTheLocalMinMax(df_col1, df_col2):\n",
    "    df_col1 = pd.to_numeric(df_col1, errors='coerce')\n",
    "    b = (np.diff(np.sign(np.diff(df_col1))) > 0).nonzero()[0] + 1         # local min\n",
    "   \n",
    "    df_col2 = pd.to_numeric(df_col2, errors='coerce')\n",
    "    f = (np.diff(np.sign(np.diff(df_col2))) < 0).nonzero()[0] + 1         # local max\n",
    "\n",
    "    try:\n",
    "        ft_cyc_lim = {}\n",
    "        for i in range(0, len(b)):\n",
    "            for j in range(0, len(f)):\n",
    "                if f[j] > b[i] and f[j] < b[i+1]:\n",
    "                    if f[j+1]<b[i+1]:\n",
    "                        cyc_start = b[i]\n",
    "                        cyc_end = f[j+1]\n",
    "                        ft_cyc_lim.setdefault(cyc_start,cyc_end)\n",
    "    except IndexError:\n",
    "        print('\\nDone')\n",
    "    return ft_cyc_lim\n",
    "\n",
    "#end of function----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the local min, max. Then generate the keys\n",
    "ft_cyc_lim = getTheLocalMinMax(df_comb['A_t_FEED'], df_comb['A_t_DRY'])\n",
    "keyslist = list(ft_cyc_lim.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_DF_parameter(df_col, str_colname_f):\n",
    "    df_col_indexList = list()\n",
    "    df_col_indexList[:] = []\n",
    "    df_col_1_inner = list()\n",
    "    df_col_1_inner[:] = []\n",
    "    Pair_df_col_List = list()\n",
    "    df_col_for_sort = list()\n",
    "    df_col_for_sort[:] = []\n",
    "\n",
    "    try:\n",
    "        for i in range(0, len(keyslist)):\n",
    "            df_colList_i = list()\n",
    "            df_colList_i[:] = []\n",
    "            d = list()\n",
    "            d[:] = []\n",
    "\n",
    "            df_colList_i = list(np.arange(keyslist[i],ft_cyc_lim[keyslist[i]]))\n",
    "            d = (np.diff(np.sign(np.diff(df_col.iloc[keyslist[i]:ft_cyc_lim[keyslist[i]]])))< 0).nonzero()[0] + 1\n",
    "\n",
    "            df_col_for_sort = list()\n",
    "            df_col_for_sort[:] = [] \n",
    "\n",
    "            df_col_indexList.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "\n",
    "            for i in range(0, len(d)):\n",
    "                shortlist = df_col.iloc[df_colList_i[d[i]]]\n",
    "                df_col_for_sort.append(shortlist)\n",
    "\n",
    "            df_col_1 = sorted(df_col_for_sort)[len(df_col_for_sort)-1]\n",
    "            df_col_1_inner.append(df_col_1)\n",
    "    except IndexError:\n",
    "        print('IndexError')\n",
    "        \n",
    "    Pair_df_col_List = list(zip(df_col_1_inner, df_col_indexList[0:]))\n",
    "    df_col1_Pair = pd.DataFrame(Pair_df_col_List)\n",
    "    df_col1_Pair.columns = [str_colname_f, 'DATETIME']\n",
    "    \n",
    "    return df_col1_Pair.set_index('DATETIME', inplace=True)\n",
    "     \n",
    "#----End of fcn-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A_CWP = generate_DF_parameterCW(df_comb['106FT02A_CWP'], 'A_CWP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A_CWP = df_comb['106FT02A_CWP']\n",
    "dfnam = 'A_CWP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_col_indexList = list()\n",
    "df_col_indexList[:] = []\n",
    "df_col_inner = list()\n",
    "df_col_inner[:] = []\n",
    "Pair_df_col_List = list()\n",
    "\n",
    "try:\n",
    "    for i in range(0, len(keyslist)):\n",
    "        df_colList_i = list()\n",
    "        df_colList_i[:] = []\n",
    "        d = list()\n",
    "        d[:] = []\n",
    "\n",
    "        df_colList_i = list(np.arange(keyslist[i],ft_cyc_lim[keyslist[i]]))\n",
    "        d = (np.diff(np.sign(np.diff(df_A_CWP.iloc[keyslist[i]:ft_cyc_lim[keyslist[i]]])))< 0).nonzero()[0] + 1\n",
    "\n",
    "        df_col_for_sort = list()\n",
    "        df_col_for_sort[:] = [] \n",
    "\n",
    "        df_col_indexList.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "\n",
    "        for i in range(0, len(d)):\n",
    "            shortlist = df_A_CWP.iloc[df_colList_i[d[i]]]\n",
    "            df_col_for_sort.append(shortlist)\n",
    "\n",
    "        df_col_1 = sorted(df_col_for_sort)[len(df_col_for_sort)-2]\n",
    "        df_col_inner.append(df_col_1)\n",
    "\n",
    "        \n",
    "except IndexError:\n",
    "    print('IndexError')\n",
    "        \n",
    "Pair_df_col_List = list(zip(df_col_inner, df_col_indexList[0:]))\n",
    "df_col1_Pair = pd.DataFrame(Pair_df_col_List)\n",
    "df_col1_Pair.columns = [dfnam, 'DATETIME']\n",
    "    \n",
    "   # return df_col1_Pair.set_index('DATETIME', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = generate_DF_parameterCW(df_comb['106FT02A_CWP'], 'A_CWP')\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyslist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_DF_parameterCW(df_col, str_colname_f):\n",
    "    df_col_indexList = list()\n",
    "    df_col_indexList[:] = []\n",
    "    df_col_inner = list()\n",
    "    df_col_inner[:] = []\n",
    "    Pair_df_col_List = list()\n",
    "\n",
    "    try:\n",
    "        for i in range(0, len(keyslist)):\n",
    "            df_colList_i = list()\n",
    "            df_colList_i[:] = []\n",
    "            d = list()\n",
    "            d[:] = []\n",
    "\n",
    "            df_colList_i = list(np.arange(keyslist[i],ft_cyc_lim[keyslist[i]]))\n",
    "            d = (np.diff(np.sign(np.diff(df_col.iloc[keyslist[i]:ft_cyc_lim[keyslist[i]]])))< 0).nonzero()[0] + 1\n",
    "\n",
    "            df_col_for_sort = list()\n",
    "            df_col_for_sort[:] = [] \n",
    "\n",
    "            df_col_indexList.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "\n",
    "            for i in range(0, len(d)):\n",
    "                shortlist = df_col.iloc[df_colList_i[d[i]]]\n",
    "                df_col_for_sort.append(shortlist)\n",
    "\n",
    "            df_col_1 = sorted(df_col_for_sort)[len(df_col_for_sort)-2]\n",
    "            df_col_inner.append(df_col_1)\n",
    "        \n",
    "    except IndexError:\n",
    "        print('IndexError')\n",
    "        \n",
    "    Pair_df_col_List = list(zip(df_col_inner, df_col_indexList[0:]))\n",
    "    df_col1_Pair = pd.DataFrame(Pair_df_col_List)\n",
    "    df_col1_Pair.columns = [str_colname_f, 'DATETIME']\n",
    "    \n",
    "    return df_col1_Pair.set_index('DATETIME', inplace=True)\n",
    "     \n",
    "#----End of fcn-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for feedT, A_MANFP, A_t_CWSH1, A_t_CWSH2, A_t_Press1, A_t_Press2, A_PU26A_OP, A_PU27A_CUR\n",
    "#A_PU26A_CUR, A_PU27A_OP\n",
    "\n",
    "def generate_DF_Parameter_comm(df_col, str_colname): \n",
    "    Val_List = list()\n",
    "    Val_List[:] = []\n",
    "    MaxList= list()\n",
    "    MaxList[:] = []\n",
    "    index_List = list()\n",
    "    index_List[:] = []\n",
    "    Pair_List = list()\n",
    "    Pair_List[:] = []\n",
    "  \n",
    "    try:         \n",
    "        for i in range(0, len(keyslist)):\n",
    "            Val_List = list()\n",
    "            Val_List[:] = []\n",
    "            for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):\n",
    "                print(df_col[j])\n",
    "                temp_var = df_col[j]\n",
    "                Val_List.append(temp_var)\n",
    "            MaxList.append(sorted(Val_List)[-1])\n",
    "            index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))  \n",
    "        Pair_List = list(zip(MaxList, index_List[0:]))      \n",
    "    except:\n",
    "        pass\n",
    "    df_param = pd.DataFrame(Pair_List)\n",
    "    df_param.columns = [str_colname, 'DATETIME']\n",
    "    df_param.set_index('DATETIME', inplace=True)\n",
    "\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will use the index range (or limits) to create a list of values: feedtime range, cake wash time range,\n",
    "# then selects the max value as the parameter value for that range\n",
    "\n",
    "\n",
    "#FILTRATION TIME\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "#For loop Dictionary names \n",
    "A_Filt_MaxList= list()\n",
    "A_Filt_MaxList[:] = []\n",
    "A_feedt_Val_List = list()\n",
    "A_feedt_Val_List[:] = []\n",
    "A_feedt_index_List = list()\n",
    "A_feedt_index_List[:] = []\n",
    "A_Pair_Filt_List = list()\n",
    "A_Pair_Filt_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        A_feedt_Val_List = list()\n",
    "        A_feedt_Val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):\n",
    "            A_feedt = df_comb['A_t_FEED'][j]\n",
    "            A_feedt_Val_List.append(A_feedt)\n",
    "        A_Filt_MaxList.append(sorted(A_feedt_Val_List)[-1])\n",
    "        A_feedt_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))  \n",
    "    A_Pair_Filt_List = list(zip(A_Filt_MaxList, A_feedt_index_List[0:]))      \n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_A_Pair_Filt_t = pd.DataFrame(A_Pair_Filt_List)\n",
    "df_A_Pair_Filt_t.columns = ['A_FILT_T', 'DATETIME']\n",
    "df_A_Pair_Filt_t.set_index('DATETIME', inplace=True)\n",
    "\n",
    "print('Done', str(datetime.datetime.now()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MANIFOLD PRESSURE\n",
    "\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "#For loop Dictionary names \n",
    "A_MANFP_MaxList = list()\n",
    "A_MANFP_MaxList[:] = []\n",
    "A_Pair_MANFP_List =list() \n",
    "A_Pair_MANFP_List[:] = []\n",
    "A_MANFP_index_List=list()      \n",
    "A_MANFP_index_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        A_MANFP_val_List=list() \n",
    "        A_MANFP_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            #Gets A manifold pressure of the cycle\n",
    "            A_MANFP = df_comb['106FT02A_MANFP'][j]         \n",
    "            A_MANFP_val_List.append(A_MANFP)\n",
    "        A_MANFP_MaxList.append(sorted(A_MANFP_val_List)[-1])\n",
    "        A_MANFP_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    A_Pair_MANFP_List = list(zip(A_MANFP_MaxList, A_MANFP_index_List[0:]))      \n",
    "                        \n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_A_Pair_MANF_P = pd.DataFrame(A_Pair_MANFP_List)\n",
    "df_A_Pair_MANF_P.columns = ['A_MANF_P', 'DATETIME']\n",
    "df_A_Pair_MANF_P.set_index('DATETIME', inplace=True)\n",
    "\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CWSH1\n",
    "##wORKING as of 2020.01.15\n",
    "\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "A_t_CWSH1_Maxlist = list()\n",
    "A_t_CWSH1_Maxlist[:] = []\n",
    "\n",
    "A_CWSH1_index_List=list()      \n",
    "A_CWSH1_index_List[:] = []\n",
    "\n",
    "A_Pair_CWSH1_List =list() \n",
    "A_Pair_CWSH1_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        A_t_CWSH1_val_List=list() \n",
    "        A_t_CWSH1_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            A_t_CWSH1 = df_comb['A_t_CWSH1'][j]         \n",
    "            A_t_CWSH1_val_List.append(A_t_CWSH1)\n",
    "        A_t_CWSH1_Maxlist.append(sorted(A_t_CWSH1_val_List)[-1])\n",
    "        A_CWSH1_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    A_Pair_CWSH1_List = list(zip(A_t_CWSH1_Maxlist, A_CWSH1_index_List[0:]))      \n",
    "                        \n",
    "except:\n",
    "    pass\n",
    "df_A_Pair_CWSH1 = pd.DataFrame(A_Pair_CWSH1_List)\n",
    "df_A_Pair_CWSH1.columns = ['A_CWSH1', 'DATETIME']\n",
    "df_A_Pair_CWSH1.set_index('DATETIME', inplace=True)\n",
    "\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CWSH2\n",
    "\n",
    "##wORKING as of 2020.01.15\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "A_t_CWSH2_Maxlist = list()\n",
    "A_t_CWSH2_Maxlist[:] = []\n",
    "\n",
    "A_CWSH2_index_List=list()      \n",
    "A_CWSH2_index_List[:] = []\n",
    "\n",
    "A_Pair_CWSH2_List =list() \n",
    "A_Pair_CWSH2_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        A_t_CWSH2_val_List=list() \n",
    "        A_t_CWSH2_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            A_t_CWSH2 = df_comb['A_t_CWSH2'][j]         \n",
    "            A_t_CWSH2_val_List.append(A_t_CWSH2)\n",
    "        A_t_CWSH2_Maxlist.append(sorted(A_t_CWSH2_val_List)[-1])\n",
    "        A_CWSH2_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    A_Pair_CWSH2_List = list(zip(A_t_CWSH2_Maxlist, A_CWSH2_index_List[0:]))      \n",
    "                        \n",
    "except:\n",
    "    pass\n",
    "  \n",
    "df_A_Pair_CWSH2 = pd.DataFrame(A_Pair_CWSH2_List)\n",
    "df_A_Pair_CWSH2.columns = ['A_CWSH2', 'DATETIME']\n",
    "df_A_Pair_CWSH2.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pressing_1\n",
    "##wORKING as of 2020.01.15\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "A_t_Press1_Maxlist = list()\n",
    "A_t_Press1_Maxlist[:] = []\n",
    "\n",
    "A_t_Press1_index_List=list()      \n",
    "A_t_Press1_index_List[:] = []\n",
    "\n",
    "A_Pair_A_t_Press1_List =list() \n",
    "A_Pair_A_t_Press1_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        A_t_Press1_val_List=list() \n",
    "        A_t_Press1_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            A_t_Press1 = df_comb['A_t_PRESS1'][j]         \n",
    "            A_t_Press1_val_List.append(A_t_Press1)\n",
    "        A_t_Press1_Maxlist.append(sorted(A_t_Press1_val_List)[-1])\n",
    "        A_t_Press1_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    A_Pair_Press1_List = list(zip(A_t_Press1_Maxlist, A_t_Press1_index_List[0:]))      \n",
    "                        \n",
    "except:\n",
    "    pass\n",
    "df_A_Pair_PRESS1_t = pd.DataFrame(A_Pair_Press1_List)\n",
    "df_A_Pair_PRESS1_t.columns = ['A_PRESS1_t', 'DATETIME']\n",
    "df_A_Pair_PRESS1_t.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pressing_2\n",
    "\n",
    "##wORKING as of 2020.01.15\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "A_t_Press2_Maxlist = list()\n",
    "A_t_Press2_Maxlist[:] = []\n",
    "\n",
    "A_t_Press2_index_List=list()      \n",
    "A_t_Press2_index_List[:] = []\n",
    "\n",
    "A_Pair_Press2_List =list() \n",
    "A_Pair_Press2_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        A_t_Press2_val_List=list() \n",
    "        A_t_Press2_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            A_t_Press2 = df_comb['A_t_PRESS2'][j]         \n",
    "            A_t_Press2_val_List.append(A_t_Press2)\n",
    "        A_t_Press2_Maxlist.append(sorted(A_t_Press2_val_List)[-1])\n",
    "        A_t_Press2_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    A_Pair_Press2_List = list(zip(A_t_Press2_Maxlist, A_t_Press2_index_List[0:]))      \n",
    "                        \n",
    "except:\n",
    "    pass\n",
    "  \n",
    "df_A_Pair_PRESS2_t = pd.DataFrame(A_Pair_Press2_List)\n",
    "df_A_Pair_PRESS2_t.columns = ['A_PRESS2_t', 'DATETIME']\n",
    "df_A_Pair_PRESS2_t.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A_PU26A_OP\n",
    "\n",
    "\n",
    "##Working as of 2020.01.15\n",
    "A_PU26A_OP_Maxlist = list()\n",
    "A_PU26A_OP_Maxlist[:] = []\n",
    "\n",
    "A_PU26A_OP_index_List=list()      \n",
    "A_PU26A_OP_index_List[:] = []\n",
    "\n",
    "A_Pair_PU26A_OP_List =list() \n",
    "A_Pair_PU26A_OP_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        A_PU26A_OP_val_List=list() \n",
    "        A_PU26A_OP_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            A_PU26A_OP = df_comb['PU26A_OP'][j]      \n",
    "            A_PU26A_OP_val_List.append(A_PU26A_OP)\n",
    "        A_PU26A_OP_Maxlist.append(sorted(A_PU26A_OP_val_List)[-1])\n",
    "        A_PU26A_OP_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    A_Pair_PU26A_OP_List = list(zip(A_PU26A_OP_Maxlist, A_PU26A_OP_index_List[0:]))          \n",
    "except:\n",
    "    pass\n",
    "df_A_Pair_PU26A_OP = pd.DataFrame(A_Pair_PU26A_OP_List)\n",
    "df_A_Pair_PU26A_OP.columns = ['A_PU26A_OP', 'DATETIME']\n",
    "df_A_Pair_PU26A_OP.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A_PU27A_OP\n",
    "##Working as of 2020.01.15\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "A_PU27A_OP_Maxlist = list()\n",
    "A_PU27A_OP_Maxlist[:] = []\n",
    "\n",
    "A_PU27A_OP_index_List=list()      \n",
    "A_PU27A_OP_index_List[:] = []\n",
    "\n",
    "A_Pair_PU27A_OP_List =list() \n",
    "A_Pair_PU27A_OP_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        A_PU27A_OP_val_List=list() \n",
    "        A_PU27A_OP_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            A_PU27A_OP = df_comb['PU27A_OP'][j]      \n",
    "            A_PU27A_OP_val_List.append(A_PU27A_OP)\n",
    "        A_PU27A_OP_Maxlist.append(sorted(A_PU27A_OP_val_List)[-1])\n",
    "        A_PU27A_OP_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    A_Pair_PU27A_OP_List = list(zip(A_PU27A_OP_Maxlist, A_PU27A_OP_index_List[0:]))          \n",
    "except:\n",
    "    pass\n",
    "  \n",
    "df_A_Pair_PU27A_OP = pd.DataFrame(A_Pair_PU27A_OP_List)\n",
    "df_A_Pair_PU27A_OP.columns = ['A_PU27A_OP', 'DATETIME']\n",
    "df_A_Pair_PU27A_OP.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code replaces the 'No Good Data' from 106PU26A_CUR column\n",
    "\n",
    "#A_PU26A_CUR\n",
    "##Working as of 2020.01.15\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "A_PU26A_CUR_Maxlist = list()\n",
    "A_PU26A_CUR_Maxlist[:] = []\n",
    "\n",
    "A_PU26A_CUR_index_List=list()      \n",
    "A_PU26A_CUR_index_List[:] = []\n",
    "\n",
    "A_Pair_PU26A_CUR_List =list() \n",
    "A_Pair_PU26A_CUR_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        A_PU26A_CUR_val=list() \n",
    "        A_PU26A_CUR_val[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            A_PU26A_CUR = df_comb['106PU26A_CUR'][j]      \n",
    "            A_PU26A_CUR_val.append(A_PU26A_CUR)\n",
    "        A_PU26A_CUR_Maxlist.append(sorted(A_PU26A_CUR_val)[-1])\n",
    "        A_PU26A_CUR_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    A_Pair_PU26A_CUR_List = list(zip(A_PU26A_CUR_Maxlist, A_PU26A_CUR_index_List[0:]))          \n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_A_Pair_PU26A_CUR = pd.DataFrame(A_Pair_PU26A_CUR_List)\n",
    "df_A_Pair_PU26A_CUR.columns = ['A_PU26A_CUR', 'DATETIME']\n",
    "df_A_Pair_PU26A_CUR.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code replaces the 'No Good Data' from 106PU26A_CUR column\n",
    "\n",
    "#A_PU27A_CUR\n",
    "##Working as of 2020.01.15 after a million trials!!!!!!!!!! mabuak na akong utok!!!\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "A_PU27A_CUR_Maxlist = list()\n",
    "A_PU27A_CUR_Maxlist[:] = []\n",
    "\n",
    "A_PU27A_CUR_index_List=list()      \n",
    "A_PU27A_CUR_index_List[:] = []\n",
    "\n",
    "A_Pair_PU27A_CUR_List =list() \n",
    "A_Pair_PU27A_CUR_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        A_PU27A_CUR_val=list() \n",
    "        A_PU27A_CUR_val[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            A_PU27A_CUR = df_comb['106PU27A_CUR'][j]      \n",
    "            A_PU27A_CUR_val.append(A_PU27A_CUR)\n",
    "        A_PU27A_CUR_Maxlist.append(sorted(A_PU27A_CUR_val)[-1])\n",
    "        A_PU27A_CUR_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    A_Pair_PU27A_CUR_List = list(zip(A_PU27A_CUR_Maxlist, A_PU27A_CUR_index_List[0:]))          \n",
    "except:\n",
    "    pass\n",
    "  \n",
    "df_A_Pair_PU27A_CUR = pd.DataFrame(A_Pair_PU27A_CUR_List)\n",
    "df_A_Pair_PU27A_CUR.columns = ['A_PU27A_CUR', 'DATETIME']\n",
    "df_A_Pair_PU27A_CUR.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A_N2 Dry\n",
    "##Working as of 2020-01-15 17:26:09.736327\n",
    "#this code replaces the 'No Good Data' from 106PU26A_CUR column\n",
    "A_Pair_N2_Dry_List = list()\n",
    "A_Pair_N2_Dry_List[:] = []\n",
    "A_t_N2_Dry_indexList = list()\n",
    "A_t_N2_Dry_indexList[:] = []\n",
    "A_t_N2_Dry_val = list()\n",
    "A_t_N2_Dry_val[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):   \n",
    "        A_t_N2_Dry = df_comb['A_t_DRY'][ft_cyc_lim[keyslist[i]]]\n",
    "        A_t_N2_Dry_val.append(A_t_N2_Dry)\n",
    "        A_t_N2_Dry_indexList.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    \n",
    "    A_Pair_N2_Dry_List = list(zip(A_t_N2_Dry_val, A_t_N2_Dry_indexList[0:]))          \n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_A_Pair_N2DRY = pd.DataFrame(A_Pair_N2_Dry_List)\n",
    "df_A_Pair_N2DRY.columns = ['A_N2DRY', 'DATETIME']\n",
    "df_A_Pair_N2DRY.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn = df_A_Pair_CWP_1.join(df_A_Pair_CWP_2, how='outer')\n",
    "dfn1 = df_A_Pair_Filt_t.join(df_A_Pair_MANF_P, how='outer')\n",
    "dfn2 = df_A_Pair_CWSH1.join(df_A_Pair_CWSH2, how='outer')\n",
    "dfn3 = df_A_Pair_PRESS1_t.join(df_A_Pair_PRESS2_t, how='outer')\n",
    "dfn4 = df_A_Pair_PU26A_OP.join(df_A_Pair_PU27A_OP, how='outer')\n",
    "dfn5 = df_A_Pair_PU26A_CUR.join(df_A_Pair_PU27A_CUR, how='outer')\n",
    "dfn6 = df_A_Pair_N2DRY.join(dfn, how='outer')\n",
    "dfn7 = dfn1.join(dfn2, how='outer')\n",
    "dfn8 = dfn3.join(dfn4, how='outer')\n",
    "dfn9 = dfn5.join(dfn6, how='outer')\n",
    "dfn10 = dfn7.join(dfn8, how='outer')\n",
    "dfn11 = dfn9.join(dfn10, how='outer')\n",
    "df_PressureFilter_Param = dfn11\n",
    "\n",
    "\n",
    "dfn11['Cycle_Count'] = np.arange(1, len(dfn11['startdate'])+1)\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn11.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECT THE BRAVO PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb['B_t_DRY'] = pd.to_numeric(df_comb['B_t_DRY'], errors='coerce')\n",
    "df_comb['B_t_FEED'] = pd.to_numeric(df_comb['B_t_FEED'], errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "#remove the no good data by first replacing it with np.nan\n",
    "for x in range(0, len(df_comb['B_t_DRY'])):\n",
    "    if type(df_comb['B_t_DRY'][x]== str):\n",
    "        if det_no_data(df_comb['B_t_DRY'][x]) == 'Calculation':\n",
    "            df_comb['B_t_DRY'][x] = np.nan\n",
    "            \n",
    "for x in range(0, len(df_comb['B_t_FEED'])):\n",
    "    if type(df_comb['B_t_FEED'][x]== str):\n",
    "        if det_no_data(df_comb['B_t_FEED'][x]) == 'Calculation':\n",
    "            df_comb['B_t_FEED'][x] = np.nan\n",
    "            \n",
    "#then filling in the Nan value with the last valid value           \n",
    "df_comb['B_t_FEED'].fillna(method='ffill', inplace= True)\n",
    "df_comb['B_t_DRY'].fillna(method='ffill', inplace= True)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb['B_t_CWSH1'] = pd.to_numeric(df_comb['B_t_CWSH1'], errors='coerce')\n",
    "for x in range(0, len(df_comb['B_t_CWSH1'])):\n",
    "    if type(df_comb['B_t_CWSH1'][x]== str):\n",
    "        if det_no_data(df_comb['B_t_CWSH1'][x]) == 'Calculation':\n",
    "            df_comb['B_t_CWSH1'][x] = np.nan\n",
    "\n",
    "df_comb['B_t_CWSH1'].fillna(method='ffill', inplace= True)\n",
    "#-------------------------\n",
    "\n",
    "df_comb['B_t_CWSH2'] = pd.to_numeric(df_comb['B_t_CWSH2'], errors='coerce')\n",
    "for x in range(0, len(df_comb['B_t_CWSH2'])):\n",
    "    if type(df_comb['B_t_CWSH2'][x]== str):\n",
    "        if det_no_data(df_comb['B_t_CWSH2'][x]) == 'Calculation':\n",
    "            df_comb['B_t_CWSH2'][x] = np.nan\n",
    "\n",
    "df_comb['B_t_CWSH2'].fillna(method='ffill', inplace= True)\n",
    "\n",
    "#-------------------------\n",
    "df_comb['B_t_PRESS1'] = pd.to_numeric(df_comb['B_t_PRESS1'], errors='coerce')\n",
    "for x in range(0, len(df_comb['B_t_PRESS1'])):\n",
    "    if type(df_comb['B_t_PRESS1'][x]== str):\n",
    "        if det_no_data(df_comb['B_t_PRESS1'][x]) == 'Calculation':\n",
    "            df_comb['B_t_PRESS1'][x] = np.nan\n",
    "\n",
    "df_comb['B_t_PRESS1'].fillna(method='ffill', inplace= True)\n",
    "\n",
    "#-------------------------\n",
    "df_comb['B_t_PRESS2'] = pd.to_numeric(df_comb['B_t_PRESS2'], errors='coerce')\n",
    "for x in range(0, len(df_comb['B_t_PRESS2'])):\n",
    "    if type(df_comb['B_t_PRESS2'][x]== str):\n",
    "        if det_no_data(df_comb['B_t_PRESS2'][x]) == 'Calculation':\n",
    "            df_comb['B_t_PRESS2'][x] = np.nan\n",
    "\n",
    "df_comb['B_t_PRESS2'].fillna(method='ffill', inplace= True)\n",
    "#-------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "df_comb['FT02B_CWP'] = pd.to_numeric(df_comb['FT02B_CWP'], errors='coerce')\n",
    "\n",
    "for x in range(0, len(df_comb['FT02B_CWP'])):\n",
    "    if type(df_comb['FT02B_CWP'][x]) == str:\n",
    "        if det_no_data(df_comb['FT02B_CWP'][x]) == 'Calculation':\n",
    "            df_comb['FT02B_CWP'][x] = 0\n",
    "\n",
    "df_comb['FT02B_CWP'].fillna(value=0, inplace=True)\n",
    "#--------------------------------------------\n",
    "\n",
    "#--------------------------------------------\n",
    "df_comb['106PU26B_CUR'] = pd.to_numeric(df_comb['106PU26B_CUR'], errors='coerce')\n",
    "\n",
    "for x in range(0, len(df_comb['106PU26B_CUR'])):\n",
    "    if type(df_comb['106PU26B_CUR'][x]) == str:\n",
    "        if det_no_data(df_comb['106PU26B_CUR'][x]) == 'Calculation':\n",
    "            df_comb['106PU26B_CUR'][x] = 0\n",
    "\n",
    "df_comb['106PU26B_CUR'].fillna(value=0, inplace=True)\n",
    "#--------------------------------------------\n",
    "\n",
    "#--------------------------------------------\n",
    "df_comb['PU26B_OP'] = pd.to_numeric(df_comb['PU26B_OP'], errors='coerce')\n",
    "\n",
    "for x in range(0, len(df_comb['PU26B_OP'])):\n",
    "    if type(df_comb['PU26B_OP'][x]) == str:\n",
    "        if det_no_data(df_comb['PU26B_OP'][x]) == 'Calculation':\n",
    "            df_comb['PU26B_OP'][x] = 0\n",
    "\n",
    "df_comb['PU26B_OP'].fillna(value=0, inplace=True)\n",
    "#--------------------------------------------\n",
    "df_comb['PU27B_OP'] = pd.to_numeric(df_comb['PU27B_OP'], errors='coerce')\n",
    "\n",
    "for x in range(0, len(df_comb['PU27B_OP'])):\n",
    "    if type(df_comb['PU27B_OP'][x]) == str:\n",
    "        if det_no_data(df_comb['PU27B_OP'][x]) == 'Calculation':\n",
    "            df_comb['PU27B_OP'][x] = 0\n",
    "\n",
    "df_comb['PU27B_OP'].fillna(value=0, inplace=True)\n",
    "#--------------------------------------------\n",
    "\n",
    "df_comb['106PU27B_CUR'] = pd.to_numeric(df_comb['106PU27B_CUR'], errors='coerce')\n",
    "\n",
    "for x in range(0, len(df_comb['106PU27B_CUR'])):\n",
    "    if type(df_comb['106PU27B_CUR'][x]) == str:\n",
    "        if det_no_data(df_comb['106PU27B_CUR'][x]) == 'Calculation':\n",
    "            df_comb['106PU27B_CUR'][x] = 0\n",
    "\n",
    "df_comb['106PU27B_CUR'].fillna(value=0, inplace=True)\n",
    "#--------------------------------------------\n",
    "df_comb['FT02B_MANFP'] = pd.to_numeric(df_comb['FT02B_MANFP'], errors='coerce')\n",
    "\n",
    "for x in range(0, len(df_comb['FT02B_MANFP'])):\n",
    "    if type(df_comb['FT02B_MANFP'][x]) == str:\n",
    "        if det_no_data(df_comb['FT02B_MANFP'][x]) == 'Calculation':\n",
    "            df_comb['FT02B_MANFP'][x] = 0\n",
    "\n",
    "df_comb['FT02B_MANFP'].fillna(value=0, inplace=True)\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exc7\n",
    "#finding peaks FT for B_filtration time\n",
    "\n",
    "a = np.diff(np.sign(np.diff(df_comb['B_t_FEED']))).nonzero()[0] + 1               # local min & max\n",
    "b = (np.diff(np.sign(np.diff(df_comb['B_t_FEED']))) > 0).nonzero()[0] + 1         # local min\n",
    "c = (np.diff(np.sign(np.diff(df_comb['B_t_FEED']))) < 0).nonzero()[0] + 1         # local max\n",
    "# +1 due to the fact that diff reduces the original index number\n",
    "\n",
    "\n",
    "#finding peaks FT for A_N2 Dry time\n",
    "\n",
    "d = np.diff(np.sign(np.diff(df_comb['B_t_DRY']))).nonzero()[0] + 1               # local min & max\n",
    "e = (np.diff(np.sign(np.diff(df_comb['B_t_DRY']))) > 0).nonzero()[0] + 1         # local min\n",
    "f = (np.diff(np.sign(np.diff(df_comb['B_t_DRY']))) < 0).nonzero()[0] + 1         # local max\n",
    "# +1 due to the fact that diff reduces the original index number\n",
    "\n",
    "#Use b as the starting index and f as the ending index for one full cylcle of Filtration to N2 Drying\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------\n",
    "#From the peaks computed in previous code, the index range for each cycle is stored in a dictionary called ft_cyc_lim\n",
    "try:\n",
    "    ft_cyc_lim = {}\n",
    "\n",
    "    for i in range(0, len(b)):\n",
    "        for j in range(0, len(f)):\n",
    "            if f[j] > b[i] and f[j] < b[i+1]:\n",
    "                if f[j+1]<b[i+1]:\n",
    "                    cyc_start = b[i]\n",
    "                    cyc_end = f[j+1]\n",
    "                    ft_cyc_lim.setdefault(cyc_start,cyc_end)\n",
    "\n",
    "except IndexError:\n",
    "    print('\\nDone')\n",
    "#------------------------------------------------------------     \n",
    "\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb['FT02B_CWP'].to_excel(r'C:\\Users\\v.t.flores\\Documents\\fttwmp.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "#code for getting the CWP for CakeWash1 and CakeWash2\n",
    "B_CWP_1new = list()\n",
    "B_CWP_1new[:] = []\n",
    "\n",
    "B_CWP_1_indexList = list()\n",
    "B_CWP_1_indexList[:] = []\n",
    "\n",
    "B_CWP_2new = list()\n",
    "B_CWP_2new[:] = []\n",
    "\n",
    "B_CWP_2_indexList = list()\n",
    "B_CWP_2_indexList[:] = []\n",
    "\n",
    "B_CWP_1_inner = list()\n",
    "B_CWP_1_inner[:] = []\n",
    "\n",
    "B_CWP_2_inner = list()\n",
    "B_CWP_2_inner[:] = []\n",
    "\n",
    "B_Pair_CWP_1_List = list()\n",
    "B_Pair_CWP_2_List = list()\n",
    "\n",
    "try:\n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_CWPList_i = list()\n",
    "        B_CWPList_i[:] = []\n",
    "        d = list()\n",
    "        d[:] = []\n",
    "        \n",
    "        B_CWPList_i = list(np.arange(keyslist[i],ft_cyc_lim[keyslist[i]]))\n",
    "        d = (np.diff(np.sign(np.diff(df_comb['FT02B_CWP'].iloc[keyslist[i]:ft_cyc_lim[keyslist[i]]])))< 0).nonzero()[0] + 1\n",
    "        B_CWP_for_sort = list()\n",
    "        B_CWP_for_sort[:] = [] \n",
    "        \n",
    "        B_CWP_1_indexList.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "        B_CWP_2_indexList.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "        \n",
    "        if d.size != 0:\n",
    "            for i in range(0, len(d)):\n",
    "                CWP_val_shortlist = df_comb['FT02B_CWP'].iloc[B_CWPList_i[d[i]]]\n",
    "                B_CWP_for_sort.append(CWP_val_shortlist)\n",
    "\n",
    "            B_CWP_1 = sorted(B_CWP_for_sort)[len(B_CWP_for_sort)-1]\n",
    "            B_CWP_1_inner.append(B_CWP_1)\n",
    "\n",
    "            B_CWP_2 = sorted(B_CWP_for_sort)[len(B_CWP_for_sort)-2]\n",
    "            B_CWP_2_inner.append(B_CWP_2)\n",
    "            \n",
    "        if d.size == 0:\n",
    "            B_CWP_1_inner.append(0)\n",
    "            B_CWP_2_inner.append(0)\n",
    "\n",
    "    B_Pair_CWP_1_List = list(zip(B_CWP_1_inner, B_CWP_1_indexList[0:]))\n",
    "    B_Pair_CWP_2_List = list(zip(B_CWP_2_inner, B_CWP_2_indexList[0:]))\n",
    "            \n",
    "except IndexError:\n",
    "    print('IndexError')\n",
    "    \n",
    "    \n",
    "df_B_Pair_CWP_1 = pd.DataFrame(B_Pair_CWP_1_List)\n",
    "df_B_Pair_CWP_1.columns = ['B_CWP_1', 'DATETIME']\n",
    "df_B_Pair_CWP_1.set_index('DATETIME', inplace=True)\n",
    "    \n",
    "df_B_Pair_CWP_2 = pd.DataFrame(B_Pair_CWP_2_List)\n",
    "df_B_Pair_CWP_2.columns = ['B_CWP_2', 'DATETIME']\n",
    "df_B_Pair_CWP_2.set_index('DATETIME', inplace=True)\n",
    "   \n",
    "    \n",
    "print('Done', str(datetime.datetime.now()))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will use the index range (or limits) to create a list of values: feedtime range, cake wash time range,\n",
    "# then selects the max value as the parameter value for that range\n",
    "\n",
    "#FILTRATION TIME\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "#For loop Dictionary names \n",
    "B_Filt_MaxList= list()\n",
    "B_Filt_MaxList[:] = []\n",
    "B_feedt_Val_List = list()\n",
    "B_feedt_Val_List[:] = []\n",
    "B_feedt_index_List = list()\n",
    "B_feedt_index_List[:] = []\n",
    "B_Pair_Filt_List = list()\n",
    "B_Pair_Filt_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_feedt_Val_List = list()\n",
    "        B_feedt_Val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):\n",
    "            B_feedt = df_comb['B_t_FEED'][j]\n",
    "            B_feedt_Val_List.append(B_feedt)\n",
    "        B_Filt_MaxList.append(sorted(B_feedt_Val_List)[-1])\n",
    "        B_feedt_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))  \n",
    "    B_Pair_Filt_List = list(zip(B_Filt_MaxList, B_feedt_index_List[0:]))      \n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_B_Pair_Filt_t = pd.DataFrame(B_Pair_Filt_List)\n",
    "df_B_Pair_Filt_t.columns = ['B_FILT_T', 'DATETIME']\n",
    "df_B_Pair_Filt_t.set_index('DATETIME', inplace=True)\n",
    "\n",
    "print('Done', str(datetime.datetime.now()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "df_comb['FT02B_MANFP'] = pd.to_numeric(df_comb['FT02B_MANFP'], errors='coerce')\n",
    "\n",
    "for x in range(0, len(df_comb['FT02B_MANFP'])):\n",
    "    if type(df_comb['FT02B_MANFP'][x]) == str:\n",
    "        if det_no_data(df_comb['FT02B_MANFP'][x]) == 'Calculation':\n",
    "            df_comb['FT02B_MANFP'][x] = 0\n",
    "\n",
    "df_comb['FT02B_MANFP'].fillna(value=0, inplace=True)\n",
    "#--------------------------------------------\n",
    "\n",
    "#MANIFOLD PRESSURE_B\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "#For loop Dictionary names \n",
    "B_MANFP_MaxList = list()\n",
    "B_MANFP_MaxList[:] = []\n",
    "B_Pair_MANFP_List =list() \n",
    "B_Pair_MANFP_List[:] = []\n",
    "B_MANFP_index_List=list()      \n",
    "B_MANFP_index_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_MANFP_val_List=list() \n",
    "        B_MANFP_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            #Gets A manifold pressure of the cycle\n",
    "            B_MANFP = df_comb['FT02B_MANFP'][j]         \n",
    "            B_MANFP_val_List.append(B_MANFP)\n",
    "        B_MANFP_MaxList.append(sorted(B_MANFP_val_List)[-1])\n",
    "        B_MANFP_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    B_Pair_MANFP_List = list(zip(B_MANFP_MaxList, B_MANFP_index_List[0:]))      \n",
    "                        \n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_B_Pair_MANF_P = pd.DataFrame(B_Pair_MANFP_List)\n",
    "df_B_Pair_MANF_P.columns = ['B_MANF_P', 'DATETIME']\n",
    "df_B_Pair_MANF_P.set_index('DATETIME', inplace=True)\n",
    "\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CWSH1\n",
    "##wORKING as of 2020.01.15\n",
    "\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "B_t_CWSH1_Maxlist = list()\n",
    "B_t_CWSH1_Maxlist[:] = []\n",
    "\n",
    "B_CWSH1_index_List=list()      \n",
    "B_CWSH1_index_List[:] = []\n",
    "\n",
    "B_Pair_CWSH1_List =list() \n",
    "B_Pair_CWSH1_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_t_CWSH1_val_List=list() \n",
    "        B_t_CWSH1_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            B_t_CWSH1 = df_comb['B_t_CWSH1'][j]         \n",
    "            B_t_CWSH1_val_List.append(B_t_CWSH1)\n",
    "        B_t_CWSH1_Maxlist.append(sorted(B_t_CWSH1_val_List)[-1])\n",
    "        B_CWSH1_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    B_Pair_CWSH1_List = list(zip(B_t_CWSH1_Maxlist, B_CWSH1_index_List[0:]))      \n",
    "                        \n",
    "except:\n",
    "    pass\n",
    "df_B_Pair_CWSH1 = pd.DataFrame(B_Pair_CWSH1_List)\n",
    "df_B_Pair_CWSH1.columns = ['B_CWSH1', 'DATETIME']\n",
    "df_B_Pair_CWSH1.set_index('DATETIME', inplace=True)\n",
    "\n",
    "print('Done', str(datetime.datetime.now()))\n",
    "#CWSH2\n",
    "##wORKING as of 2020.01.15\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "B_t_CWSH2_Maxlist = list()\n",
    "B_t_CWSH2_Maxlist[:] = []\n",
    "\n",
    "B_CWSH2_index_List=list()      \n",
    "B_CWSH2_index_List[:] = []\n",
    "\n",
    "B_Pair_CWSH2_List =list() \n",
    "B_Pair_CWSH2_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_t_CWSH2_val_List=list() \n",
    "        B_t_CWSH2_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            B_t_CWSH2 = df_comb['B_t_CWSH2'][j]         \n",
    "            B_t_CWSH2_val_List.append(B_t_CWSH2)\n",
    "        B_t_CWSH2_Maxlist.append(sorted(B_t_CWSH2_val_List)[-1])\n",
    "        B_CWSH2_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    B_Pair_CWSH2_List = list(zip(B_t_CWSH2_Maxlist, B_CWSH2_index_List[0:]))      \n",
    "                        \n",
    "except:\n",
    "    pass\n",
    "  \n",
    "df_B_Pair_CWSH2 = pd.DataFrame(B_Pair_CWSH2_List)\n",
    "df_B_Pair_CWSH2.columns = ['B_CWSH2', 'DATETIME']\n",
    "df_B_Pair_CWSH2.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))\n",
    "#Pressing_1\n",
    "##wORKING as of 2020.01.15\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "B_t_Press1_Maxlist = list()\n",
    "B_t_Press1_Maxlist[:] = []\n",
    "\n",
    "B_t_Press1_index_List=list()      \n",
    "B_t_Press1_index_List[:] = []\n",
    "\n",
    "B_Pair_B_t_Press1_List =list() \n",
    "B_Pair_B_t_Press1_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_t_Press1_val_List=list() \n",
    "        B_t_Press1_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            B_t_Press1 = df_comb['B_t_PRESS1'][j]         \n",
    "            B_t_Press1_val_List.append(B_t_Press1)\n",
    "        B_t_Press1_Maxlist.append(sorted(B_t_Press1_val_List)[-1])\n",
    "        B_t_Press1_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    B_Pair_Press1_List = list(zip(B_t_Press1_Maxlist, B_t_Press1_index_List[0:]))      \n",
    "                        \n",
    "except:\n",
    "    pass\n",
    "df_B_Pair_PRESS1_t = pd.DataFrame(B_Pair_Press1_List)\n",
    "df_B_Pair_PRESS1_t.columns = ['B_PRESS1_t', 'DATETIME']\n",
    "df_B_Pair_PRESS1_t.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))\n",
    "#Pressing_2\n",
    "##wORKING as of 2020.01.15\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "B_t_Press2_Maxlist = list()\n",
    "B_t_Press2_Maxlist[:] = []\n",
    "\n",
    "B_t_Press2_index_List=list()      \n",
    "B_t_Press2_index_List[:] = []\n",
    "\n",
    "B_Pair_Press2_List =list() \n",
    "B_Pair_Press2_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_t_Press2_val_List=list() \n",
    "        B_t_Press2_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            B_t_Press2 = df_comb['B_t_PRESS2'][j]         \n",
    "            B_t_Press2_val_List.append(B_t_Press2)\n",
    "        B_t_Press2_Maxlist.append(sorted(B_t_Press2_val_List)[-1])\n",
    "        B_t_Press2_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    B_Pair_Press2_List = list(zip(B_t_Press2_Maxlist, B_t_Press2_index_List[0:]))      \n",
    "                        \n",
    "except:\n",
    "    pass\n",
    "  \n",
    "df_B_Pair_PRESS2_t = pd.DataFrame(B_Pair_Press2_List)\n",
    "df_B_Pair_PRESS2_t.columns = ['B_PRESS2_t', 'DATETIME']\n",
    "df_B_Pair_PRESS2_t.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))\n",
    "\n",
    "#B_PU26B_OP\n",
    "\n",
    "B_PU26B_OP_Maxlist = list()\n",
    "B_PU26B_OP_Maxlist[:] = []\n",
    "\n",
    "B_PU26B_OP_index_List=list()      \n",
    "B_PU26B_OP_index_List[:] = []\n",
    "\n",
    "B_Pair_PU26B_OP_List =list() \n",
    "B_Pair_PU26B_OP_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_PU26B_OP_val_List=list() \n",
    "        B_PU26B_OP_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            B_PU26B_OP = df_comb['PU26B_OP'][j]      \n",
    "            B_PU26B_OP_val_List.append(B_PU26B_OP)\n",
    "        B_PU26B_OP_Maxlist.append(sorted(B_PU26B_OP_val_List)[-1])\n",
    "        B_PU26B_OP_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    B_Pair_PU26B_OP_List = list(zip(B_PU26B_OP_Maxlist, B_PU26B_OP_index_List[0:]))          \n",
    "except:\n",
    "    pass\n",
    "df_B_Pair_PU26B_OP = pd.DataFrame(B_Pair_PU26B_OP_List)\n",
    "df_B_Pair_PU26B_OP.columns = ['B_PU26B_OP', 'DATETIME']\n",
    "df_B_Pair_PU26B_OP.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))\n",
    "\n",
    "#B_PU27B_OP\n",
    "##Working as of 2020.01.15\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "B_PU27B_OP_Maxlist = list()\n",
    "B_PU27B_OP_Maxlist[:] = []\n",
    "\n",
    "B_PU27B_OP_index_List=list()      \n",
    "B_PU27B_OP_index_List[:] = []\n",
    "\n",
    "B_Pair_PU27B_OP_List =list() \n",
    "B_Pair_PU27B_OP_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_PU27B_OP_val_List=list() \n",
    "        B_PU27B_OP_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            B_PU27B_OP = df_comb['PU27B_OP'][j]      \n",
    "            B_PU27B_OP_val_List.append(B_PU27B_OP)\n",
    "        B_PU27B_OP_Maxlist.append(sorted(B_PU27B_OP_val_List)[-1])\n",
    "        B_PU27B_OP_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    B_Pair_PU27B_OP_List = list(zip(B_PU27B_OP_Maxlist, B_PU27B_OP_index_List[0:]))          \n",
    "except:\n",
    "    pass\n",
    "  \n",
    "df_B_Pair_PU27B_OP = pd.DataFrame(B_Pair_PU27B_OP_List)\n",
    "df_B_Pair_PU27B_OP.columns = ['B_PU27B_OP', 'DATETIME']\n",
    "df_B_Pair_PU27B_OP.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))\n",
    "\n",
    "#this code replaces the 'No Good Data' from 106PU26A_CUR column\n",
    "for x in range(0, len(df_comb['106PU26B_CUR'])):\n",
    "    if df_comb['106PU26B_CUR'][x] =='[-11059] No Good Data For Calculation':\n",
    "            df_comb['106PU26B_CUR'][x] = 0\n",
    "#B_PU26A_CUR\n",
    "##Working as of 2020.01.15\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "B_PU26B_CUR_Maxlist = list()\n",
    "B_PU26B_CUR_Maxlist[:] = []\n",
    "\n",
    "B_PU26B_CUR_index_List=list()      \n",
    "B_PU26B_CUR_index_List[:] = []\n",
    "\n",
    "B_Pair_PU26B_CUR_List =list() \n",
    "B_Pair_PU26B_CUR_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_PU26B_CUR_val=list() \n",
    "        B_PU26B_CUR_val[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            B_PU26B_CUR = df_comb['106PU26B_CUR'][j]      \n",
    "            B_PU26B_CUR_val.append(B_PU26B_CUR)\n",
    "        B_PU26B_CUR_Maxlist.append(sorted(B_PU26B_CUR_val)[-1])\n",
    "        B_PU26B_CUR_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    B_Pair_PU26B_CUR_List = list(zip(B_PU26B_CUR_Maxlist, B_PU26B_CUR_index_List[0:]))          \n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_B_Pair_PU26B_CUR = pd.DataFrame(B_Pair_PU26B_CUR_List)\n",
    "df_B_Pair_PU26B_CUR.columns = ['B_PU26B_CUR', 'DATETIME']\n",
    "df_B_Pair_PU26B_CUR.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))\n",
    "\n",
    "#this code replaces the 'No Good Data' from 106PU26A_CUR column\n",
    "for x in range(0, len(df_comb['106PU27B_CUR'])):\n",
    "    if df_comb['106PU27B_CUR'][x] =='[-11059] No Good Data For Calculation':\n",
    "            df_comb['106PU27B_CUR'][x] = 0\n",
    "#B_PU27B_CUR\n",
    "##Working as of 2020.01.15 after a million trials!!!!!!!!!! mabuak na akong utok!!!\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "B_PU27B_CUR_Maxlist = list()\n",
    "B_PU27B_CUR_Maxlist[:] = []\n",
    "\n",
    "B_PU27B_CUR_index_List=list()      \n",
    "B_PU27B_CUR_index_List[:] = []\n",
    "\n",
    "B_Pair_PU27B_CUR_List =list() \n",
    "B_Pair_PU27B_CUR_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_PU27B_CUR_val=list() \n",
    "        B_PU27B_CUR_val[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            B_PU27B_CUR = df_comb['106PU27B_CUR'][j]      \n",
    "            B_PU27B_CUR_val.append(B_PU27B_CUR)\n",
    "        B_PU27B_CUR_Maxlist.append(sorted(B_PU27B_CUR_val)[-1])\n",
    "        B_PU27B_CUR_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    B_Pair_PU27B_CUR_List = list(zip(B_PU27B_CUR_Maxlist, B_PU27B_CUR_index_List[0:]))          \n",
    "except:\n",
    "    pass\n",
    "  \n",
    "df_B_Pair_PU27B_CUR = pd.DataFrame(B_Pair_PU27B_CUR_List)\n",
    "df_B_Pair_PU27B_CUR.columns = ['B_PU27B_CUR', 'DATETIME']\n",
    "df_B_Pair_PU27B_CUR.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))\n",
    "\n",
    "#A_N2 Dry\n",
    "##Working as of 2020-01-15 17:26:09.736327\n",
    "B_Pair_N2_Dry_List = list()\n",
    "B_Pair_N2_Dry_List[:] = []\n",
    "B_t_N2_Dry_indexList = list()\n",
    "B_t_N2_Dry_indexList[:] = []\n",
    "B_t_N2_Dry_val = list()\n",
    "B_t_N2_Dry_val[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):   \n",
    "        B_t_N2_Dry = df_comb['B_t_DRY'][ft_cyc_lim[keyslist[i]]]\n",
    "        B_t_N2_Dry_val.append(B_t_N2_Dry)\n",
    "        B_t_N2_Dry_indexList.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    \n",
    "    B_Pair_N2_Dry_List = list(zip(B_t_N2_Dry_val, B_t_N2_Dry_indexList[0:]))          \n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_B_Pair_N2DRY = pd.DataFrame(B_Pair_N2_Dry_List)\n",
    "df_B_Pair_N2DRY.columns = ['B_N2DRY', 'DATETIME']\n",
    "df_B_Pair_N2DRY.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining the dataframes for bravo parameters\n",
    "dfnB = df_B_Pair_CWP_1.join(df_B_Pair_CWP_2, how='outer')\n",
    "dfn1B = df_B_Pair_Filt_t.join(df_B_Pair_MANF_P, how='outer')\n",
    "dfn2B = df_B_Pair_CWSH1.join(df_B_Pair_CWSH2, how='outer')\n",
    "dfn3B = df_B_Pair_PRESS1_t.join(df_B_Pair_PRESS2_t, how='outer')\n",
    "dfn4B = df_B_Pair_PU26B_OP.join(df_B_Pair_PU27B_OP, how='outer')\n",
    "dfn5B = df_B_Pair_PU26B_CUR.join(df_B_Pair_PU27B_CUR, how='outer')\n",
    "dfn6B = df_B_Pair_N2DRY.join(dfnB, how='outer')\n",
    "dfn7B = dfn1B.join(dfn2B, how='outer')\n",
    "dfn8B = dfn3B.join(dfn4B, how='outer')\n",
    "dfn9B = dfn5B.join(dfn6B, how='outer')\n",
    "dfn10B = dfn7B.join(dfn8B, how='outer')\n",
    "dfn11B = dfn9B.join(dfn10B, how='outer')\n",
    "df_PressureFilter_Param_B = dfn11B\n",
    "\n",
    "dfn11B['Cycle_Count'] = np.arange(1, len(dfn11B['startdate'])+1)\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABORATORY ANALYSIS RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "wb1 = openpyxl.load_workbook(r'\\\\thgobna001\\userdata\\THPAL\\Department\\TAGANITO\\SHARED\\AnalysisResult\\ANALYSIS RESULTS 2020\\JANUARY 2020.xlsx')\n",
    "\n",
    "#####This block of code appends the Analysis Results from January2020\n",
    "FT02wb = wb1['106FT02AB']\n",
    "#update for the targetmonth\n",
    "currentmonthyr = datetime.date(2020,1,1)\n",
    "print('Done', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------\n",
    "#collectst the index for the date.day the sample was taken and puts it in the list Smpl_day_index\n",
    "Smpl_day_index = list()\n",
    "Smpl_day_index[:] = []\n",
    "for i in range (1, FT02wb.max_row):\n",
    "    if type(FT02wb['B'+ str(i)].value) == int:\n",
    "        #print(str(i)+':'+str(TH01_UF_sheet['B'+ str(i)].value))\n",
    "        Smpl_day_index.append(i)\n",
    "\n",
    "#smpl_end_index is the last row index for the date.day the sample was taken\n",
    "for i in range(1, FT02wb.max_row):\n",
    "    if str(FT02wb['B'+str(i)].value).split(' ')[0] == 'Daily':\n",
    "        smpl_end_index = i\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "#Pb\n",
    "#Code for creating a dataframe for MS_A analysis result for Element Pb\n",
    "A_Pb_datelist = list()\n",
    "A_Pb_datelist[:] = []\n",
    "B_Pb_datelist = list()\n",
    "B_Pb_datelist[:] = []\n",
    "\n",
    "A_Pb_list = list()\n",
    "A_Pb_list[:] = []\n",
    "B_Pb_list = list()\n",
    "B_Pb_list[:] = []\n",
    "\n",
    "A_Pb_list_pair = list()\n",
    "A_Pb_list_pair[:] = []\n",
    "B_Pb_list_pair = list()\n",
    "B_Pb_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':  \n",
    "                    \n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    \n",
    "                    A_Pb_datelist.append(parse(A_date))  \n",
    "                    A_Pb = FT02wb['T'+str(x)].value\n",
    "                    A_Pb_list.append(A_Pb)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_datelist)\n",
    "                    B_Pb_datelist.append(parse(B_date)) \n",
    "                    B_Pb = FT02wb['T'+str(x)].value\n",
    "                    B_Pb_list.append(B_Pb)\n",
    "             \n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(A_date) \n",
    "                    A_Pb_datelist.append(parse(A_date))\n",
    "                    A_Pb = FT02wb['T'+str(x)].value\n",
    "                    A_Pb_list.append(A_Pb)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_date)\n",
    "                    B_Pb_datelist.append(parse(B_date))\n",
    "                    B_Pb = FT02wb['T'+str(x)].value\n",
    "                    B_Pb_list.append(B_Pb)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_Pb_list_pair = list(zip(A_Pb_list, A_Pb_datelist[0:]))\n",
    "A_Pb_df = pd.DataFrame(A_Pb_list_pair)\n",
    "A_Pb_df.columns = ['MS_A_Pb', 'DATETIME'] ##check if list title is updated\n",
    "A_Pb_df.set_index('DATETIME', inplace=True)\n",
    "A_Pb_df = A_Pb_df.loc[~A_Pb_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "B_Pb_list_pair = list(zip(B_Pb_list, B_Pb_datelist[0:]))\n",
    "B_Pb_df = pd.DataFrame(B_Pb_list_pair)\n",
    "B_Pb_df.columns = ['MS_B_Pb', 'DATETIME'] ##check if list title is updated\n",
    "\n",
    "B_Pb_df.set_index('DATETIME', inplace=True)\n",
    "B_Pb_df = B_Pb_df.loc[~B_Pb_df.index.duplicated(keep = 'last')]\n",
    "#-----End for Pb code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zn\n",
    "#Code for creating a dataframe for MS_A analysis result for Element Zn\n",
    "A_Zn_datelist = list()\n",
    "A_Zn_datelist[:] = []\n",
    "B_Zn_datelist = list()\n",
    "B_Zn_datelist[:] = []\n",
    "\n",
    "A_Zn_list = list()\n",
    "A_Zn_list[:] = []\n",
    "B_Zn_list = list()\n",
    "B_Zn_list[:] = []\n",
    "\n",
    "A_Zn_list_pair = list()\n",
    "A_Zn_list_pair[:] = []\n",
    "B_Zn_list_pair = list()\n",
    "B_Zn_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(parse(A_date))\n",
    "                    A_Zn_datelist.append(parse(A_date))  \n",
    "                    A_Zn = FT02wb['U'+str(x)].value\n",
    "                    A_Zn_list.append(A_Zn)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_datelist)\n",
    "                    B_Zn_datelist.append(parse(B_date)) \n",
    "                    B_Zn = FT02wb['U'+str(x)].value\n",
    "                    B_Zn_list.append(B_Zn)\n",
    "\n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(A_date) \n",
    "                    A_Zn_datelist.append(parse(A_date))\n",
    "                    A_Zn = FT02wb['U'+str(x)].value\n",
    "                    A_Zn_list.append(A_Zn)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_date)\n",
    "                    B_Zn_datelist.append(parse(B_date))\n",
    "                    B_Zn = FT02wb['U'+str(x)].value\n",
    "                    B_Zn_list.append(B_Zn)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_Zn_list_pair = list(zip(A_Zn_list, A_Zn_datelist[0:]))\n",
    "A_Zn_df = pd.DataFrame(A_Zn_list_pair)\n",
    "A_Zn_df.columns = ['MS_A_Zn', 'DATETIME'] ##check if list title is updated\n",
    "A_Zn_df.set_index('DATETIME', inplace=True)\n",
    "A_Zn_df = A_Zn_df.loc[~A_Zn_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "\n",
    "B_Zn_list_pair = list(zip(B_Zn_list, B_Zn_datelist[0:]))\n",
    "B_Zn_df = pd.DataFrame(B_Zn_list_pair)\n",
    "B_Zn_df.columns = ['MS_B_Zn', 'DATETIME'] ##check if list title is updated\n",
    "B_Zn_df.set_index('DATETIME', inplace=True)\n",
    "B_Zn_df = B_Zn_df.loc[~B_Zn_df.index.duplicated(keep = 'last')]\n",
    "#-----End for Zn code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Cu\n",
    "#Code for creating a dataframe for MS_A analysis result for Element Cu\n",
    "A_Cu_datelist = list()\n",
    "A_Cu_datelist[:] = []\n",
    "B_Cu_datelist = list()\n",
    "B_Cu_datelist[:] = []\n",
    "\n",
    "A_Cu_list = list()\n",
    "A_Cu_list[:] = []\n",
    "B_Cu_list = list()\n",
    "B_Cu_list[:] = []\n",
    "\n",
    "A_Cu_list_pair = list()\n",
    "A_Cu_list_pair[:] = []\n",
    "B_Cu_list_pair = list()\n",
    "B_Cu_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(parse(A_date))\n",
    "                    A_Cu_datelist.append(parse(A_date))  \n",
    "                    A_Cu = FT02wb['V'+str(x)].value\n",
    "                    A_Cu_list.append(A_Cu)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_datelist)\n",
    "                    B_Cu_datelist.append(parse(B_date)) \n",
    "                    B_Cu = FT02wb['V'+str(x)].value\n",
    "                    B_Cu_list.append(B_Cu)\n",
    "\n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(A_date) \n",
    "                    A_Cu_datelist.append(parse(A_date))\n",
    "                    A_Cu = FT02wb['V'+str(x)].value\n",
    "                    A_Cu_list.append(A_Cu)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_date)\n",
    "                    B_Cu_datelist.append(parse(B_date))\n",
    "                    B_Cu = FT02wb['V'+str(x)].value\n",
    "                    B_Cu_list.append(B_Cu)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_Cu_list_pair = list(zip(A_Cu_list, A_Cu_datelist[0:]))\n",
    "A_Cu_df = pd.DataFrame(A_Cu_list_pair)\n",
    "A_Cu_df.columns = ['MS_A_Cu', 'DATETIME'] ##check if list title is updated\n",
    "A_Cu_df.set_index('DATETIME', inplace=True)\n",
    "A_Cu_df = A_Cu_df.loc[~A_Cu_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "B_Cu_list_pair = list(zip(B_Cu_list, B_Cu_datelist[0:]))\n",
    "B_Cu_df = pd.DataFrame(B_Cu_list_pair)\n",
    "B_Cu_df.columns = ['MS_B_Cu', 'DATETIME'] ##check if list title is updated\n",
    "B_Cu_df.set_index('DATETIME', inplace=True)\n",
    "B_Cu_df = B_Cu_df.loc[~B_Cu_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "#-----End for Cu code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ni\n",
    "#Code for creating a dataframe for MS_A analysis result for Element Ni\n",
    "A_Ni_datelist = list()\n",
    "A_Ni_datelist[:] = []\n",
    "B_Ni_datelist = list()\n",
    "B_Ni_datelist[:] = []\n",
    "\n",
    "A_Ni_list = list()\n",
    "A_Ni_list[:] = []\n",
    "B_Ni_list = list()\n",
    "B_Ni_list[:] = []\n",
    "\n",
    "A_Ni_list_pair = list()\n",
    "A_Ni_list_pair[:] = []\n",
    "B_Ni_list_pair = list()\n",
    "B_Ni_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    A_Ni_datelist.append(parse(A_date))  \n",
    "                    A_Ni = FT02wb['W'+str(x)].value\n",
    "                    A_Ni_list.append(A_Ni)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                  \n",
    "                    B_Ni_datelist.append(parse(B_date)) \n",
    "                    B_Ni = FT02wb['W'+str(x)].value\n",
    "                    B_Ni_list.append(B_Ni)\n",
    "\n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    A_Ni_datelist.append(parse(A_date))\n",
    "                    A_Ni = FT02wb['W'+str(x)].value\n",
    "                    A_Ni_list.append(A_Ni)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    B_Ni_datelist.append(parse(B_date))\n",
    "                    B_Ni = FT02wb['W'+str(x)].value\n",
    "                    B_Ni_list.append(B_Ni)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_Ni_list_pair = list(zip(A_Ni_list, A_Ni_datelist[0:]))\n",
    "A_Ni_df = pd.DataFrame(A_Ni_list_pair)\n",
    "A_Ni_df.columns = ['MS_A_Ni', 'DATETIME'] ##check if list title is updated\n",
    "A_Ni_df.set_index('DATETIME', inplace=True)\n",
    "A_Ni_df = A_Ni_df.loc[~A_Ni_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "B_Ni_list_pair = list(zip(B_Ni_list, B_Ni_datelist[0:]))\n",
    "B_Ni_df = pd.DataFrame(B_Ni_list_pair)\n",
    "B_Ni_df.columns = ['MS_B_Ni', 'DATETIME'] ##check if list title is updated\n",
    "B_Ni_df.set_index('DATETIME', inplace=True)\n",
    "B_Ni_df = B_Ni_df.loc[~B_Ni_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "#-----End for Ni code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Co\n",
    "#Code for creating a dataframe for MS_A analysis result for Element Co\n",
    "A_Co_datelist = list()\n",
    "A_Co_datelist[:] = []\n",
    "B_Co_datelist = list()\n",
    "B_Co_datelist[:] = []\n",
    "\n",
    "A_Co_list = list()\n",
    "A_Co_list[:] = []\n",
    "B_Co_list = list()\n",
    "B_Co_list[:] = []\n",
    "\n",
    "A_Co_list_pair = list()\n",
    "A_Co_list_pair[:] = []\n",
    "B_Co_list_pair = list()\n",
    "B_Co_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(parse(A_date))\n",
    "                    A_Co_datelist.append(parse(A_date))  \n",
    "                    A_Co = FT02wb['X'+str(x)].value\n",
    "                    A_Co_list.append(A_Co)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_datelist)\n",
    "                    B_Co_datelist.append(parse(B_date)) \n",
    "                    B_Co = FT02wb['X'+str(x)].value\n",
    "                    B_Co_list.append(B_Co)\n",
    "             \n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(A_date) \n",
    "                    A_Co_datelist.append(parse(A_date))\n",
    "                    A_Co = FT02wb['X'+str(x)].value\n",
    "                    A_Co_list.append(A_Co)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_date)\n",
    "                    B_Co_datelist.append(parse(B_date))\n",
    "                    B_Co = FT02wb['X'+str(x)].value\n",
    "                    B_Co_list.append(B_Co)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_Co_list_pair = list(zip(A_Co_list, A_Co_datelist[0:]))\n",
    "A_Co_df = pd.DataFrame(A_Co_list_pair)\n",
    "A_Co_df.columns = ['MS_A_Co', 'DATETIME'] ##check if list title is updated\n",
    "A_Co_df.set_index('DATETIME', inplace=True)\n",
    "A_Co_df = A_Co_df.loc[~A_Co_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "B_Co_list_pair = list(zip(B_Co_list, B_Co_datelist[0:]))\n",
    "B_Co_df = pd.DataFrame(B_Co_list_pair)\n",
    "B_Co_df.columns = ['MS_B_Co', 'DATETIME'] ##check if list title is updated\n",
    "B_Co_df.set_index('DATETIME', inplace=True)\n",
    "B_Co_df = B_Co_df.loc[~B_Co_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "#-----End for Co code\n",
    "\n",
    "#Fe\n",
    "#Fede for creating a dataframe for MS_A analysis result for Element Fe\n",
    "A_Fe_datelist = list()\n",
    "A_Fe_datelist[:] = []\n",
    "B_Fe_datelist = list()\n",
    "B_Fe_datelist[:] = []\n",
    "\n",
    "A_Fe_list = list()\n",
    "A_Fe_list[:] = []\n",
    "B_Fe_list = list()\n",
    "B_Fe_list[:] = []\n",
    "\n",
    "A_Fe_list_pair = list()\n",
    "A_Fe_list_pair[:] = []\n",
    "B_Fe_list_pair = list()\n",
    "B_Fe_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(parse(A_date))\n",
    "                    A_Fe_datelist.append(parse(A_date))  \n",
    "                    A_Fe = FT02wb['Y'+str(x)].value\n",
    "                    A_Fe_list.append(A_Fe)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_datelist)\n",
    "                    B_Fe_datelist.append(parse(B_date)) \n",
    "                    B_Fe = FT02wb['Y'+str(x)].value\n",
    "                    B_Fe_list.append(B_Fe)\n",
    "\n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(A_date) \n",
    "                    A_Fe_datelist.append(parse(A_date))\n",
    "                    A_Fe = FT02wb['Y'+str(x)].value\n",
    "                    A_Fe_list.append(A_Fe)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_date)\n",
    "                    B_Fe_datelist.append(parse(B_date))\n",
    "                    B_Fe = FT02wb['Y'+str(x)].value\n",
    "                    B_Fe_list.append(B_Fe)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_Fe_list_pair = list(zip(A_Fe_list, A_Fe_datelist[0:]))\n",
    "A_Fe_df = pd.DataFrame(A_Fe_list_pair)\n",
    "A_Fe_df.columns = ['MS_A_Fe', 'DATETIME'] ##check if list title is updated\n",
    "A_Fe_df.set_index('DATETIME', inplace=True)\n",
    "A_Fe_df = A_Fe_df.loc[~A_Fe_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "B_Fe_list_pair = list(zip(B_Fe_list, B_Fe_datelist[0:]))\n",
    "B_Fe_df = pd.DataFrame(B_Fe_list_pair)\n",
    "B_Fe_df.columns = ['MS_B_Fe', 'DATETIME'] ##check if list title is updated\n",
    "B_Fe_df.set_index('DATETIME', inplace=True)\n",
    "B_Fe_df = B_Fe_df.loc[~B_Fe_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "#-----End for Fe code\n",
    "\n",
    "#Mn\n",
    "#Mnde for creating a dataframe for MS_A analysis result for Element Mn\n",
    "A_Mn_datelist = list()\n",
    "A_Mn_datelist[:] = []\n",
    "B_Mn_datelist = list()\n",
    "B_Mn_datelist[:] = []\n",
    "\n",
    "A_Mn_list = list()\n",
    "A_Mn_list[:] = []\n",
    "B_Mn_list = list()\n",
    "B_Mn_list[:] = []\n",
    "\n",
    "A_Mn_list_pair = list()\n",
    "A_Mn_list_pair[:] = []\n",
    "B_Mn_list_pair = list()\n",
    "B_Mn_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(parse(A_date))\n",
    "                    A_Mn_datelist.append(parse(A_date))  \n",
    "                    A_Mn = FT02wb['Z'+str(x)].value\n",
    "                    A_Mn_list.append(A_Mn)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_datelist)\n",
    "                    B_Mn_datelist.append(parse(B_date)) \n",
    "                    B_Mn = FT02wb['Z'+str(x)].value\n",
    "                    B_Mn_list.append(B_Mn)\n",
    "             \n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(A_date) \n",
    "                    A_Mn_datelist.append(parse(A_date))\n",
    "                    A_Mn = FT02wb['Z'+str(x)].value\n",
    "                    A_Mn_list.append(A_Mn)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_date)\n",
    "                    B_Mn_datelist.append(parse(B_date))\n",
    "                    B_Mn = FT02wb['Z'+str(x)].value\n",
    "                    B_Mn_list.append(B_Mn)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_Mn_list_pair = list(zip(A_Mn_list, A_Mn_datelist[0:]))\n",
    "A_Mn_df = pd.DataFrame(A_Mn_list_pair)\n",
    "A_Mn_df.columns = ['MS_A_Mn', 'DATETIME'] ##check if list title is updated\n",
    "A_Mn_df.set_index('DATETIME', inplace=True)\n",
    "A_Mn_df = A_Mn_df.loc[~A_Mn_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "\n",
    "B_Mn_list_pair = list(zip(B_Mn_list, B_Mn_datelist[0:]))\n",
    "B_Mn_df = pd.DataFrame(B_Mn_list_pair)\n",
    "B_Mn_df.columns = ['MS_B_Mn', 'DATETIME'] ##check if list title is updated\n",
    "B_Mn_df.set_index('DATETIME', inplace=True)\n",
    "B_Mn_df = B_Mn_df.loc[~B_Mn_df.index.duplicated(keep = 'last')]\n",
    "#-----End for Mn code\n",
    "\n",
    "#Cr\n",
    "#Crde for creating a dataframe for MS_A analysis result for Element Cr\n",
    "A_Cr_datelist = list()\n",
    "A_Cr_datelist[:] = []\n",
    "B_Cr_datelist = list()\n",
    "B_Cr_datelist[:] = []\n",
    "\n",
    "A_Cr_list = list()\n",
    "A_Cr_list[:] = []\n",
    "B_Cr_list = list()\n",
    "B_Cr_list[:] = []\n",
    "\n",
    "A_Cr_list_pair = list()\n",
    "A_Cr_list_pair[:] = []\n",
    "B_Cr_list_pair = list()\n",
    "B_Cr_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(parse(A_date))\n",
    "                    A_Cr_datelist.append(parse(A_date))  \n",
    "                    A_Cr = FT02wb['AA'+str(x)].value\n",
    "                    A_Cr_list.append(A_Cr)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_datelist)\n",
    "                    B_Cr_datelist.append(parse(B_date)) \n",
    "                    B_Cr = FT02wb['AA'+str(x)].value\n",
    "                    B_Cr_list.append(B_Cr)\n",
    "\n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(A_date) \n",
    "                    A_Cr_datelist.append(parse(A_date))\n",
    "                    A_Cr = FT02wb['AA'+str(x)].value\n",
    "                    A_Cr_list.append(A_Cr)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_date)\n",
    "                    B_Cr_datelist.append(parse(B_date))\n",
    "                    B_Cr = FT02wb['AA'+str(x)].value\n",
    "                    B_Cr_list.append(B_Cr)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_Cr_list_pair = list(zip(A_Cr_list, A_Cr_datelist[0:]))\n",
    "A_Cr_df = pd.DataFrame(A_Cr_list_pair)\n",
    "A_Cr_df.columns = ['MS_A_Cr', 'DATETIME'] ##check if list title is updated\n",
    "A_Cr_df.set_index('DATETIME', inplace=True)\n",
    "A_Cr_df = A_Cr_df.loc[~A_Cr_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "\n",
    "\n",
    "B_Cr_list_pair = list(zip(B_Cr_list, B_Cr_datelist[0:]))\n",
    "B_Cr_df = pd.DataFrame(B_Cr_list_pair)\n",
    "B_Cr_df.columns = ['MS_B_Cr', 'DATETIME'] ##check if list title is updated\n",
    "B_Cr_df.set_index('DATETIME', inplace=True)\n",
    "B_Cr_df = B_Cr_df.loc[~B_Cr_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "#-----End for Cr code\n",
    "\n",
    "#Ca\n",
    "#Cade for creating a dataframe for MS_A analysis result for Element Ca\n",
    "A_Ca_datelist = list()\n",
    "A_Ca_datelist[:] = []\n",
    "B_Ca_datelist = list()\n",
    "B_Ca_datelist[:] = []\n",
    "\n",
    "A_Ca_list = list()\n",
    "A_Ca_list[:] = []\n",
    "B_Ca_list = list()\n",
    "B_Ca_list[:] = []\n",
    "\n",
    "A_Ca_list_pair = list()\n",
    "A_Ca_list_pair[:] = []\n",
    "B_Ca_list_pair = list()\n",
    "B_Ca_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(parse(A_date))\n",
    "                    A_Ca_datelist.append(parse(A_date))  \n",
    "                    A_Ca = FT02wb['AB'+str(x)].value\n",
    "                    A_Ca_list.append(A_Ca)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_datelist)\n",
    "                    B_Ca_datelist.append(parse(B_date)) \n",
    "                    B_Ca = FT02wb['AB'+str(x)].value\n",
    "                    B_Ca_list.append(B_Ca)\n",
    "             \n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(A_date) \n",
    "                    A_Ca_datelist.append(parse(A_date))\n",
    "                    A_Ca = FT02wb['AB'+str(x)].value\n",
    "                    A_Ca_list.append(A_Ca)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_date)\n",
    "                    B_Ca_datelist.append(parse(B_date))\n",
    "                    B_Ca = FT02wb['AB'+str(x)].value\n",
    "                    B_Ca_list.append(B_Ca)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_Ca_list_pair = list(zip(A_Ca_list, A_Ca_datelist[0:]))\n",
    "A_Ca_df = pd.DataFrame(A_Ca_list_pair)\n",
    "A_Ca_df.columns = ['MS_A_Ca', 'DATETIME'] ##check if list title is updated\n",
    "A_Ca_df.set_index('DATETIME', inplace=True)\n",
    "A_Ca_df = A_Ca_df.loc[~A_Ca_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "\n",
    "\n",
    "B_Ca_list_pair = list(zip(B_Ca_list, B_Ca_datelist[0:]))\n",
    "B_Ca_df = pd.DataFrame(B_Ca_list_pair)\n",
    "B_Ca_df.columns = ['MS_B_Ca', 'DATETIME'] ##check if list title is updated\n",
    "B_Ca_df.set_index('DATETIME', inplace=True)\n",
    "B_Ca_df = B_Ca_df.loc[~B_Ca_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "\n",
    "#-----End for Ca code\n",
    "\n",
    "#Si\n",
    "#Side for creating a dataframe for MS_A analysis result for Element Si\n",
    "A_Si_datelist = list()\n",
    "A_Si_datelist[:] = []\n",
    "B_Si_datelist = list()\n",
    "B_Si_datelist[:] = []\n",
    "\n",
    "A_Si_list = list()\n",
    "A_Si_list[:] = []\n",
    "B_Si_list = list()\t\n",
    "B_Si_list[:] = []\n",
    "\n",
    "A_Si_list_pair = list()\n",
    "A_Si_list_pair[:] = []\n",
    "B_Si_list_pair = list()\n",
    "B_Si_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(parse(A_date))\n",
    "                    A_Si_datelist.append(parse(A_date))  \n",
    "                    A_Si = FT02wb['AC'+str(x)].value\n",
    "                    A_Si_list.append(A_Si)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_datelist)\n",
    "                    B_Si_datelist.append(parse(B_date)) \n",
    "                    B_Si = FT02wb['AC'+str(x)].value\n",
    "                    B_Si_list.append(B_Si)\n",
    "             \n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(A_date) \n",
    "                    A_Si_datelist.append(parse(A_date))\n",
    "                    A_Si = FT02wb['AC'+str(x)].value\n",
    "                    A_Si_list.append(A_Si)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_date)\n",
    "                    B_Si_datelist.append(parse(B_date))\n",
    "                    B_Si = FT02wb['AC'+str(x)].value\n",
    "                    B_Si_list.append(B_Si)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_Si_list_pair = list(zip(A_Si_list, A_Si_datelist[0:]))\n",
    "A_Si_df = pd.DataFrame(A_Si_list_pair)\n",
    "A_Si_df.columns = ['MS_A_Si', 'DATETIME'] ##check if list title is updated\n",
    "A_Si_df.set_index('DATETIME', inplace=True)\n",
    "A_Si_df = A_Si_df.loc[~A_Si_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "\n",
    "\n",
    "B_Si_list_pair = list(zip(B_Si_list, B_Si_datelist[0:]))\n",
    "B_Si_df = pd.DataFrame(B_Si_list_pair)\n",
    "B_Si_df.columns = ['MS_B_Si', 'DATETIME'] ##check if list title is updated\n",
    "B_Si_df.set_index('DATETIME', inplace=True)\n",
    "B_Si_df = B_Si_df.loc[~B_Si_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "#-----End for Si code\n",
    "\n",
    "#Al\n",
    "#Alde for creating a dataframe for MS_A analysis result for Element Al\n",
    "A_Al_datelist = list()\n",
    "A_Al_datelist[:] = []\n",
    "B_Al_datelist = list()\n",
    "B_Al_datelist[:] = []\n",
    "\n",
    "A_Al_list = list()\n",
    "A_Al_list[:] = []\n",
    "B_Al_list = list()\t\n",
    "B_Al_list[:] = []\n",
    "\n",
    "A_Al_list_pair = list()\n",
    "A_Al_list_pair[:] = []\n",
    "B_Al_list_pair = list()\n",
    "B_Al_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(parse(A_date))\n",
    "                    A_Al_datelist.append(parse(A_date))  \n",
    "                    A_Al = FT02wb['AD'+str(x)].value\n",
    "                    A_Al_list.append(A_Al)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_datelist)\n",
    "                    B_Al_datelist.append(parse(B_date)) \n",
    "                    B_Al = FT02wb['AD'+str(x)].value\n",
    "                    B_Al_list.append(B_Al)\n",
    "             \n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(A_date) \n",
    "                    A_Al_datelist.append(parse(A_date))\n",
    "                    A_Al = FT02wb['AD'+str(x)].value\n",
    "                    A_Al_list.append(A_Al)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_date)\n",
    "                    B_Al_datelist.append(parse(B_date))\n",
    "                    B_Al = FT02wb['AD'+str(x)].value\n",
    "                    B_Al_list.append(B_Al)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_Al_list_pair = list(zip(A_Al_list, A_Al_datelist[0:]))\n",
    "A_Al_df = pd.DataFrame(A_Al_list_pair)\n",
    "A_Al_df.columns = ['MS_A_Al', 'DATETIME'] ##check if list title is updated\n",
    "A_Al_df.set_index('DATETIME', inplace=True)\n",
    "A_Al_df = A_Al_df.loc[~A_Al_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "\n",
    "\n",
    "B_Al_list_pair = list(zip(B_Al_list, B_Al_datelist[0:]))\n",
    "B_Al_df = pd.DataFrame(B_Al_list_pair)\n",
    "B_Al_df.columns = ['MS_B_Al', 'DATETIME'] ##check if list title is updated\n",
    "B_Al_df.set_index('DATETIME', inplace=True)\n",
    "B_Al_df = B_Al_df.loc[~B_Al_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "#-----End for Al code\n",
    "\n",
    "#Mg\n",
    "#Mgde for creating a dataframe for MS_A analysis result for Element Mg\n",
    "A_Mg_datelist = list()\n",
    "A_Mg_datelist[:] = []\n",
    "B_Mg_datelist = list()\n",
    "B_Mg_datelist[:] = []\n",
    "\n",
    "A_Mg_list = list()\n",
    "A_Mg_list[:] = []\n",
    "B_Mg_list = list()\t\n",
    "B_Mg_list[:] = []\n",
    "\n",
    "A_Mg_list_pair = list()\n",
    "A_Mg_list_pair[:] = []\n",
    "B_Mg_list_pair = list()\n",
    "B_Mg_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(parse(A_date))\n",
    "                    A_Mg_datelist.append(parse(A_date))  \n",
    "                    A_Mg = FT02wb['AE'+str(x)].value\n",
    "                    A_Mg_list.append(A_Mg)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_datelist)\n",
    "                    B_Mg_datelist.append(parse(B_date)) \n",
    "                    B_Mg = FT02wb['AE'+str(x)].value\n",
    "                    B_Mg_list.append(B_Mg)\n",
    "\n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(A_date) \n",
    "                    A_Mg_datelist.append(parse(A_date))\n",
    "                    A_Mg = FT02wb['AE'+str(x)].value\n",
    "                    A_Mg_list.append(A_Mg)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_date)\n",
    "                    B_Mg_datelist.append(parse(B_date))\n",
    "                    B_Mg = FT02wb['AE'+str(x)].value\n",
    "                    B_Mg_list.append(B_Mg)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_Mg_list_pair = list(zip(A_Mg_list, A_Mg_datelist[0:]))\n",
    "A_Mg_df = pd.DataFrame(A_Mg_list_pair)\n",
    "A_Mg_df.columns = ['MS_A_Mg', 'DATETIME'] ##check if list title is updated\n",
    "A_Mg_df.set_index('DATETIME', inplace=True)\n",
    "A_Mg_df = A_Mg_df.loc[~A_Mg_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "\n",
    "B_Mg_list_pair = list(zip(B_Mg_list, B_Mg_datelist[0:]))\n",
    "B_Mg_df = pd.DataFrame(B_Mg_list_pair)\n",
    "B_Mg_df.columns = ['MS_B_Mg', 'DATETIME'] ##check if list title is updated\n",
    "B_Mg_df.set_index('DATETIME', inplace=True)\n",
    "B_Mg_df = B_Mg_df.loc[~B_Mg_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "#-----End for Mg code\n",
    "\n",
    "#S\n",
    "#Sde for creating a dataframe for MS_A analysis result for Element S\n",
    "A_S_datelist = list()\n",
    "A_S_datelist[:] = []\n",
    "B_S_datelist = list()\n",
    "B_S_datelist[:] = []\n",
    "\n",
    "A_S_list = list()\n",
    "A_S_list[:] = []\t\n",
    "B_S_list = list()\t\n",
    "B_S_list[:] = []\n",
    "\n",
    "A_S_list_pair = list()\n",
    "A_S_list_pair[:] = []\n",
    "B_S_list_pair = list()\n",
    "B_S_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(parse(A_date))\n",
    "                    A_S_datelist.append(parse(A_date))  \n",
    "                    A_S = FT02wb['AF'+str(x)].value\n",
    "                    A_S_list.append(A_S)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_datelist)\n",
    "                    B_S_datelist.append(parse(B_date)) \n",
    "                    B_S = FT02wb['AF'+str(x)].value\n",
    "                    B_S_list.append(B_S)\n",
    "\n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(A_date) \n",
    "                    A_S_datelist.append(parse(A_date))\n",
    "                    A_S = FT02wb['AF'+str(x)].value\n",
    "                    A_S_list.append(A_S)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_date)\n",
    "                    B_S_datelist.append(parse(B_date))\n",
    "                    B_S = FT02wb['AF'+str(x)].value\n",
    "                    B_S_list.append(B_S)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_S_list_pair = list(zip(A_S_list, A_S_datelist[0:]))\n",
    "A_S_df = pd.DataFrame(A_S_list_pair)\n",
    "A_S_df.columns = ['MS_A_S', 'DATETIME'] ##check if list title is updated\n",
    "A_S_df.set_index('DATETIME', inplace=True)\n",
    "A_S_df = A_S_df.loc[~A_S_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "\n",
    "B_S_list_pair = list(zip(B_S_list, B_S_datelist[0:]))\n",
    "B_S_df = pd.DataFrame(B_S_list_pair)\n",
    "B_S_df.columns = ['MS_B_S', 'DATETIME'] ##check if list title is updated\n",
    "B_S_df.set_index('DATETIME', inplace=True)\n",
    "B_S_df = B_S_df.loc[~B_S_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "#-----End for S code\n",
    "\n",
    "#S\n",
    "#Sde for creating a dataframe for MS_A analysis result for Element S\n",
    "A_h2o_datelist = list()\n",
    "A_h2o_datelist[:] = []\n",
    "B_h2o_datelist = list()\n",
    "B_h2o_datelist[:] = []\n",
    "\n",
    "A_h2o_list = list()\n",
    "A_h2o_list[:] = []\t\n",
    "B_h2o_list = list()\t\n",
    "B_h2o_list[:] = []\n",
    "\n",
    "A_h2o_list_pair = list()\n",
    "A_h2o_list_pair[:] = []\n",
    "B_h2o_list_pair = list()\n",
    "B_h2o_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(parse(A_date))\n",
    "                    A_h2o_datelist.append(parse(A_date))  \n",
    "                    A_h2o = FT02wb['AP'+str(x)].value\n",
    "                    A_h2o_list.append(A_h2o)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_datelist)\n",
    "                    B_h2o_datelist.append(parse(B_date)) \n",
    "                    B_h2o = FT02wb['AP'+str(x)].value\n",
    "                    B_h2o_list.append(B_h2o)\n",
    "             \n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(A_date) \n",
    "                    A_h2o_datelist.append(parse(A_date))\n",
    "                    A_h2o = FT02wb['AP'+str(x)].value\n",
    "                    B_h2o_list.append(A_h2o)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_date)\n",
    "                    B_h2o_datelist.append(parse(B_date))\n",
    "                    B_h2o = FT02wb['AP'+str(x)].value\n",
    "                    B_h2o_list.append(B_h2o)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_h2o_list_pair = list(zip(A_h2o_list, A_h2o_datelist[0:]))\n",
    "A_h2o_df = pd.DataFrame(A_h2o_list_pair)\n",
    "A_h2o_df.columns = ['MS_A_moisture', 'DATETIME'] ##check if list title is updated\n",
    "A_h2o_df.set_index('DATETIME', inplace=True)\n",
    "A_h2o_df = A_h2o_df.loc[~A_h2o_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "\n",
    "\n",
    "B_h2o_list_pair = list(zip(B_h2o_list, B_h2o_datelist[0:]))\n",
    "B_h2o_df = pd.DataFrame(B_h2o_list_pair)\n",
    "B_h2o_df.columns = ['MS_B_moisture', 'DATETIME'] ##check if list title is updated\n",
    "B_h2o_df.set_index('DATETIME', inplace=True)\n",
    "B_h2o_df = B_h2o_df.loc[~B_h2o_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "#-----End for S code\n",
    "\n",
    "#PS_10D\n",
    "#PS_10Dde for creating a dataframe for MS_A analysis result for Element PS_9D\n",
    "A_PS_10D_datelist = list()\n",
    "A_PS_10D_datelist[:] = []\n",
    "B_PS_10D_datelist = list()\n",
    "B_PS_10D_datelist[:] = []\n",
    "\n",
    "A_PS_10D_list = list()\n",
    "A_PS_10D_list[:] = []\n",
    "B_PS_10D_list = list()\t\n",
    "B_PS_10D_list[:] = []\t\n",
    "\n",
    "A_PS_10D_list_pair = list()\n",
    "A_PS_10D_list_pair[:] = []\n",
    "B_PS_10D_list_pair = list()\n",
    "B_PS_10D_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(parse(A_date))\n",
    "                    A_PS_10D_datelist.append(parse(A_date))  \n",
    "                    A_PS_10D = FT02wb['AQ'+str(x)].value\n",
    "                    A_PS_10D_list.append(A_PS_10D)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_datelist)\n",
    "                    B_PS_10D_datelist.append(parse(B_date)) \n",
    "                    B_PS_10D = FT02wb['AQ'+str(x)].value\n",
    "                    B_PS_10D_list.append(B_PS_10D)\n",
    "             \n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(A_date) \n",
    "                    A_PS_10D_datelist.append(parse(A_date))\n",
    "                    A_PS_10D = FT02wb['AQ'+str(x)].value\n",
    "                    A_PS_10D_list.append(A_PS_10D)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_date)\n",
    "                    B_PS_10D_datelist.append(parse(B_date))\n",
    "                    B_PS_10D = FT02wb['AQ'+str(x)].value\n",
    "                    B_PS_10D_list.append(B_PS_10D)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_PS_10D_list_pair = list(zip(A_PS_10D_list, A_PS_10D_datelist[0:]))\n",
    "A_PS_10D_df = pd.DataFrame(A_PS_10D_list_pair)\n",
    "A_PS_10D_df.columns = ['MS_A_PS_10D', 'DATETIME'] ##check if list title is updated\n",
    "A_PS_10D_df.set_index('DATETIME', inplace=True)\n",
    "A_PS_10D_df = A_PS_10D_df.loc[~A_PS_10D_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "\n",
    "\n",
    "B_PS_10D_list_pair = list(zip(B_PS_10D_list, B_PS_10D_datelist[0:]))\n",
    "B_PS_10D_df = pd.DataFrame(B_PS_10D_list_pair)\n",
    "B_PS_10D_df.columns = ['MS_B_PS_10D', 'DATETIME'] ##check if list title is updated\n",
    "B_PS_10D_df.set_index('DATETIME', inplace=True)\n",
    "B_PS_10D_df = B_PS_10D_df.loc[~B_PS_10D_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "#-----End for PS_9D code\n",
    "\n",
    "#PS_50D\n",
    "#PS_50Dde for creating a dataframe for MS_A analysis result for Element PS_50D\n",
    "A_PS_50D_datelist = list()\n",
    "A_PS_50D_datelist[:] = []\n",
    "B_PS_50D_datelist = list()\n",
    "B_PS_50D_datelist[:] = []\n",
    "\n",
    "A_PS_50D_list = list()\n",
    "A_PS_50D_list[:] = []\n",
    "B_PS_50D_list = list()\t\n",
    "B_PS_50D_list[:] = []\t\n",
    "\n",
    "A_PS_50D_list_pair = list()\n",
    "A_PS_50D_list_pair[:] = []\n",
    "B_PS_50D_list_pair = list()\n",
    "B_PS_50D_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(parse(A_date))\n",
    "                    A_PS_50D_datelist.append(parse(A_date))  \n",
    "                    A_PS_50D = FT02wb['AR'+str(x)].value\n",
    "                    A_PS_50D_list.append(A_PS_50D)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_datelist)\n",
    "                    B_PS_50D_datelist.append(parse(B_date)) \n",
    "                    B_PS_50D = FT02wb['AR'+str(x)].value\n",
    "                    B_PS_50D_list.append(B_PS_50D)\n",
    "             \n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(A_date) \n",
    "                    A_PS_50D_datelist.append(parse(A_date))\n",
    "                    A_PS_50D = FT02wb['AR'+str(x)].value\n",
    "                    A_PS_50D_list.append(A_PS_50D)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_date)\n",
    "                    B_PS_50D_datelist.append(parse(B_date))\n",
    "                    B_PS_50D = FT02wb['AR'+str(x)].value\n",
    "                    B_PS_50D_list.append(B_PS_50D)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_PS_50D_list_pair = list(zip(A_PS_50D_list, A_PS_50D_datelist[0:]))\n",
    "A_PS_50D_df = pd.DataFrame(A_PS_50D_list_pair)\n",
    "A_PS_50D_df.columns = ['MS_A_PS_50D', 'DATETIME'] ##check if list title is updated\n",
    "A_PS_50D_df.set_index('DATETIME', inplace=True)\n",
    "A_PS_50D_df = A_PS_50D_df.loc[~A_PS_50D_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "\n",
    "B_PS_50D_list_pair = list(zip(B_PS_50D_list, B_PS_50D_datelist[0:]))\n",
    "B_PS_50D_df = pd.DataFrame(B_PS_50D_list_pair)\n",
    "B_PS_50D_df.columns = ['MS_B_PS_50D', 'DATETIME'] ##check if list title is updated\n",
    "B_PS_50D_df.set_index('DATETIME', inplace=True)\n",
    "B_PS_50D_df = B_PS_50D_df.loc[~B_PS_50D_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "#-----End for PS_50D code\n",
    "\n",
    "#PS_90D\n",
    "#PS_90Dde for creating a dataframe for MS_A analysis result for Element PS_90D\n",
    "A_PS_90D_datelist = list()\n",
    "A_PS_90D_datelist[:] = []\n",
    "B_PS_90D_datelist = list()\n",
    "B_PS_90D_datelist[:] = []\n",
    "\n",
    "A_PS_90D_list = list()\n",
    "A_PS_90D_list[:] = []\n",
    "B_PS_90D_list = list()\t\n",
    "B_PS_90D_list[:] = []\t\n",
    "\n",
    "A_PS_90D_list_pair = list()\n",
    "A_PS_90D_list_pair[:] = []\n",
    "B_PS_90D_list_pair = list()\n",
    "B_PS_90D_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(parse(A_date))\n",
    "                    A_PS_90D_datelist.append(parse(A_date))  \n",
    "                    A_PS_90D = FT02wb['AS'+str(x)].value\n",
    "                    A_PS_90D_list.append(A_PS_90D)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_datelist)\n",
    "                    B_PS_90D_datelist.append(parse(B_date)) \n",
    "                    B_PS_90D = FT02wb['AS'+str(x)].value\n",
    "                    B_PS_90D_list.append(B_PS_90D)\n",
    "             \n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(A_date) \n",
    "                    A_PS_90D_datelist.append(parse(A_date))\n",
    "                    A_PS_90D = FT02wb['AS'+str(x)].value\n",
    "                    A_PS_90D_list.append(A_PS_90D)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_date)\n",
    "                    B_PS_90D_datelist.append(parse(B_date))\n",
    "                    B_PS_90D = FT02wb['AS'+str(x)].value\n",
    "                    B_PS_90D_list.append(B_PS_90D)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_PS_90D_list_pair = list(zip(A_PS_90D_list, A_PS_90D_datelist[0:]))\n",
    "A_PS_90D_df = pd.DataFrame(A_PS_90D_list_pair)\n",
    "A_PS_90D_df.columns = ['MS_A_PS_90D', 'DATETIME'] ##check if list title is updated\n",
    "A_PS_90D_df.set_index('DATETIME', inplace=True)\n",
    "A_PS_90D_df = A_PS_90D_df.loc[~A_PS_90D_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "\n",
    "B_PS_90D_list_pair = list(zip(B_PS_90D_list, B_PS_90D_datelist[0:]))\n",
    "B_PS_90D_df = pd.DataFrame(B_PS_90D_list_pair)\n",
    "B_PS_90D_df.columns = ['MS_B_PS_90D', 'DATETIME'] ##check if list title is updated\n",
    "B_PS_90D_df.set_index('DATETIME', inplace=True)\n",
    "B_PS_90D_df = B_PS_90D_df.loc[~B_PS_90D_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "#-----End for PS_90D code\n",
    "\n",
    "#PS_mean_v\n",
    "#PS_mean_vde for creating a dataframe for MS_A analysis result for Element PS_mean_v\n",
    "A_PS_mean_v_datelist = list()\n",
    "A_PS_mean_v_datelist[:] = []\n",
    "B_PS_mean_v_datelist = list()\n",
    "B_PS_mean_v_datelist[:] = []\n",
    "\n",
    "A_PS_mean_v_list = list()\n",
    "A_PS_mean_v_list[:] = []\n",
    "B_PS_mean_v_list = list()\t\n",
    "B_PS_mean_v_list[:] = []\t\n",
    "\n",
    "A_PS_mean_v_list_pair = list()\n",
    "A_PS_mean_v_list_pair[:] = []\n",
    "B_PS_mean_v_list_pair = list()\n",
    "B_PS_mean_v_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(parse(A_date))\n",
    "                    A_PS_mean_v_datelist.append(parse(A_date))  \n",
    "                    A_PS_mean_v = FT02wb['AT'+str(x)].value\n",
    "                    A_PS_mean_v_list.append(A_PS_mean_v)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_datelist)\n",
    "                    B_PS_mean_v_datelist.append(parse(B_date)) \n",
    "                    B_PS_mean_v = FT02wb['AT'+str(x)].value\n",
    "                    B_PS_mean_v_list.append(B_PS_mean_v)\n",
    "             \n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(A_date) \n",
    "                    A_PS_mean_v_datelist.append(parse(A_date))\n",
    "                    A_PS_mean_v = FT02wb['AT'+str(x)].value\n",
    "                    A_PS_mean_v_list.append(A_PS_mean_v)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_date)\n",
    "                    B_PS_mean_v_datelist.append(parse(B_date))\n",
    "                    B_PS_mean_v = FT02wb['AT'+str(x)].value\n",
    "                    B_PS_mean_v_list.append(B_PS_mean_v)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_PS_mean_v_list_pair = list(zip(A_PS_mean_v_list, A_PS_mean_v_datelist[0:]))\n",
    "A_PS_mean_v_df = pd.DataFrame(A_PS_mean_v_list_pair)\n",
    "A_PS_mean_v_df.columns = ['MS_A_PS_mean_v', 'DATETIME'] ##check if list title is updated\n",
    "A_PS_mean_v_df.set_index('DATETIME', inplace=True)\n",
    "A_PS_mean_v_df = A_PS_mean_v_df.loc[~A_PS_mean_v_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "\n",
    "\n",
    "B_PS_mean_v_list_pair = list(zip(B_PS_mean_v_list, B_PS_mean_v_datelist[0:]))\n",
    "B_PS_mean_v_df = pd.DataFrame(B_PS_mean_v_list_pair)\n",
    "B_PS_mean_v_df.columns = ['MS_B_PS_mean_v', 'DATETIME'] ##check if list title is updated\n",
    "B_PS_mean_v_df.set_index('DATETIME', inplace=True)\n",
    "B_PS_mean_v_df = B_PS_mean_v_df.loc[~B_PS_mean_v_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "\n",
    "#-----End for PS_mean_v code\n",
    "\n",
    "#PS_stdev\n",
    "#PS_stdevde for creating a dataframe for MS_A analysis result for Element PS_stdev\n",
    "A_PS_stdev_datelist = list()\n",
    "A_PS_stdev_datelist[:] = []\n",
    "B_PS_stdev_datelist = list()\n",
    "B_PS_stdev_datelist[:] = []\n",
    "\n",
    "A_PS_stdev_list = list()\n",
    "A_PS_stdev_list[:] = []\n",
    "B_PS_stdev_list = list()\t\n",
    "B_PS_stdev_list[:] = []\n",
    "\n",
    "A_PS_stdev_list_pair = list()\n",
    "A_PS_stdev_list_pair[:] = []\n",
    "B_PS_stdev_list_pair = list()\n",
    "B_PS_stdev_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(parse(A_date))\n",
    "                    A_PS_stdev_datelist.append(parse(A_date))  \n",
    "                    A_PS_stdev = FT02wb['AU'+str(x)].value\n",
    "                    A_PS_stdev_list.append(A_PS_stdev)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_datelist)\n",
    "                    B_PS_stdev_datelist.append(parse(B_date)) \n",
    "                    B_PS_stdev = FT02wb['AU'+str(x)].value\n",
    "                    B_PS_stdev_list.append(B_PS_stdev)\n",
    "             \n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(A_date) \n",
    "                    A_PS_stdev_datelist.append(parse(A_date))\n",
    "                    A_PS_stdev = FT02wb['AU'+str(x)].value\n",
    "                    A_PS_stdev_list.append(A_PS_stdev)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_date)\n",
    "                    B_PS_stdev_datelist.append(parse(B_date))\n",
    "                    B_PS_stdev = FT02wb['AU'+str(x)].value\n",
    "                    B_PS_stdev_list.append(B_PS_stdev)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_PS_stdev_list_pair = list(zip(A_PS_stdev_list, A_PS_stdev_datelist[0:]))\n",
    "A_PS_stdev_df = pd.DataFrame(A_PS_stdev_list_pair)\n",
    "A_PS_stdev_df.columns = ['MS_A_PS_stdev', 'DATETIME'] ##check if list title is updated\n",
    "A_PS_stdev_df.set_index('DATETIME', inplace=True)\n",
    "A_PS_stdev_df = A_PS_stdev_df.loc[~A_PS_stdev_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "B_PS_stdev_list_pair = list(zip(B_PS_stdev_list, B_PS_stdev_datelist[0:]))\n",
    "B_PS_stdev_df = pd.DataFrame(B_PS_stdev_list_pair)\n",
    "B_PS_stdev_df.columns = ['MS_B_PS_stdev', 'DATETIME'] ##check if list title is updated\n",
    "B_PS_stdev_df.set_index('DATETIME', inplace=True)\n",
    "B_PS_stdev_df = B_PS_stdev_df.loc[~B_PS_stdev_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "\n",
    "#-----End for PS_stdev code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pressure Filter A lab results in merged dataframe\n",
    "df_A_PbZn = A_Pb_df.join(A_Zn_df, how='outer')\n",
    "df_A_CuNi = A_Cu_df.join(A_Ni_df, how='outer')\n",
    "df_A_CoFe = A_Co_df.join(A_Fe_df, how='outer')\n",
    "df_A_MnCr = A_Mn_df.join(A_Cr_df, how='outer')\n",
    "df_A_CaSi = A_Ca_df.join(A_Si_df, how='outer')\n",
    "df_A_AlMg = A_Al_df.join(A_Mg_df, how='outer')\n",
    "df_A_Sh2o = A_S_df.join(A_h2o_df, how='outer')\n",
    "df_A_PS_10D50D = A_PS_10D_df.join(A_PS_50D_df, how='outer')\n",
    "df_A_PS_90Dmean = A_PS_90D_df.join(A_PS_mean_v_df, how='outer')\n",
    "df_A_PbZnCuNi = df_A_PbZn.join(df_A_CuNi, how='outer')\n",
    "df_A_CoFeMnCr = df_A_CoFe.join(df_A_MnCr, how='outer')\n",
    "df_A_CaSiAlMg = df_A_CaSi.join(df_A_AlMg, how='outer')\n",
    "df_A_Sh2oPS10D50D = df_A_Sh2o.join(df_A_PS_10D50D, how='outer')\n",
    "df_A_PS_90Dmeanstdev = df_A_PS_90Dmean.join(A_PS_stdev_df, how='outer')\n",
    "df_A_PbZnCuNiCoFeMnCr = df_A_PbZnCuNi.join(df_A_CoFeMnCr, how='outer')\n",
    "df_A_CaSiAlMgSh2oPS10D50D = df_A_CaSiAlMg.join(df_A_Sh2oPS10D50D, how='outer')\n",
    "df_A_CaSiAlMgSh2oPS10D50D90Dmeanstdev = df_A_CaSiAlMgSh2oPS10D50D.join(df_A_PS_90Dmeanstdev, how='outer')\n",
    "df_A_PbZnCuNiCoFeMnCrCaSiAlMgSh2oPS10D50D90Dmeanstdev = df_A_PbZnCuNiCoFeMnCr.join(df_A_CaSiAlMgSh2oPS10D50D90Dmeanstdev, how='outer')\n",
    "df_A_merged_lab_res = df_A_PbZnCuNiCoFeMnCrCaSiAlMgSh2oPS10D50D90Dmeanstdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pressure Filter B lab results in merged DF\n",
    "df_B_PbZn = B_Pb_df.join(B_Zn_df, how='outer')\n",
    "df_B_CuNi = B_Cu_df.join(B_Ni_df, how='outer')\n",
    "df_B_CoFe = B_Co_df.join(B_Fe_df, how='outer')\n",
    "df_B_MnCr = B_Mn_df.join(B_Cr_df, how='outer')\n",
    "df_B_CaSi = B_Ca_df.join(B_Si_df, how='outer')\n",
    "df_B_AlMg = B_Al_df.join(B_Mg_df, how='outer')\n",
    "df_B_Sh2o = B_S_df.join(B_h2o_df, how='outer')\n",
    "df_B_PS_D50D = B_PS_10D_df.join(B_PS_50D_df, how='outer')\n",
    "df_B_PS_90Dmean = B_PS_90D_df.join(B_PS_mean_v_df, how='outer')\n",
    "\n",
    "df_B_PS_90Dmeanstdev = df_B_PS_90Dmean.join(B_PS_stdev_df, how='outer')\n",
    "df_B_PbZnCuNi = df_B_PbZn.join(df_B_CuNi, how='outer')\n",
    "df_B_CoFeMnCr = df_B_CoFe.join(df_B_MnCr, how='outer')\n",
    "df_B_CaSiAlMg = df_B_CaSi.join(df_B_AlMg, how='outer')\n",
    "df_B_Sh2o9D50D = df_B_Sh2o.join(df_B_PS_D50D, how='outer')\n",
    "\n",
    "df_B_PbZnCuNiCoFeMnCr = df_B_PbZnCuNi.join(df_B_CoFeMnCr, how='outer')\n",
    "df_B_CaSiAlMgSh2o9D50D = df_B_CaSiAlMg.join(df_B_Sh2o9D50D, how='outer')\n",
    "\n",
    "df_B_PbZnCuNiCoFeMnCrCaSiAlMgSh2o9D50D = df_B_PbZnCuNiCoFeMnCr.join(df_B_CaSiAlMgSh2o9D50D, how='outer')\n",
    "df_B_PbZnCuNiCoFeMnCrCaSiAlMgSh2o9D50D90Dmeanstdev = df_B_PbZnCuNiCoFeMnCrCaSiAlMgSh2o9D50D.join(df_B_PS_90Dmeanstdev, how='outer')\n",
    "\n",
    "df_B_merged_lab_res = df_B_PbZnCuNiCoFeMnCrCaSiAlMgSh2o9D50D90Dmeanstdev\n",
    "\n",
    "#deletes the rows whose index has duplicates but keeps the last value\n",
    "df_A_merged_lab_res = df_A_merged_lab_res.loc[~df_A_merged_lab_res.index.duplicated(keep='last')]\n",
    "df_B_merged_lab_res = df_B_merged_lab_res.loc[~df_B_merged_lab_res.index.duplicated(keep='last')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Done', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMBINE PiDATA WITH LABORATORY RESULTS FOR ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fcn for formatting the dates from range str\n",
    "def datesplitter_start(x):\n",
    "    return parse(x.split('to')[0])\n",
    "def datesplitter_end(x):\n",
    "    return parse(x.split('to')[1])\n",
    "#---------\n",
    "df_PressureFilter_Param['newdt'] = df_PressureFilter_Param.index\n",
    "df_PressureFilter_Param['startdate'] = df_PressureFilter_Param['newdt'].apply(lambda x: datesplitter_start(x))\n",
    "df_PressureFilter_Param['enddate'] = df_PressureFilter_Param['newdt'].apply(lambda x: datesplitter_end(x))\n",
    "\n",
    "#code for changing the index to Datetime in range format (from date1 to date2)\n",
    "df_anl_indexList = list()\n",
    "df_anl_indexList[:] = []\n",
    "df_n11_indexList = list()\n",
    "df_n11_indexList[:] = []\n",
    "try:\n",
    "    for i in range(0, len(df_A_merged_lab_res)):\n",
    "        for j in range(0, len(dfn11)):\n",
    "            if dfn11['enddate'][j-1] <= df_A_merged_lab_res.index[i] and dfn11['enddate'][j] >= df_A_merged_lab_res.index[i]:\n",
    "                df_n11_indexList.append(dfn11.index[j])\n",
    "                df_anl_indexList.append(df_A_merged_lab_res.index[i]) \n",
    "except IndexError:\n",
    "    print('IndexError')  \n",
    "Analys_indexList_Pair = list(zip(df_anl_indexList, df_n11_indexList[0:]))\n",
    "dfEL4 = pd.DataFrame(Analys_indexList_Pair)\n",
    "dfEL4.columns = ['DATETIME', 'FT_cycletimerange']\n",
    "dfEL4.set_index('DATETIME', inplace=True)\n",
    "\n",
    "dfEL5 = df_A_merged_lab_res.join(dfEL4, how='outer')\n",
    "dfEL5.columns = ['MS_A_Pb_spcl', 'MS_A_Zn_spcl', 'MS_A_Cu_spcl', 'MS_A_Ni_spcl', 'MS_A_Co_spcl', 'MS_A_Fe_spcl',\n",
    "       'MS_A_Mn_spcl', 'MS_A_Cr_spcl', 'MS_A_Ca_spcl', 'MS_A_Si_spcl', 'MS_A_Al_spcl', 'MS_A_Mg_spcl','MS_A_S_spcl', 'MS_A_moisture_spcl', 'MS_A_PS_10D_spcl', 'MS_A_PS_50D_spcl', 'MS_A_PS_90D_spcl',\n",
    "       'MS_A_PS_mean_v_spcl', 'MS_A_PS_stdev_spcl','DATETIME']\n",
    "\n",
    "dfEL5.set_index('DATETIME', inplace=True)\n",
    "df_Cycle_LabRes = dfn11.join(dfEL5, how='outer')\n",
    "df_ffillna = df_Cycle_LabRes.fillna(method='ffill')\n",
    "df_Cycle_LabRes['Cycle_Count'] = np.arange(1, len(df_Cycle_LabRes['A_FILT_T'])+1)\n",
    "df_Cycle_LabRes_2=df_Cycle_LabRes.reindex(columns= ['Cycle_Count','A_PU26A_CUR', 'A_PU27A_CUR', 'A_N2DRY', 'A_CWP_1', 'A_CWP_2',\n",
    "       'A_FILT_T', 'A_MANF_P', 'A_CWSH1', 'A_CWSH2', 'A_PRESS1_t',\n",
    "       'A_PRESS2_t', 'A_PU26A_OP', 'A_PU27A_OP', 'newdt', 'startdate',\n",
    "       'enddate', 'MS_A_Pb_spcl', 'MS_A_Zn_spcl', 'MS_A_Cu_spcl',\n",
    "       'MS_A_Ni_spcl', 'MS_A_Co_spcl', 'MS_A_Fe_spcl', 'MS_A_Mn_spcl',\n",
    "       'MS_A_Cr_spcl', 'MS_A_Ca_spcl', 'MS_A_Si_spcl', 'MS_A_Al_spcl',\n",
    "       'MS_A_Mg_spcl', 'MS_A_S_spcl', 'MS_A_moisture_spcl', 'MS_A_PS_10D_spcl',\n",
    "       'MS_A_PS_50D_spcl', 'MS_A_PS_90D_spcl', 'MS_A_PS_mean_v_spcl',\n",
    "       'MS_A_PS_stdev_spcl'])\n",
    "df_Cycle_LabRes_2_ffillna = df_Cycle_LabRes_2.fillna(method='ffill')\n",
    "df_JAN2020_cycLab_A= df_Cycle_LabRes_2_ffillna\n",
    "\n",
    "#---------------dfEL5 is the is the analysis results data indexed by daterange format. readt for merging with dfn11(FT02A data from Pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOTALIZER_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------Replacing the 'no good data for calculation'\n",
    "#TOTALIZER\n",
    "\n",
    "df_totalizer['Totalzr_wt_A'] = pd.to_numeric(df_totalizer['Totalzr_wt_A'], errors='coerce')\n",
    "for x in range(0, len(df_totalizer['Totalzr_wt_A'])):\n",
    "    if type(df_totalizer['Totalzr_wt_A'][x]== str):\n",
    "        if det_no_data(df_totalizer['Totalzr_wt_A'][x]) == 'Calculation':\n",
    "            df_totalizer['Totalzr_wt_A'][x] = np.nan\n",
    "\n",
    "df_totalizer['Totalzr_wt_A'].fillna(method='ffill', inplace= True)\n",
    "\n",
    "df_totalizer['Totalzr_wt_B'] = pd.to_numeric(df_totalizer['Totalzr_wt_B'], errors='coerce')\n",
    "for x in range(0, len(df_totalizer['Totalzr_wt_B'])):\n",
    "    if type(df_totalizer['Totalzr_wt_B'][x]== str):\n",
    "        if det_no_data(df_totalizer['Totalzr_wt_B'][x]) == 'Calculation':\n",
    "            df_totalizer['Totalzr_wt_B'][x] = np.nan\n",
    "\n",
    "df_totalizer['Totalzr_wt_B'].fillna(method='ffill', inplace= True)\n",
    "#-------------------------\n",
    "\n",
    "#WEIGHER\n",
    "\n",
    "df_totalizer['Weigher_A'] = pd.to_numeric(df_totalizer['Weigher_A'], errors='coerce')\n",
    "for x in range(0, len(df_totalizer['Weigher_A'])):\n",
    "    if type(df_totalizer['Weigher_A'][x]== str):\n",
    "        if det_no_data(df_totalizer['Weigher_A'][x]) == 'Calculation':\n",
    "            df_totalizer['Weigher_A'][x] = np.nan\n",
    "\n",
    "df_totalizer['Weigher_A'].fillna(method='ffill', inplace= True)\n",
    "\n",
    "df_totalizer['Weigher_B'] = pd.to_numeric(df_totalizer['Weigher_B'], errors='coerce')\n",
    "for x in range(0, len(df_totalizer['Weigher_B'])):\n",
    "    if type(df_totalizer['Weigher_B'][x]== str):\n",
    "        if det_no_data(df_totalizer['Weigher_B'][x]) == 'Calculation':\n",
    "            df_totalizer['Weigher_B'][x] = np.nan\n",
    "\n",
    "df_totalizer['Weigher_B'].fillna(method='ffill', inplace= True)\n",
    "#-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part of the code adds loads the totalizer weight and the weigher weight indication.\n",
    "#--------------------------------\n",
    "A_batch_cmp_index = (np.diff(np.sign(np.diff(df_totalizer['Totalzr_wt_A'].iloc[0:len(df_totalizer['Totalzr_wt_A'])])))< 0).nonzero()[0]+1\n",
    "B_batch_cmp_index = (np.diff(np.sign(np.diff(df_totalizer['Totalzr_wt_B'].iloc[0:len(df_totalizer['Totalzr_wt_B'])])))< 0).nonzero()[0]+1\n",
    "\n",
    "#creating the df for total wt\n",
    "\n",
    "lst_batch_comp_wt = []\n",
    "lst_wt_index = []\n",
    "for i in range(0, len(A_batch_cmp_index)):\n",
    "    temp_batch_wt = df_totalizer['Totalzr_wt_A'][A_batch_cmp_index[i]]-df_totalizer['Totalzr_wt_A'][A_batch_cmp_index[i]-1]\n",
    "    temp_wt_index = df_totalizer.index[A_batch_cmp_index[i]]\n",
    "    if temp_batch_wt > 0:\n",
    "        lst_batch_comp_wt.append(temp_batch_wt)\n",
    "        lst_wt_index.append(temp_wt_index)\n",
    "lst_index_batch_comp_wt = list(zip(lst_wt_index,lst_batch_comp_wt))\n",
    "A_df_batch_comp_wt_index_time = pd.DataFrame(lst_index_batch_comp_wt)\n",
    "\n",
    "A_df_batch_comp_wt_index_time.columns = ['DATETIME', 'A_weight_per_b']\n",
    "A_df_bc_wt = A_df_batch_comp_wt_index_time\n",
    "A_df_bc_wt.set_index('DATETIME', inplace= True)\n",
    "\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code that gets the weigher indicator in between batch completes\n",
    "\n",
    "#---------------------------------\n",
    "\n",
    "lst_weigher_indic = []\n",
    "lst_weigher_index = []\n",
    "for i in range(0, len(A_batch_cmp_index)-1):\n",
    "    weigher_index = df_totalizer.index[A_batch_cmp_index[i]]\n",
    "    \n",
    "    df_range = df_totalizer['Weigher_A'][A_batch_cmp_index[i]+1:A_batch_cmp_index[i+1]]\n",
    "    wt_most_freq = df_range.value_counts().idxmax()\n",
    "\n",
    "    if wt_most_freq < 0:\n",
    "        wt_most_freq = 0\n",
    "    lst_weigher_indic.append(wt_most_freq)\n",
    "    lst_weigher_index.append(weigher_index)\n",
    "    \n",
    "lst_weigher_indic = list(zip(lst_weigher_index,lst_weigher_indic))\n",
    "A_df_weigher_indic = pd.DataFrame(lst_weigher_indic)\n",
    "A_df_weigher_indic.columns = ['DATETIME', 'weight_excess']\n",
    "A_df_weigher_indic.set_index('DATETIME', inplace=True)\n",
    "\n",
    "#----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_weigher_indic.set_index('DATETIME', inplace = True)\n",
    "w_n11_indexList = []\n",
    "w_n11_bcwt_indexList = []\n",
    "\n",
    "for i in range(0, len(A_df_weigher_indic)):\n",
    "    for j in range(0, len(dfn11)):\n",
    "        if j < len(dfn11):\n",
    "            if dfn11['enddate'][j-1] <= A_df_weigher_indic.index[i] and dfn11['enddate'][j] >= A_df_weigher_indic.index[i]:\n",
    "                w_n11_indexList.append(dfn11.index[j-1])\n",
    "                w_n11_bcwt_indexList.append(A_df_weigher_indic.index[i]) \n",
    "            \n",
    "w_n11_bcwt_indexList_Pair = list(zip(w_n11_bcwt_indexList, w_n11_indexList[0:]))\n",
    "df_weigher_cycTime = pd.DataFrame(w_n11_bcwt_indexList_Pair)\n",
    "df_weigher_cycTime.columns = ['DATETIME', 'FT_cycletimerange']\n",
    "df_weigher_cycTime.set_index('DATETIME', inplace=True)\n",
    "\n",
    "df_remaining_weight_disch = df_weigher_cycTime.join(A_df_weigher_indic, how='outer')\n",
    "df_remaining_weight_dupdropd = df_remaining_weight_disch.drop_duplicates(subset = 'FT_cycletimerange', keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fcn for formatting the dates from range str\n",
    "def datesplitter_start(x):\n",
    "    return parse(x.split('to')[0])\n",
    "def datesplitter_end(x):\n",
    "    return parse(x.split('to')[1])\n",
    "#---------\n",
    "df_PressureFilter_Param['newdt'] = df_PressureFilter_Param.index\n",
    "df_PressureFilter_Param['startdate'] = df_PressureFilter_Param['newdt'].apply(lambda x: datesplitter_start(x))\n",
    "df_PressureFilter_Param['enddate'] = df_PressureFilter_Param['newdt'].apply(lambda x: datesplitter_end(x))\n",
    "\n",
    "#df_bc_wt.set_index('DATETIME', inplace = True)\n",
    "n11_indexList = []\n",
    "n11_bcwt_indexList = []\n",
    "try:\n",
    "    for i in range(0, len(A_df_bc_wt)):\n",
    "        for j in range(0, len(dfn11)):\n",
    "            if dfn11['enddate'][j-1] <= A_df_bc_wt.index[i] and dfn11['enddate'][j] >= A_df_bc_wt.index[i]:\n",
    "                n11_indexList.append(dfn11.index[j-1])\n",
    "                n11_bcwt_indexList.append(A_df_bc_wt.index[i]) \n",
    "except IndexError:\n",
    "    print('IndexError')  \n",
    "n11_bcwt_indexList_Pair = list(zip(n11_bcwt_indexList, n11_indexList[0:]))\n",
    "A_df_wt_bc_t_cyc = pd.DataFrame(n11_bcwt_indexList_Pair)\n",
    "A_df_wt_bc_t_cyc.columns = ['DATETIME', 'FT_cycletimerange']\n",
    "A_df_wt_bc_t_cyc.set_index('DATETIME', inplace=True)\n",
    "\n",
    "#combining the weight per batch and the weight remaining in the weigher\n",
    "\n",
    "A_batch_wt_with_cyclt = A_df_wt_bc_t_cyc.join(A_df_bc_wt, how='outer') # df of DATETIME, FT_cycletimerange, weight_per_batch_complete\n",
    "A_batch_wt_with_cyclt_sum = A_batch_wt_with_cyclt.groupby('FT_cycletimerange').sum()\n",
    "A_batch_wt_with_cyclt_sum1 = A_batch_wt_with_cyclt_sum.reset_index()\n",
    "\n",
    "combined_weigherInd_weightBatchcomp_type3 = pd.merge(A_batch_wt_with_cyclt_sum1, df_remaining_weight_dupdropd, on= 'FT_cycletimerange' ,how='outer')\n",
    "combined_weigherInd_weightBatchcomp_type3['A_total_wt_per_batch'] = combined_weigherInd_weightBatchcomp_type3['weight_per_b'] + combined_weigherInd_weightBatchcomp_type3['weight_excess']\n",
    "\n",
    "combined_weigherInd_weightBatchcomp_type3.columns = ['DATETIME', 'A_weight_per_b', 'A_weight_indic','A_total_wt_per_batch']\n",
    "combined_weigherInd_weightBatchcomp_type3.set_index('DATETIME', inplace=True)\n",
    "\n",
    "df_main_add_weigher = df_JAN2020_cycLab_A.join(combined_weigherInd_weightBatchcomp_type3, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMBINE PiDATA WITH LABORATORY RESULTS FOR BRAVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main_add_weigher.to_excel(r'C:\\Users\\v.t.flores\\Documents\\A_df_main_add_weigher323.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fcn for formatting the dates from range str\n",
    "def datesplitter_start(x):\n",
    "    return parse(x.split('to')[0])\n",
    "def datesplitter_end(x):\n",
    "    return parse(x.split('to')[1])\n",
    "#---------\n",
    "df_PressureFilter_Param_B['newdt'] = df_PressureFilter_Param_B.index\n",
    "df_PressureFilter_Param_B['startdate'] = df_PressureFilter_Param_B['newdt'].apply(lambda x: datesplitter_start(x))\n",
    "df_PressureFilter_Param_B['enddate'] = df_PressureFilter_Param_B['newdt'].apply(lambda x: datesplitter_end(x))\n",
    "\n",
    "#code for changing the index to Datetime in range format (from date1 to date2)\n",
    "df_anlB_indexList = list()\n",
    "df_anlB_indexList[:] = []\n",
    "df_n11B_indexList = list()\n",
    "df_n11B_indexList[:] = []\n",
    "try:\n",
    "    for i in range(0, len(df_B_merged_lab_res)):\n",
    "        for j in range(0, len(dfn11B)):\n",
    "            if dfn11B['enddate'][j-1] <= df_B_merged_lab_res.index[i] and dfn11B['enddate'][j] >= df_B_merged_lab_res.index[i]:\n",
    "                df_n11B_indexList.append(dfn11B.index[j])\n",
    "                df_anlB_indexList.append(df_B_merged_lab_res.index[i]) \n",
    "except IndexError:\n",
    "    print('IndexError')  \n",
    "Analys_indexList_Pair = list(zip(df_anlB_indexList, df_n11B_indexList[0:]))\n",
    "dfEL4B = pd.DataFrame(Analys_indexList_Pair)\n",
    "dfEL4B.columns = ['DATETIME', 'FT_cycletimerange']\n",
    "dfEL4B.set_index('DATETIME', inplace=True)\n",
    "\n",
    "dfEL5B = df_B_merged_lab_res.join(dfEL4B, how='outer')\n",
    "dfEL5B.columns = ['MS_B_Pb_spcl', 'MS_B_Zn_spcl', 'MS_B_Cu_spcl', 'MS_B_Ni_spcl', 'MS_B_Co_spcl', 'MS_B_Fe_spcl',\n",
    "       'MS_B_Mn_spcl', 'MS_B_Cr_spcl', 'MS_B_Ca_spcl', 'MS_B_Si_spcl', 'MS_B_Al_spcl', 'MS_B_Mg_spcl','MS_B_S_spcl', 'MS_B_moisture_spcl', 'MS_B_PS_10D_spcl', 'MS_B_PS_50D_spcl', 'MS_B_PS_90D_spcl',\n",
    "       'MS_B_PS_mean_v_spcl', 'MS_B_PS_stdev_spcl','DATETIME']\n",
    "\n",
    "dfEL5B.set_index('DATETIME', inplace=True)\n",
    "df_Cycle_LabResB = dfn11B.join(dfEL5B, how='outer')\n",
    "df_ffillnaB = df_Cycle_LabResB.fillna(method='ffill')\n",
    "df_Cycle_LabResB['Cycle_Count'] = np.arange(1, len(df_Cycle_LabResB['B_FILT_T'])+1)\n",
    "df_Cycle_LabRes_2B=df_Cycle_LabResB.reindex(columns= ['Cycle_Count','B_PU26B_CUR', 'B_PU27B_CUR', 'B_N2DRY', 'B_CWP_1', 'B_CWP_2',\n",
    "       'B_FILT_T', 'B_MANF_P', 'B_CWSH1', 'B_CWSH2', 'B_PRESS1_t',\n",
    "       'B_PRESS2_t', 'B_PU26B_OP', 'B_PU27B_OP', 'newdt', 'startdate',\n",
    "       'enddate', 'MS_B_Pb_spcl', 'MS_B_Zn_spcl', 'MS_B_Cu_spcl',\n",
    "       'MS_B_Ni_spcl', 'MS_B_Co_spcl', 'MS_B_Fe_spcl', 'MS_B_Mn_spcl',\n",
    "       'MS_B_Cr_spcl', 'MS_B_Ca_spcl', 'MS_B_Si_spcl', 'MS_B_Al_spcl',\n",
    "       'MS_B_Mg_spcl', 'MS_B_S_spcl', 'MS_B_moisture_spcl', 'MS_B_PS_10D_spcl',\n",
    "       'MS_B_PS_50D_spcl', 'MS_B_PS_90D_spcl', 'MS_B_PS_mean_v_spcl',\n",
    "       'MS_B_PS_stdev_spcl'])\n",
    "df_Cycle_LabRes_2B_ffillna = df_Cycle_LabRes_2B.fillna(method='ffill')\n",
    "df_JAN2020_cycLab_B = df_Cycle_LabRes_2B_ffillna\n",
    "\n",
    "#---------------dfEL5 is the is the analysis results data indexed by daterange format. readt for merging with dfn11(FT02A data from Pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOTALIZER_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part of the code adds loads the totalizer weight and the weigher weight indication.\n",
    "#--------------------------------\n",
    "B_batch_cmp_index = (np.diff(np.sign(np.diff(df_totalizer['Totalzr_wt_B'].iloc[0:len(df_totalizer['Totalzr_wt_B'])])))< 0).nonzero()[0]+1\n",
    "\n",
    "#creating the df for total wt\n",
    "\n",
    "lst_batch_comp_wt = []\n",
    "lst_wt_index = []\n",
    "for i in range(0, len(B_batch_cmp_index)):\n",
    "    temp_batch_wt = df_totalizer['Totalzr_wt_B'][B_batch_cmp_index[i]]-df_totalizer['Totalzr_wt_B'][B_batch_cmp_index[i]-1]\n",
    "    temp_wt_index = df_totalizer.index[B_batch_cmp_index[i]]\n",
    "    if temp_batch_wt > 0:\n",
    "        lst_batch_comp_wt.append(temp_batch_wt)\n",
    "        lst_wt_index.append(temp_wt_index)\n",
    "lst_index_batch_comp_wt = list(zip(lst_wt_index,lst_batch_comp_wt))\n",
    "B_df_batch_comp_wt_index_time = pd.DataFrame(lst_index_batch_comp_wt)\n",
    "\n",
    "B_df_batch_comp_wt_index_time.columns = ['DATETIME', 'weight_per_b']\n",
    "B_df_bc_wt = B_df_batch_comp_wt_index_time\n",
    "B_df_bc_wt.set_index('DATETIME', inplace= True)\n",
    "\n",
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code that gets the weigher indicator in between batch completes\n",
    "\n",
    "#---------------------------------\n",
    "lst_weigher_indic = []\n",
    "lst_weigher_index = []\n",
    "for i in range(0, len(B_batch_cmp_index)-1):\n",
    "    weigher_index = df_totalizer.index[B_batch_cmp_index[i]]\n",
    "    \n",
    "    df_range = df_totalizer['Weigher_B'][B_batch_cmp_index[i]+1:B_batch_cmp_index[i+1]]\n",
    "    wt_most_freq = df_range.value_counts().idxmax()\n",
    "\n",
    "    if wt_most_freq < 0:\n",
    "        wt_most_freq = 0\n",
    "    lst_weigher_indic.append(wt_most_freq)\n",
    "    lst_weigher_index.append(weigher_index)\n",
    "    \n",
    "lst_weigher_indic = list(zip(lst_weigher_index,lst_weigher_indic))\n",
    "B_df_weigher_indic = pd.DataFrame(lst_weigher_indic)\n",
    "B_df_weigher_indic.columns = ['DATETIME', 'B_weight_excess']\n",
    "B_df_weigher_indic.set_index('DATETIME', inplace=True)\n",
    "#----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_weigher_indic.set_index('DATETIME', inplace = True)\n",
    "Bw_n11_indexList = []\n",
    "Bw_n11_bcwt_indexList = []\n",
    "\n",
    "for i in range(0, len(B_df_weigher_indic)):\n",
    "    for j in range(0, len(dfn11B)):\n",
    "        if j < len(dfn11B):\n",
    "            if dfn11B['enddate'][j-1] <= B_df_weigher_indic.index[i] and dfn11B['enddate'][j] >= B_df_weigher_indic.index[i]:\n",
    "                Bw_n11_indexList.append(dfn11B.index[j-1])\n",
    "                Bw_n11_bcwt_indexList.append(B_df_weigher_indic.index[i]) \n",
    "            \n",
    "Bw_n11_bcwt_indexList_Pair = list(zip(Bw_n11_bcwt_indexList, Bw_n11_indexList[0:]))\n",
    "Bdf_weigher_cycTime = pd.DataFrame(Bw_n11_bcwt_indexList_Pair)\n",
    "Bdf_weigher_cycTime.columns = ['DATETIME', 'FT_cycletimerange']\n",
    "Bdf_weigher_cycTime.set_index('DATETIME', inplace=True)\n",
    "\n",
    "Bdf_remaining_weight_disch = Bdf_weigher_cycTime.join(B_df_weigher_indic, how='outer')\n",
    "Bdf_remaining_weight_dupdropd = Bdf_remaining_weight_disch.drop_duplicates(subset = 'FT_cycletimerange', keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fcn for formatting the dates from range str\n",
    "def datesplitter_start(x):\n",
    "    return parse(x.split('to')[0])\n",
    "def datesplitter_end(x):\n",
    "    return parse(x.split('to')[1])\n",
    "#---------\n",
    "df_PressureFilter_Param['newdt'] = df_PressureFilter_Param.index\n",
    "df_PressureFilter_Param['startdate'] = df_PressureFilter_Param['newdt'].apply(lambda x: datesplitter_start(x))\n",
    "df_PressureFilter_Param['enddate'] = df_PressureFilter_Param['newdt'].apply(lambda x: datesplitter_end(x))\n",
    "\n",
    "#df_bc_wt.set_index('DATETIME', inplace = True)\n",
    "n11_indexList = []\n",
    "n11_bcwt_indexList = []\n",
    "try:\n",
    "    for i in range(0, len(B_df_bc_wt)):\n",
    "        for j in range(0, len(dfn11B)):\n",
    "            if dfn11B['enddate'][j-1] <= B_df_bc_wt.index[i] and dfn11B['enddate'][j] >= B_df_bc_wt.index[i]:\n",
    "                n11_indexList.append(dfn11B.index[j-1])\n",
    "                n11_bcwt_indexList.append(B_df_bc_wt.index[i]) \n",
    "except IndexError:\n",
    "    print('IndexError')  \n",
    "n11_bcwt_indexList_Pair = list(zip(n11_bcwt_indexList, n11_indexList[0:]))\n",
    "B_df_wt_bc_t_cyc = pd.DataFrame(n11_bcwt_indexList_Pair)\n",
    "B_df_wt_bc_t_cyc.columns = ['DATETIME', 'FT_cycletimerange']\n",
    "B_df_wt_bc_t_cyc.set_index('DATETIME', inplace=True)\n",
    "\n",
    "#combining the weight per batch and the weight remaining in the weigher\n",
    "\n",
    "B_batch_wt_with_cyclt = B_df_wt_bc_t_cyc.join(B_df_bc_wt, how='outer') # df of DATETIME, FT_cycletimerange, weight_per_batch_complete\n",
    "B_batch_wt_with_cyclt_sum = B_batch_wt_with_cyclt.groupby('FT_cycletimerange').sum()\n",
    "B_batch_wt_with_cyclt_sum1 = B_batch_wt_with_cyclt_sum.reset_index()\n",
    "\n",
    "Bcombined_weigherInd_weightBatchcomp_type3 = pd.merge(B_batch_wt_with_cyclt_sum1, Bdf_remaining_weight_dupdropd, on= 'FT_cycletimerange' ,how='outer')\n",
    "Bcombined_weigherInd_weightBatchcomp_type3['total_wt_per_batch'] = Bcombined_weigherInd_weightBatchcomp_type3['weight_per_b'] + Bcombined_weigherInd_weightBatchcomp_type3['B_weight_excess']\n",
    "\n",
    "Bcombined_weigherInd_weightBatchcomp_type3.columns = ['DATETIME', 'B_weight_per_b', 'B_weight_indic','B_total_wt_per_batch']\n",
    "Bcombined_weigherInd_weightBatchcomp_type3.set_index('DATETIME', inplace=True)\n",
    "\n",
    "B_df_main_add_weigher = df_JAN2020_cycLab_B.join(Bcombined_weigherInd_weightBatchcomp_type3, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMBINE PiDATA WITH LABORATORY RESULTS FOR BRAVO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE THE DATAFRAME TO EXCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main_add_weigher.to_excel(r'C:\\Users\\v.t.flores\\Documents\\106FT02A_data_JAN2020_beta.xlsx')\n",
    "df_main_add_weigher.to_excel(r'\\\\THGOBNA002\\thpal\\05 Production\\150 DCS\\00 DCS Monitoring & Records\\00 Active Files\\2020\\Pressure_Filter_Data_2019\\106FT02A_data_JAN2020_beta.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "B_df_main_add_weigher.to_excel(r'C:\\Users\\v.t.flores\\Documents\\106FT02B_data_JAN2020_beta.xlsx')\n",
    "B_df_main_add_weigher.to_excel(r'\\\\THGOBNA002\\thpal\\05 Production\\150 DCS\\00 DCS Monitoring & Records\\00 Active Files\\2020\\Pressure_Filter_Data_2019\\106FT02B_data_JAN2020_beta.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_df_main_add_weigher.to_excel(r'C:\\Users\\v.t.flores\\Documents\\B_df_main_add_weigher409.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtlstindxlst = []\n",
    "wtlst = []\n",
    "for i in range(1, len(B_df_main_add_weigher['B_total_wt_per_batch'])):\n",
    "    diff = B_df_main_add_weigher['B_total_wt_per_batch'][i] - B_df_main_add_weigher['B_weight_indic'][i-1]\n",
    "    indxlst = B_df_main_add_weigher.index[i]\n",
    "    \n",
    "    wtlstindxlst.append(indxlst)\n",
    "    wtlst.append(diff)\n",
    "    \n",
    "pairlst_wtlstindxlst = list(zip(wtlst,wtlstindxlst[0:]))\n",
    "B_corrected_total_wt = pd.DataFrame(pairlst_wtlstindxlst)\n",
    "B_corrected_total_wt.columns = ['DATETIME', 'B_total_wt_corrected']\n",
    "B_corrected_total_wt.set_index('DATETIME', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_df_main_add_weigher = B_df_main_add_weigher.join(B_corrected_total_wt, how= 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_df_main_add_weigher.to_excel(r'C:\\Users\\v.t.flores\\Documents\\tmepB533.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
