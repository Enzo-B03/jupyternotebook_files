{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an InterActive Graph of NTRL Lab Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from dateutil.parser import parse\n",
    "from itertools import compress\n",
    "from collections import OrderedDict\n",
    "from datetime import date\n",
    "import datetime\n",
    "import re\n",
    "from datetime import timedelta\n",
    "import openpyxl\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import plotly.express as px\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used in the analysis results data. cleans up the sampling time entry\n",
    "def time_cleaner(x):\n",
    "    time_regex = re.compile(r'[0-8a-zA-Z:]', re.IGNORECASE| re.VERBOSE|re.DOTALL)\n",
    "    timeact = time_regex.findall(x)\n",
    "    newtime = ''.join(timeact)\n",
    "    t = parse(x)\n",
    "    new_str_time = str(t.hour)+':'+str(t.minute)+':'+str(t.second)\n",
    "    return new_str_time\n",
    "def ave_filler(dfname, colname):\n",
    "    dfcn = dfname[colname]\n",
    "    try:        \n",
    "        dfcn = pd.to_numeric(dfcn, errors='coerce')\n",
    "    except:\n",
    "        print('error found in' + str(colname))\n",
    "        pass\n",
    "    try:\n",
    "        dfname[colname] = dfcn.where(dfcn.notnull(), other=(dfcn.fillna(method='ffill') + dfcn.fillna(method='bfill'))/2)\n",
    "    except:\n",
    "        print('nan not fixed' + str(colname))   \n",
    "    return dfname\n",
    "def generate_df_from_analysis(workbookTab, col_sampleDate, col_sampleTime, col_analysis, col_title):\n",
    "    #get the start and end sample dates\n",
    "    Smpl_day_index = list()\n",
    "    Chem_Element_datelist = list()\n",
    "    Chem_Element_list = list()\n",
    "    Smpl_day_index = list()\n",
    "    Chem_Element_datelist = list()\n",
    "    Chem_Element_list = list()\n",
    "    plus_oneday_list = ['0:0:0', '1:0:0', '2:0:0', '3:0:0', '4:0:0', '5:0:0']\n",
    "    #gets the date day from the B column of the analysis file if there is value in the cell\n",
    "    for i in range(1, workbookTab.max_row):\n",
    "        if type(workbookTab[str(col_sampleDate) + str(i)].value) == int or type(workbookTab[str(col_sampleDate) + str(i)].value) == float:\n",
    "            Smpl_day_index.append(i)\n",
    "    #get the ending row that has date day\n",
    "    for j in range(1, workbookTab.max_row):\n",
    "        if type(workbookTab[str(col_sampleTime) + str(j)].value) == datetime.datetime:\n",
    "            smpl_end_index = j\n",
    "    for y in range(0, len(Smpl_day_index)):\n",
    "        if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "            for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "                if workbookTab[str(col_analysis)+str(x)].value is None: \n",
    "                    pass\n",
    "                else:\n",
    "                    if time_cleaner(str(workbookTab[str(col_sampleTime)+str(x)].value)) in plus_oneday_list:\n",
    "                        date = str((currentmonthyr)+datetime.timedelta(workbookTab[str(col_sampleDate)+str(Smpl_day_index[y])].value))\\\n",
    "                        +' '+time_cleaner(str(workbookTab[str(col_sampleTime)+str(x)].value))\n",
    "                        Chem_Element_datelist.append(parse(date))  \n",
    "                        chem_element = workbookTab[str(col_analysis)+str(x)].value\n",
    "                        Chem_Element_list.append(chem_element)\n",
    "                    else:\n",
    "                        date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(workbookTab[str(col_sampleDate)+str(Smpl_day_index[y])].value))\\\n",
    "                        +' '+time_cleaner(str(workbookTab[str(col_sampleTime)+str(x)].value))\n",
    "                        Chem_Element_datelist.append(parse(date))  \n",
    "                        chem_element = workbookTab[str(col_analysis)+str(x)].value\n",
    "                        Chem_Element_list.append(chem_element)\n",
    "\n",
    "        if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "            for x in range(Smpl_day_index[y], smpl_end_index+1):\n",
    "                if workbookTab[str(col_analysis)+str(x)].value is None:   \n",
    "                    pass\n",
    "                else:\n",
    "                    if time_cleaner(str(workbookTab[str(col_sampleTime)+str(x)].value)) in plus_oneday_list:\n",
    "                        date = str((currentmonthyr)+datetime.timedelta(workbookTab[str(col_sampleDate)+str(Smpl_day_index[y])].value))\\\n",
    "                        +' '+time_cleaner(str(workbookTab[str(col_sampleTime)+str(x)].value))\n",
    "                        Chem_Element_datelist.append(parse(date))\n",
    "                        chem_element = workbookTab[str(col_analysis)+str(x)].value\n",
    "                        Chem_Element_list.append(chem_element)\n",
    "                    else:\n",
    "                        date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(workbookTab[str(col_sampleDate)+str(Smpl_day_index[y])].value))\\\n",
    "                        +' '+time_cleaner(str(workbookTab[str(col_sampleTime)+str(x)].value))\n",
    "                        Chem_Element_datelist.append(parse(date))\n",
    "                        chem_element = workbookTab[str(col_analysis)+str(x)].value\n",
    "                        Chem_Element_list.append(chem_element)\n",
    "\n",
    "    Chem_Element_list_pair = list(zip(Chem_Element_list, Chem_Element_datelist[0:]))\n",
    "    Chem_Element_df = pd.DataFrame(Chem_Element_list_pair)\n",
    "    Chem_Element_df.columns = [col_title, 'DATETIME'] ##check if list title is updated\n",
    "    Chem_Element_df.set_index('DATETIME', inplace=True)\n",
    "\n",
    "    return Chem_Element_df\n",
    "\n",
    "def generate_add_trace(figname, df_elemental, axis_bool):\n",
    "    colname = df_elemental.columns[0]\n",
    "    figname.add_trace(go.Scatter(x=df_elemental.index, y=df_elemental[colname], name=colname), secondary_y=axis_bool,)\n",
    "#---------------------------------------\n",
    "\n",
    "class LabResults():\n",
    "    def __init__(self, monthyear, filePath):\n",
    "        #____monthyear format examle is datetime.date(2020,7,1)\n",
    "        #____filepath example is '\\\\thgobna001\\userdata\\THPAL\\...\\ANALYSIS RESULTS 2020\\7) July _2020.xlsx'\n",
    "        #____add r' at the start of the filepath to apply the regex that allows the use of the filepath as raw\n",
    "        #____sef.workbook is the uploaded excel workbook of the entire analysis files so it takes time to load\n",
    "        #____Create an instance of the workbook by giving it a varible name with the sample format: Jan_2020\n",
    "        \n",
    "        import openpyxl\n",
    "        import io\n",
    "        with open(filePath, \"rb\") as f:\n",
    "            in_mem_file = io.BytesIO(f.read())\n",
    "        \n",
    "        self.workbook = openpyxl.load_workbook(in_mem_file, data_only=True)\n",
    "        self.monthyear = monthyear   \n",
    "        \n",
    "    def generate_df_from_analysis(self, tabname, list_lab_index):\n",
    "        #____tabname is a string type name of the tab in the laboratory analysis excel file.\n",
    "        #____-------Example tabnames: '104PU01', '105TK03', '106TK01'\n",
    "        #____-------Inside these tabs are the specific analysis results done on the sample taken from the 'tabname' area .\n",
    "        #____list_lab_index is a list with the sample format is: ['B', 'C', 'E', '104_Pb']\n",
    "        #____-------the first letter 'B' is the column letter where the day of the month is written as integers 1,2,3..30\n",
    "        #____-------the second letter 'C' is the column letter where the sampling time is written; e.g. '7:00:00 AM'\n",
    "        #____-------the third letter 'E' is the column where the results of the specific analysis are listed\n",
    "        #____-------the last item in the list is the string you'll use as the new title; e.g. '104_Pb'\n",
    "        \n",
    "        workbookTab = self.workbook[tabname]\n",
    "        self.list_lab_index = list_lab_index\n",
    "        col_sampleDate = self.list_lab_index[0]\n",
    "        col_sampleTime = self.list_lab_index[1]\n",
    "        col_analysis = self.list_lab_index[2]\n",
    "        col_title = self.list_lab_index[3]\n",
    "        currentmonthyr = self.monthyear\n",
    "        \n",
    "        #get the start and end sample dates\n",
    "        Smpl_day_index = list()\n",
    "        Chem_Element_datelist = list()\n",
    "        Chem_Element_list = list()\n",
    "        Smpl_day_index = list()\n",
    "        Chem_Element_datelist = list()\n",
    "        Chem_Element_list = list()\n",
    "        plus_oneday_list = ['0:0:0', '1:0:0', '2:0:0', '3:0:0', '4:0:0', '5:0:0']\n",
    "        #gets the date day from the B column of the analysis file if there is value in the cell\n",
    "        for i in range(1, workbookTab.max_row):\n",
    "            if type(workbookTab[str(col_sampleDate) + str(i)].value) == int or type(workbookTab[str(col_sampleDate) + str(i)].value) == float:\n",
    "                Smpl_day_index.append(i)\n",
    "        #get the ending row that has date day\n",
    "        for j in range(1, workbookTab.max_row):\n",
    "            if type(workbookTab[str(col_sampleTime) + str(j)].value) == datetime.datetime:\n",
    "                smpl_end_index = j\n",
    "        for y in range(0, len(Smpl_day_index)):\n",
    "            if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "                for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "                    if workbookTab[str(col_analysis)+str(x)].value is None: \n",
    "                        pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            if time_cleaner(str(workbookTab[str(col_sampleTime)+str(x)].value)) in plus_oneday_list:\n",
    "                                date = str((currentmonthyr)+datetime.timedelta(workbookTab[str(col_sampleDate)+str(Smpl_day_index[y])].value))\\\n",
    "                                +' '+time_cleaner(str(workbookTab[str(col_sampleTime)+str(x)].value))\n",
    "                                Chem_Element_datelist.append(parse(date))  \n",
    "                                chem_element = workbookTab[str(col_analysis)+str(x)].value\n",
    "                                Chem_Element_list.append(chem_element)\n",
    "                            else:\n",
    "                                date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(workbookTab[str(col_sampleDate)+str(Smpl_day_index[y])].value))\\\n",
    "                                +' '+time_cleaner(str(workbookTab[str(col_sampleTime)+str(x)].value))\n",
    "                                Chem_Element_datelist.append(parse(date))  \n",
    "                                chem_element = workbookTab[str(col_analysis)+str(x)].value\n",
    "                                Chem_Element_list.append(chem_element)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "            if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "                for x in range(Smpl_day_index[y], smpl_end_index+1):\n",
    "                    if workbookTab[str(col_analysis)+str(x)].value is None:   \n",
    "                        pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            if time_cleaner(str(workbookTab[str(col_sampleTime)+str(x)].value)) in plus_oneday_list:\n",
    "                                date = str((currentmonthyr)+datetime.timedelta(workbookTab[str(col_sampleDate)+str(Smpl_day_index[y])].value))\\\n",
    "                                +' '+time_cleaner(str(workbookTab[str(col_sampleTime)+str(x)].value))\n",
    "                                Chem_Element_datelist.append(parse(date))\n",
    "                                chem_element = workbookTab[str(col_analysis)+str(x)].value\n",
    "                                Chem_Element_list.append(chem_element)\n",
    "                            else:\n",
    "                                date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(workbookTab[str(col_sampleDate)+str(Smpl_day_index[y])].value))\\\n",
    "                                +' '+time_cleaner(str(workbookTab[str(col_sampleTime)+str(x)].value))\n",
    "                                Chem_Element_datelist.append(parse(date))\n",
    "                                chem_element = workbookTab[str(col_analysis)+str(x)].value\n",
    "                                Chem_Element_list.append(chem_element)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "        Chem_Element_list_pair = list(zip(Chem_Element_list, Chem_Element_datelist[0:]))\n",
    "        Chem_Element_df = pd.DataFrame(Chem_Element_list_pair)\n",
    "        Chem_Element_df.columns = [col_title, 'DATETIME'] ##check if list title is updated\n",
    "        Chem_Element_df.set_index('DATETIME', inplace=True)\n",
    "\n",
    "        return Chem_Element_df\n",
    "    \n",
    "def join_DF(dfsList1):\n",
    "    df = dfsList1[0]\n",
    "    for i in range(1,len(dfsList1)):\n",
    "        df = df.join(dfsList1[i], how='outer')\n",
    "    return df\n",
    "\n",
    "def append_monthsDF(dfsListmonthly): #takes the list of dataframes from monthly lab results of one sampling area\n",
    "    df = dfsListmonthly[0]\n",
    "    for i in range(1,len(dfsListmonthly)):\n",
    "        df = df.append(dfsListmonthly[i], ignore_index=True)\n",
    "    return df    \n",
    "\n",
    "\n",
    "\n",
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from dateutil.parser import parse\n",
    "from itertools import compress\n",
    "from collections import OrderedDict\n",
    "from datetime import date\n",
    "import datetime\n",
    "import re\n",
    "from datetime import timedelta\n",
    "import openpyxl\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "import plotly.express as px\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "# Create figure with secondary y-axis\n",
    "\n",
    "#used in the analysis results data. cleans up the sampling time entry\n",
    "def time_cleaner(x):\n",
    "    time_regex = re.compile(r'[0-8a-zA-Z:]', re.IGNORECASE| re.VERBOSE|re.DOTALL)\n",
    "    timeact = time_regex.findall(x)\n",
    "    newtime = ''.join(timeact)\n",
    "    t = parse(x)\n",
    "    new_str_time = str(t.hour)+':'+str(t.minute)+':'+str(t.second)\n",
    "    return new_str_time\n",
    "def ave_filler(dfname, colname):\n",
    "    dfcn = dfname[colname]\n",
    "    try:        \n",
    "        dfcn = pd.to_numeric(dfcn, errors='coerce')\n",
    "    except:\n",
    "        print('error found in' + str(colname))\n",
    "        pass\n",
    "    try:\n",
    "        dfname[colname] = dfcn.where(dfcn.notnull(), other=(dfcn.fillna(method='ffill') + dfcn.fillna(method='bfill'))/2)\n",
    "    except:\n",
    "        print('nan not fixed' + str(colname))   \n",
    "    return dfname\n",
    "def generate_df_from_analysis(workbookTab, col_sampleDate, col_sampleTime, col_analysis, col_title):\n",
    "    #get the start and end sample dates\n",
    "    Smpl_day_index = list()\n",
    "    Chem_Element_datelist = list()\n",
    "    Chem_Element_list = list()\n",
    "    Smpl_day_index = list()\n",
    "    Chem_Element_datelist = list()\n",
    "    Chem_Element_list = list()\n",
    "    plus_oneday_list = ['0:0:0', '1:0:0', '2:0:0', '3:0:0', '4:0:0', '5:0:0']\n",
    "    #gets the date day from the B column of the analysis file if there is value in the cell\n",
    "    for i in range(1, workbookTab.max_row):\n",
    "        if type(workbookTab[str(col_sampleDate) + str(i)].value) == int or type(workbookTab[str(col_sampleDate) + str(i)].value) == float:\n",
    "            Smpl_day_index.append(i)\n",
    "    #get the ending row that has date day\n",
    "    for j in range(1, workbookTab.max_row):\n",
    "        if type(workbookTab[str(col_sampleTime) + str(j)].value) == datetime.datetime:\n",
    "            smpl_end_index = j\n",
    "    for y in range(0, len(Smpl_day_index)):\n",
    "        if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "            for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "                if workbookTab[str(col_analysis)+str(x)].value is None: \n",
    "                    pass\n",
    "                else:\n",
    "                    if time_cleaner(str(workbookTab[str(col_sampleTime)+str(x)].value)) in plus_oneday_list:\n",
    "                        date = str((currentmonthyr)+datetime.timedelta(workbookTab[str(col_sampleDate)+str(Smpl_day_index[y])].value))\\\n",
    "                        +' '+time_cleaner(str(workbookTab[str(col_sampleTime)+str(x)].value))\n",
    "                        Chem_Element_datelist.append(parse(date))  \n",
    "                        chem_element = workbookTab[str(col_analysis)+str(x)].value\n",
    "                        Chem_Element_list.append(chem_element)\n",
    "                    else:\n",
    "                        date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(workbookTab[str(col_sampleDate)+str(Smpl_day_index[y])].value))\\\n",
    "                        +' '+time_cleaner(str(workbookTab[str(col_sampleTime)+str(x)].value))\n",
    "                        Chem_Element_datelist.append(parse(date))  \n",
    "                        chem_element = workbookTab[str(col_analysis)+str(x)].value\n",
    "                        Chem_Element_list.append(chem_element)\n",
    "\n",
    "        if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "            for x in range(Smpl_day_index[y], smpl_end_index+1):\n",
    "                if workbookTab[str(col_analysis)+str(x)].value is None:   \n",
    "                    pass\n",
    "                else:\n",
    "                    if time_cleaner(str(workbookTab[str(col_sampleTime)+str(x)].value)) in plus_oneday_list:\n",
    "                        date = str((currentmonthyr)+datetime.timedelta(workbookTab[str(col_sampleDate)+str(Smpl_day_index[y])].value))\\\n",
    "                        +' '+time_cleaner(str(workbookTab[str(col_sampleTime)+str(x)].value))\n",
    "                        Chem_Element_datelist.append(parse(date))\n",
    "                        chem_element = workbookTab[str(col_analysis)+str(x)].value\n",
    "                        Chem_Element_list.append(chem_element)\n",
    "                    else:\n",
    "                        date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(workbookTab[str(col_sampleDate)+str(Smpl_day_index[y])].value))\\\n",
    "                        +' '+time_cleaner(str(workbookTab[str(col_sampleTime)+str(x)].value))\n",
    "                        Chem_Element_datelist.append(parse(date))\n",
    "                        chem_element = workbookTab[str(col_analysis)+str(x)].value\n",
    "                        Chem_Element_list.append(chem_element)\n",
    "\n",
    "    Chem_Element_list_pair = list(zip(Chem_Element_list, Chem_Element_datelist[0:]))\n",
    "    Chem_Element_df = pd.DataFrame(Chem_Element_list_pair)\n",
    "    Chem_Element_df.columns = [col_title, 'DATETIME'] ##check if list title is updated\n",
    "    Chem_Element_df.set_index('DATETIME', inplace=True)\n",
    "\n",
    "    return Chem_Element_df\n",
    "\n",
    "def generate_add_trace(figname, df_elemental, axis_bool):\n",
    "    colname = df_elemental.columns[0]\n",
    "    figname.add_trace(go.Scatter(x=df_elemental.index, y=df_elemental[colname], name=colname), secondary_y=axis_bool,)\n",
    "#---------------------------------------\n",
    "\n",
    "#--------------------------------------------------End-------------------------------------------------------------------\n",
    "#\n",
    "#\n",
    "def generate_plot_from_df(figname, df,df_axis_truefalse_list):\n",
    "    for i in range(0, len(df.columns)):\n",
    "        colname = df.columns[i]\n",
    "        figname.add_trace(go.Scatter(x=df[colname].index, y=df[colname], name=colname), secondary_y=df_axis_truefalse_list[i],)\n",
    "#\n",
    "#\n",
    "#--------------------------------------------------------------End-----------------------------------------------------------------------\n",
    "\n",
    "def generate_add_trace(figname, df_elemental, axis_bool):\n",
    "    colname = df_elemental.columns[0]\n",
    "    figname.add_trace(go.Scatter(x=df_elemental.index, y=df_elemental[colname], name=colname), secondary_y=axis_bool,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentmonthyr_08_2020 = datetime.date(2020,8,1)\n",
    "currentmonthyr_09_2020 = datetime.date(2020,9,1)\n",
    "\n",
    "#filepath\n",
    "\n",
    "filepath_08_2020 = r'\\\\thgobna001\\userdata\\THPAL\\Department\\TAGANITO\\SHARED\\AnalysisResult\\ANALYSIS RESULTS 2020\\08 AUGUST 2020\\8) August _2020.xlsx'\n",
    "filepath_09_2020 = r'\\\\thgobna001\\userdata\\THPAL\\Department\\TAGANITO\\SHARED\\AnalysisResult\\ANALYSIS RESULTS 2020\\9) September _2020.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the filepaths and the monthyr data for year 2019\n",
    "currentmonthyr_09_2020 = datetime.date(2020,9,1)\n",
    "\n",
    "\n",
    "#filepath\n",
    "filepath_09_2020 = r'\\\\thgobna001\\userdata\\THPAL\\Department\\TAGANITO\\SHARED\\AnalysisResult\\ANALYSIS RESULTS 2020\\9) September _2020.xlsx'\n",
    "\n",
    "ntrl_Pb_3 = ['B', 'C', 'E', '104_Pb']\n",
    "ntrl_Zn_3 = ['B', 'C', 'F', '104_Zn']\n",
    "ntrl_Cu_3 = ['B', 'C', 'G', '104_Cu']\n",
    "ntrl_Ni_3 = ['B', 'C', 'H', '104_Ni']\n",
    "ntrl_Co_3 = ['B', 'C', 'I', '104_Co']\n",
    "ntrl_Fe_3 = ['B', 'C', 'J', '104_Fe']\n",
    "ntrl_Mn_3 = ['B', 'C', 'K', '104_Mn']\n",
    "ntrl_Cr_3 = ['B', 'C', 'L', '104_Cr']\n",
    "ntrl_Ca_3 = ['B', 'C', 'M', '104_Ca']\n",
    "ntrl_Si_3 = ['B', 'C', 'N', '104_Si']\n",
    "ntrl_Al_3 = ['B', 'C', 'O', '104_Al']\n",
    "ntrl_Mg_3 = ['B', 'C', 'P', '104_Mg']\n",
    "ntrl_Fe2_3 = ['B', 'C', 'R', '104_Fe2']\n",
    "ntrl_pH60_3 = ['B', 'C', 'BB', '104_pH60'] # check\n",
    "ntrl_ORP_3 = ['B', 'C', 'AJ', '104_ORP'] \n",
    "ntrl_NTU_3 = ['B', 'C', 'AN', '104_NTU'] # check\n",
    "ntrl_Ft_3 = ['B', 'C', 'AW', '104_Ft'] #check\n",
    "\n",
    "tk13_Zn_3 = ['B', 'C', 'E', 'tk13_Zn']\n",
    "tk13_Solids_3 = ['B', 'C', 'AZ', 'tk13_solids%'] #check\n",
    "\n",
    "MStk01_Zn_drop_3 = ['B', 'C', 'BD', 'Zn_drop']\n",
    "MStk01_Zn_3 = ['B', 'C', 'F', '106_Zn']\n",
    "MStk01_Ni_3 = ['B', 'C', 'H', '106_Ni']\n",
    "\n",
    "\n",
    "tk05_Pb = ['B', 'C', 'E', 'tk05_Pb']\n",
    "tk05_Zn = ['B', 'C', 'F', 'tk05_Zn']\n",
    "tk05_Cu = ['B', 'C', 'G', 'tk05_Cu']\n",
    "tk05_Ni = ['B', 'C', 'H', 'tk05_Ni']\n",
    "tk05_Co = ['B', 'C', 'I', 'tk05_Co']\n",
    "tk05_Fe = ['B', 'C', 'J', 'tk05_Fe']\n",
    "tk05_Mn = ['B', 'C', 'K', 'tk05_Mn']\n",
    "tk05_Cr = ['B', 'C', 'L', 'tk05_Cr']\n",
    "tk05_Ca = ['B', 'C', 'M', 'tk05_Ca']\n",
    "tk05_Si = ['B', 'C', 'N', 'tk05_Si']\n",
    "tk05_Al = ['B', 'C', 'O', 'tk05_Al']\n",
    "tk05_Mg = ['B', 'C', 'P', 'tk05_Mg']\n",
    " \n",
    "monthLab_3 = LabResults(currentmonthyr_09_2020, filepath_09_2020)\n",
    "df_month_3 = monthLab_3.generate_df_from_analysis('104PU01', ntrl_Pb_3).join(\n",
    "    monthLab_3.generate_df_from_analysis('104PU01', ntrl_Zn_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('104PU01', ntrl_Cu_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('104PU01', ntrl_Ni_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('104PU01', ntrl_Co_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('104PU01', ntrl_Fe_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('104PU01', ntrl_Mn_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('104PU01', ntrl_Cr_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('104PU01', ntrl_Ca_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('104PU01', ntrl_Si_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('104PU01', ntrl_Al_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('104PU01', ntrl_Mg_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('104PU01', ntrl_Fe2_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('104PU01', ntrl_pH60_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('104PU01', ntrl_ORP_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('104PU01', ntrl_NTU_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('104PU01', ntrl_Ft_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('105TK13', tk13_Zn_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('105TK13', tk13_Solids_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('106TK01', MStk01_Zn_drop_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('106TK01', MStk01_Zn_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('106TK01', MStk01_Ni_3), how='outer').join(\n",
    "    monthLab_3.generate_df_from_analysis('105TK05', tk05_Pb).join(\n",
    "        monthLab_3.generate_df_from_analysis('105TK05', tk05_Zn), how='outer').join(\n",
    "        monthLab_3.generate_df_from_analysis('105TK05', tk05_Cu), how='outer').join(\n",
    "        monthLab_3.generate_df_from_analysis('105TK05', tk05_Ni), how='outer').join(\n",
    "        monthLab_3.generate_df_from_analysis('105TK05', tk05_Co), how='outer').join(\n",
    "        monthLab_3.generate_df_from_analysis('105TK05', tk05_Fe), how='outer').join(\n",
    "        monthLab_3.generate_df_from_analysis('105TK05', tk05_Mn), how='outer').join(\n",
    "        monthLab_3.generate_df_from_analysis('105TK05', tk05_Cr), how='outer').join(\n",
    "        monthLab_3.generate_df_from_analysis('105TK05', tk05_Ca), how='outer').join(\n",
    "        monthLab_3.generate_df_from_analysis('105TK05', tk05_Si), how='outer').join(\n",
    "        monthLab_3.generate_df_from_analysis('105TK05', tk05_Al), how='outer').join(\n",
    "        monthLab_3.generate_df_from_analysis('105TK05', tk05_Mg), how='outer'))\n",
    "\n",
    "for i in df_month_3.columns:\n",
    "    df_month_3[i] = pd.to_numeric(df_month_3[i], errors='coerce')\n",
    "\n",
    "df_month_3 = df_month_3.interpolate(method='linear', limit_direction='forward', axis=0)\n",
    "df_month_3['Zn_drop_adj'] = df_month_3['Zn_drop']*100\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------extracting data from the August 2020 dataset-__start__-----------------------------------------------------------------------\n",
    "monthLab_2 = LabResults(currentmonthyr_08_2020, filepath_08_2020) \n",
    "df_month_2 = monthLab_2.generate_df_from_analysis('104PU01', ntrl_Pb_3).join(\n",
    "    monthLab_2.generate_df_from_analysis('104PU01', ntrl_Zn_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('104PU01', ntrl_Cu_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('104PU01', ntrl_Ni_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('104PU01', ntrl_Co_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('104PU01', ntrl_Fe_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('104PU01', ntrl_Mn_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('104PU01', ntrl_Cr_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('104PU01', ntrl_Ca_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('104PU01', ntrl_Si_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('104PU01', ntrl_Al_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('104PU01', ntrl_Mg_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('104PU01', ntrl_Fe2_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('104PU01', ntrl_pH60_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('104PU01', ntrl_ORP_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('104PU01', ntrl_NTU_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('104PU01', ntrl_Ft_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('105TK13', tk13_Zn_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('105TK13', tk13_Solids_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('106TK01', MStk01_Zn_drop_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('106TK01', MStk01_Zn_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('106TK01', MStk01_Ni_3), how='outer').join(\n",
    "    monthLab_2.generate_df_from_analysis('105TK05', tk05_Pb).join(\n",
    "        monthLab_2.generate_df_from_analysis('105TK05', tk05_Zn), how='outer').join(\n",
    "        monthLab_2.generate_df_from_analysis('105TK05', tk05_Cu), how='outer').join(\n",
    "        monthLab_2.generate_df_from_analysis('105TK05', tk05_Ni), how='outer').join(\n",
    "        monthLab_2.generate_df_from_analysis('105TK05', tk05_Co), how='outer').join(\n",
    "        monthLab_2.generate_df_from_analysis('105TK05', tk05_Fe), how='outer').join(\n",
    "        monthLab_2.generate_df_from_analysis('105TK05', tk05_Mn), how='outer').join(\n",
    "        monthLab_2.generate_df_from_analysis('105TK05', tk05_Cr), how='outer').join(\n",
    "        monthLab_2.generate_df_from_analysis('105TK05', tk05_Ca), how='outer').join(\n",
    "        monthLab_2.generate_df_from_analysis('105TK05', tk05_Si), how='outer').join(\n",
    "        monthLab_2.generate_df_from_analysis('105TK05', tk05_Al), how='outer').join(\n",
    "        monthLab_2.generate_df_from_analysis('105TK05', tk05_Mg), how='outer'))\n",
    "#------------------------------------------------------------------------------------------extracting data from the August 2020 dataset-__end__-----------------------------------------------------------------------\n",
    "\n",
    "for i in df_month_2.columns:\n",
    "    df_month_2[i] = pd.to_numeric(df_month_2[i], errors='coerce')\n",
    "\n",
    "df_month_2 = df_month_2.interpolate(method='linear', limit_direction='forward', axis=0)\n",
    "df_month_2['Zn_drop_adj'] = df_month_2['Zn_drop']*100\n",
    "\n",
    "df_month2_3 = df_month_2.append(df_month_3, sort=True) #-----------combines the data for August2020 with the current available data for September2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------------#\n",
    "#---------------------Code for predicting the Zn drop value in 106TK01 by applying the coefficients  from RidgeRegression----------#\n",
    "\n",
    "#----Importing the Pi data for MS flow, rec gas, feed temp\n",
    "df_MSPi = pd.read_excel(r'C:\\Users\\v.t.flores\\Documents\\ZnDrop_Project_Pi_Sept2020.xlsx', sheet_name='PI_sept2020', index_col=False)\n",
    "\n",
    "#------------------Drop the row[0]. This is the title tag use in the pi datalink. We don't need it in pandas df.\n",
    "#------------------What remain are the column title and the rest of the rows of data.\n",
    "\n",
    "df_MSPi = df_MSPi.drop(df_MSPi.index[0])\n",
    "\n",
    "#Step2: Parse the DATETIME column\n",
    "df_MSPi['DATETIME'] = df_MSPi['DATETIME'].apply(lambda x: parse(str(x)))\n",
    "df_MSPi.set_index('DATETIME', inplace=True)\n",
    "\n",
    "\n",
    "# Step3: Convert each column to numeric to handle nan values and other comments like 'No data...'. Convering to numeric\n",
    "#------ converts them into a NaN value that pandas understands.\n",
    "for i in df_MSPi.columns:\n",
    "    df_MSPi[i] = pd.to_numeric(df_MSPi[i], errors='coerce')\n",
    "\n",
    "#Creating a column for the correct H2S flowrate\n",
    "df_MSPi['H2S_tk01'] = np.where(df_MSPi['H2S_FT_TK01_A']>df_MSPi['H2S_FT_TK01_B'],\\\n",
    "                                    df_MSPi['H2S_FT_TK01_A'], df_MSPi['H2S_FT_TK01_B'])\n",
    "\n",
    "df_MSPi['H2S_tk02'] = np.where(df_MSPi['H2S_FT_TK02_A']>df_MSPi['H2S_FT_TK02_B'],\\\n",
    "                                    df_MSPi['H2S_FT_TK02_A'], df_MSPi['H2S_FT_TK02_B'])    \n",
    "\n",
    "DF_labdata_and_Pi_Sept2020 = df_month2_3.join(df_MSPi, how='outer')\n",
    "\n",
    "DF_labdata_and_Pi_Sept2020_ntrpltd = DF_labdata_and_Pi_Sept2020.interpolate(method='linear', limit_direction='both', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REFER to the ZnDrop_Project_RidgeReg.ipynb file on how the df_reg_coef_intercepts_zndropPred.xlsx was calculated\n",
    "df_reg_coef_intercepts_zndropPred = pd.read_excel(r'C:\\Users\\v.t.flores\\Documents\\df_reg_coef_intercepts_zndropPred.xlsx', index_col=False)\n",
    "#\n",
    "#\n",
    "df_reg_coef_intercepts_zndropPred.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "reg_intercept = df_reg_coef_intercepts_zndropPred['intercept'][0]\n",
    "\n",
    "DF = DF_labdata_and_Pi_Sept2020_ntrpltd.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_3_input to have shape (34,) but got array with shape (35,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-1897750d9b7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m#predictions3 = new_model3.predict(normed_new_data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mpredictions4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_model4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormed_new_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mZN_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtk01_Zn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormed_new_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[0mNI_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtk01_Ni_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormed_new_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    906\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    909\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_or_infer_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m     x, _, _ = model._standardize_user_data(\n\u001b[1;32m--> 716\u001b[1;33m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[0;32m    717\u001b[0m     return predict_loop(\n\u001b[0;32m    718\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2469\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2470\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2471\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2473\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    570\u001b[0m                              \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m                              str(data_shape))\n\u001b[0m\u001b[0;32m    573\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_3_input to have shape (34,) but got array with shape (35,)"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------------------- Code block for tensorflow model------------------------------------------------------------------------\n",
    "\n",
    "#model.load(r'C:\\Users\\v.t.flores\\Documents\\Zn_drop_TF_model')\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "new_model3 = tf.keras.models.load_model(r'C:\\Users\\v.t.flores\\Documents\\Zn_drop_TF_model3')\n",
    "new_model2 = tf.keras.models.load_model(r'C:\\Users\\v.t.flores\\Documents\\Zn_drop_TF_model2')\n",
    "new_model4 = tf.keras.models.load_model(r'C:\\Users\\v.t.flores\\Documents\\Zn_drop_TF_model4')\n",
    "\n",
    "tk01_Zn_model = tf.keras.models.load_model(r'C:\\Users\\v.t.flores\\Documents\\Zn_TF_model1')\n",
    "tk01_Ni_model = tf.keras.models.load_model(r'C:\\Users\\v.t.flores\\Documents\\Ni_TF_model1')\n",
    "\n",
    "# slice the DF for the columns that will be fed into the tensorflow model\n",
    "\n",
    "DF_TF = DF[['104_Al', '104_Ca', '104_Co', '104_Cr', '104_Cu', '104_Fe', '104_Fe2',\n",
    "       '104_Ft', '104_Mg', '104_Mn', '104_NTU', '104_Ni', '104_ORP', '104_Pb',\n",
    "       '104_Si', '104_Zn', '104_pH60', 'DeZn_Feed_Flow', 'DeZn_Feed_T',\n",
    "       'DeZn_Seed_Flow', 'DeZn_TK05_T',\n",
    "       'H2S_tk01', 'H2S_tk02', 'tk05_Al',\n",
    "       'tk05_Ca', 'tk05_Co', 'tk05_Cr', 'tk05_Cu', 'tk05_Fe', 'tk05_Mg',\n",
    "       'tk05_Mn', 'tk05_Ni', 'tk05_Pb', 'tk05_Si', 'tk05_Zn']]\n",
    "\n",
    "\n",
    "# look at the overall statistics:\n",
    "new_data_stats = DF_TF.describe()\n",
    "new_data_stats = new_data_stats.transpose()\n",
    "\n",
    "def norm(x):\n",
    "    return (x - new_data_stats['mean']) / new_data_stats['std']\n",
    "normed_new_data = norm(DF_TF)\n",
    "\n",
    "#predictions2 = new_model2.predict(normed_new_data)\n",
    "#predictions3 = new_model3.predict(normed_new_data)\n",
    "predictions4 = new_model4.predict(normed_new_data)\n",
    "ZN_predictions = tk01_Zn_model.predict(normed_new_data)\n",
    "NI_predictions = tk01_Ni_model.predict(normed_new_data)\n",
    "\n",
    "\n",
    "#DF_TF['TF_pred2'] = predictions2\n",
    "#DF_TF['TF_pred3'] = predictions3\n",
    "DF_TF['TF_pred4'] = predictions4\n",
    "\n",
    "DF_TF['MSTK01_Zn_Pred'] = ZN_predictions\n",
    "DF_TF['MSTK01_Ni_Pred'] = NI_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create the dataframe for DeZn_TK05 Temp v. Zn Drop\n",
    "\n",
    "df_test_columns_Include = ['104_Al', '104_Ca', '104_Co', '104_Cr', '104_Cu', '104_Fe', '104_Fe2',\n",
    "       '104_Ft', '104_Mg', '104_Mn', '104_NTU', '104_Ni', '104_ORP', '104_Pb',\n",
    "       '104_Si', '104_Zn', '104_pH60', 'DeZn_Feed_Flow', 'DeZn_Feed_T',\n",
    "       'DeZn_Seed_Flow', 'DeZn_TK05_T', \n",
    "       'H2S_tk01', 'H2S_tk02', 'tk05_Al',\n",
    "       'tk05_Ca', 'tk05_Co', 'tk05_Cr', 'tk05_Cu', 'tk05_Fe', 'tk05_Mg',\n",
    "       'tk05_Mn', 'tk05_Ni', 'tk05_Pb', 'tk05_Si', 'tk05_Zn']\n",
    "\n",
    "DF_test2 = DF_TF[df_test_columns_Include].dropna()[-1:] # DF_test2 is the last non NA value in the dataframe\n",
    "\n",
    "#DF_test2 = DF_TF[df_test_columns_Include][757:758]\n",
    "\n",
    "temp_range_start = round((DF_test2['DeZn_TK05_T'][0]-10), 0)\n",
    "temp_range_end = round((DF_test2['DeZn_TK05_T'][0]+10), 0)\n",
    "temp_range = np.linspace(temp_range_start, temp_range_end, 20)\n",
    "\n",
    "zn_drop_pred = []\n",
    "x_list = []\n",
    "for temp in temp_range:\n",
    "    DF_test2['DeZn_TK05_T'] = temp\n",
    "    normed_DF_test2 = norm(DF_test2)\n",
    "    predictions2 = new_model2.predict(normed_DF_test2)\n",
    "    zn_drop_pred.append(predictions2[0][0])\n",
    "    x_list.append(temp)\n",
    "    \n",
    "df_tk05temp_v_zndrop = pd.DataFrame(zip(x_list, zn_drop_pred), columns=['DeZn_TK05_T', 'Zn_drop'])\n",
    "df_tk05temp_v_zndrop = df_tk05temp_v_zndrop.set_index('DeZn_TK05_T')\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create the dataframe for H2S_tk01 v. Zn drop\n",
    "\n",
    "gas_tk01_start = round(DF_test2['H2S_tk01'][0]-10, 2)\n",
    "gas_tk01_end = round(DF_test2['H2S_tk01'][0]+10, 2)\n",
    "gas_1_range = np.linspace(gas_tk01_start, gas_tk01_end, 20)\n",
    "\n",
    "zn_drop_pred_h2s1 = []\n",
    "x_list_h2s1 = []\n",
    "for gas1 in gas_1_range:\n",
    "    DF_test2['H2S_tk01'] = gas1\n",
    "    normed_DF_test2 = norm(DF_test2)\n",
    "    predictions2 = new_model2.predict(normed_DF_test2)\n",
    "    zn_drop_pred_h2s1.append(predictions2[0][0])\n",
    "    x_list_h2s1.append(gas1)\n",
    "    \n",
    "df_H2STK01_v_zndrop = pd.DataFrame(zip(x_list_h2s1, zn_drop_pred_h2s1), columns=['H2S_tk01', 'Zn_drop'])\n",
    "df_H2STK01_v_zndrop = df_H2STK01_v_zndrop.set_index('H2S_tk01')\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Create the dataframe for H2S_tk02 v. Zn drop\n",
    "\n",
    "gas_tk02_start = round(DF_test2['H2S_tk02'][0]-20, 2)\n",
    "gas_tk02_end = round(DF_test2['H2S_tk02'][0]+20, 2)\n",
    "\n",
    "gas_2_range = np.linspace(gas_tk02_start, gas_tk02_end, 40)\n",
    "\n",
    "\n",
    "zn_drop_pred_h2s2 = []\n",
    "x_list_h2s2 = []\n",
    "for gas2 in gas_2_range:\n",
    "    DF_test2['H2S_tk02'] = gas2\n",
    "    normed_DF_test2 = norm(DF_test2)\n",
    "    predictions2 = new_model2.predict(normed_DF_test2)\n",
    "    zn_drop_pred_h2s2.append(predictions2[0][0])\n",
    "    x_list_h2s2.append(gas2)\n",
    "    \n",
    "\n",
    "    \n",
    "df_H2STK02_v_zndrop = pd.DataFrame(zip(x_list_h2s2, zn_drop_pred_h2s2), columns=['H2S_tk02', 'Zn_drop'])\n",
    "df_H2STK02_v_zndrop = df_H2STK02_v_zndrop.set_index('H2S_tk02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Create the dataframe for varying H2S_tk01 and H2S_tk02 combination and predicting the Zn drop\n",
    "\n",
    "df_test_columns_Include = ['104_Al', '104_Ca', '104_Co', '104_Cr', '104_Cu', '104_Fe', '104_Fe2',\n",
    "       '104_Ft', '104_Mg', '104_Mn', '104_NTU', '104_Ni', '104_ORP', '104_Pb',\n",
    "       '104_Si', '104_Zn', '104_pH60', 'DeZn_Feed_Flow', 'DeZn_Feed_T',\n",
    "       'DeZn_Seed_Flow', 'DeZn_TK05_T', 'H2S_tk01', 'H2S_tk02', 'tk05_Al',\n",
    "       'tk05_Ca', 'tk05_Co', 'tk05_Cr', 'tk05_Cu', 'tk05_Fe', 'tk05_Mg',\n",
    "       'tk05_Mn', 'tk05_Ni', 'tk05_Pb', 'tk05_Si', 'tk05_Zn']\n",
    "\n",
    "zn_drop_pred_h2s3 = []\n",
    "#x_list_h2s3 = []\n",
    "x_list_h2s1_3 =[]\n",
    "x_list_h2s2_3 =[]\n",
    "for tk01_gas in gas_1_range:\n",
    "    for tk02_gas in gas_2_range:\n",
    "        DF_test2['H2S_tk01'] = tk01_gas\n",
    "        DF_test2['H2S_tk02'] = tk02_gas\n",
    "        \n",
    "        normed_DF_test2 = norm(DF_test2)\n",
    "        predictions2 = new_model2.predict(normed_DF_test2)\n",
    "        zn_drop_pred_h2s3.append(predictions2[0][0])\n",
    "        x_list_h2s1_3.append(tk01_gas)\n",
    "        x_list_h2s2_3.append(tk02_gas)\n",
    "    \n",
    "df_H2STK01_v_zndrop_fac = pd.DataFrame(zip(x_list_h2s1_3, zn_drop_pred_h2s3), columns=['H2S_tk01', 'Zn_drop'])\n",
    "df_H2STK02_v_zndrop_fac = pd.DataFrame(zip(x_list_h2s2_3, zn_drop_pred_h2s3), columns=['H2S_tk02', 'Zn_drop'])\n",
    "\n",
    "\n",
    "\n",
    "df_H2S_tk01_02 = pd.merge(df_H2STK01_v_zndrop_fac, df_H2STK02_v_zndrop_fac, on=\"Zn_drop\") #df containing zndrop, h2s_tk01, h2s_tk02\n",
    "\n",
    "\n",
    "# Create a df containing the H2S flowrate to tk01 and tk02 and temperature of tk05 for the maximum zn_drop and minimum zn drop\n",
    "\n",
    "max_zndrop = df_H2S_tk01_02[df_H2S_tk01_02['Zn_drop'] == df_H2S_tk01_02['Zn_drop'].max()]\n",
    "min_zndrop = df_H2S_tk01_02[df_H2S_tk01_02['Zn_drop'] == df_H2S_tk01_02['Zn_drop'].min()]\n",
    "\n",
    "# Compute the latest H2S factor\n",
    "\n",
    "Total_H2S_105 = DF_TF['H2S_tk01'][-1]+DF_TF['H2S_tk02'][-1]\n",
    "H2S_factor = Total_H2S_105/(DF_TF['DeZn_Feed_Flow'][-1]*DF_TF['104_Zn'][-1:]*22.4/65)\n",
    "\n",
    "# Compute the H2S factor for the minimum and maximum zn drop\n",
    "#\n",
    "#\n",
    "\n",
    "H2S_tk01_min = min_zndrop['H2S_tk01'].iloc[0]\n",
    "H2S_tk02_min = min_zndrop['H2S_tk02'].iloc[0]\n",
    "Total_H2S_105_min = H2S_tk01_min + H2S_tk02_min\n",
    "\n",
    "H2S_factor_min = Total_H2S_105_min/(DF_TF['DeZn_Feed_Flow'][-1]*DF_TF['104_Zn'][-1:]*22.4/65) # the H2S factor at the minimum zn drop\n",
    "Fraction_H2S_tk01_min = H2S_tk01_min/Total_H2S_105_min\n",
    "Fraction_H2S_tk02_min = H2S_tk02_min/Total_H2S_105_min\n",
    "\n",
    "recomm_text1 = '<b>{:.2f}</b> : H2S Factor at min_Zn_drop (<b>{:.2f}</b>) <br> H2S Ratio (tk01:tk02): {:.2f}:{:.2f}'.format(H2S_factor_min.iloc[0], min_zndrop['Zn_drop'].iloc[0],Fraction_H2S_tk01_min, Fraction_H2S_tk02_min)\n",
    "\n",
    "# Compute the H2S factor for the minimum and maximum zn drop\n",
    "\n",
    "H2S_tk01_max = max_zndrop['H2S_tk01'].iloc[0]\n",
    "H2S_tk02_max = max_zndrop['H2S_tk02'].iloc[0]\n",
    "Total_H2S_105_max = H2S_tk01_max + H2S_tk02_max\n",
    "\n",
    "H2S_factor_max = Total_H2S_105_max/(DF_TF['DeZn_Feed_Flow'][-1]*DF_TF['104_Zn'][-1:]*22.4/65)\n",
    "Fraction_H2S_tk01_max = H2S_tk01_max/Total_H2S_105_max\n",
    "Fraction_H2S_tk02_max = H2S_tk02_max/Total_H2S_105_max\n",
    "\n",
    "H2S_factor_max = Total_H2S_105_max/(DF_TF['DeZn_Feed_Flow'][-1]*DF_TF['104_Zn'][-1:]*22.4/65) # the H2S factor at the maximum zn drop\n",
    "\n",
    "recomm_text2 = '<b>{:.2f}</b> : H2S Factor at max_Zn_drop (<b>{:.2f}</b>) <br> H2S Ratio (tk01:tk02): {:.2f}:{:.2f}'.format(H2S_factor_max.iloc[0], max_zndrop['Zn_drop'].iloc[0], Fraction_H2S_tk01_max, Fraction_H2S_tk02_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot for the Zn drop predictions and the sensitivity to tk05 temp and H2Stk01 and tk02 flow\n",
    "#\n",
    "# DF prerequisite: DF, DF_TF\n",
    "\n",
    "figzndrop_axis_truefalse_list = [False, False, False]\n",
    "subplotTitle = ['Zn Drop','Temperature Sensitivity', 'H2S_TK01 Flowrate Sensitivity', \n",
    "                     'H2S_TK02 Flowrate Sensitivity',\"\",\"\",\"\",\"\"]\n",
    "\n",
    "\n",
    "\n",
    "figzndrop_PRED_TF = make_subplots(\n",
    "    rows=2, cols=4,\n",
    "    specs=[[{\"rowspan\": 1, \"colspan\": 4},None, None, None], [{\"type\": \"scatter\"}, {\"type\": \"scatter\"},  {\"type\": \"scatter\"}, None]],\n",
    "    subplot_titles=(subplotTitle),\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "figzndrop_axis_truefalse_list = [False]\n",
    "generate_plot_from_df(figzndrop_PRED_TF, DF[['Zn_drop_adj',]], figzndrop_axis_truefalse_list)\n",
    "\n",
    "figzndrop_PRED_TF.add_trace(go.Scatter(x=DF_TF.index, y=DF_TF['TF_pred2'], name='TF_pred2'), secondary_y=False,) #This code adds the TF_pred2 to the plot\n",
    "\n",
    "# Mark the datetime when there were problems with the pidata\n",
    "figzndrop_PRED_TF.add_shape(\n",
    "        # filled Rectangle\n",
    "            type=\"rect\",\n",
    "            x0='2020-08-17 23:00:00',\n",
    "            y0=-1000,\n",
    "            x1='2020-08-19 13:00:00',\n",
    "            y1=200,\n",
    "            fillcolor=\"LightSalmon\",\n",
    "            opacity=0.5,\n",
    "            layer=\"below\",\n",
    "            line_width=0,\n",
    "        )\n",
    "figzndrop_PRED_TF.add_shape(\n",
    "        # filled Rectangle\n",
    "            type=\"rect\",\n",
    "            x0='2020-08-26 16:00:00',\n",
    "            y0=-1000,\n",
    "            x1='2020-08-28 18:00:00',\n",
    "            y1=200,\n",
    "            fillcolor=\"LightSalmon\",\n",
    "            opacity=0.5,\n",
    "            layer=\"below\",\n",
    "            line_width=0,\n",
    "        )\n",
    "\n",
    "figzndrop_PRED_TF.add_shape(\n",
    "        # filled Rectangle\n",
    "            type=\"rect\",\n",
    "            x0='2020-08-26 03:00:00',\n",
    "            y0=-1000,\n",
    "            x1='2020-08-26 06:00:00',\n",
    "            y1=200,\n",
    "            fillcolor=\"LightSalmon\",\n",
    "            opacity=0.5,\n",
    "            layer=\"below\",\n",
    "            line_width=0,\n",
    "        )\n",
    "\n",
    "figzndrop_PRED_TF.update_shapes(dict(xref='x', yref='y'))\n",
    "\n",
    "#figzndrop_PRED.add_trace(go.Scatter(x=DF.loc['2020-08-17 23:00:00':'2020-08-19 13:00:00'], y=DF['ZnDrop_pred'], name='ZnDrop_pred'), secondary_y=True,)\n",
    "\n",
    "\n",
    "title_plotZNDROP = \"<b>Zn Drop Pred Mach 2 Beta</b>\" + '<br>'+ \"Bottom subplots show zn drop sensitivity to 105TK05 temp, H2S to TK01, H2S to TK02\" + '<br>' + 'Updated: {}'.format(datetime.datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\")) + \"<br>\" + \"\"\n",
    "figzndrop_PRED_TF.update_layout(title_text=title_plotZNDROP,hovermode=\"x\",\n",
    "                       hoverdistance=100, # Distance to show hover label of data point\n",
    "                       spikedistance=1000, # Distance to show spike\n",
    "                       xaxis=dict(linecolor=\"#BCCCDC\",\n",
    "                                   showspikes=True, # Show spike line for X-axis\n",
    "                                   # Format spike\n",
    "                                   spikethickness=2,\n",
    "                                   spikedash=\"dot\",\n",
    "                                   spikecolor=\"#999999\",\n",
    "                                   spikemode=\"across\"), font=dict(size=10)\n",
    "                               )\n",
    "# Adds figure title\n",
    "figzndrop_PRED_TF.update_xaxes(title_text=\"DateTime\", row=1, col=1) # Set x-axis title\n",
    "figzndrop_PRED_TF.update_yaxes(title_text=\"Zndrop\", row=1, col=1, secondary_y=False) # Set y-axes titles primary axis\n",
    "\n",
    "#\n",
    "#       \n",
    "figzndrop_PRED_TF.add_trace(go.Scatter(x=df_tk05temp_v_zndrop.index,\n",
    "                             y=df_tk05temp_v_zndrop['Zn_drop'].dropna(how='any'),\n",
    "                             name='Temp Sensitivity'),row=2, col=1)\n",
    "figzndrop_PRED_TF.add_trace(go.Scatter(x=df_H2STK01_v_zndrop.index,\n",
    "                             y=df_H2STK01_v_zndrop['Zn_drop'].dropna(how='any'),\n",
    "                             name='H2S_TK01 Sensitivity'),row=2, col=2)\n",
    "figzndrop_PRED_TF.add_trace(go.Scatter(x=df_H2STK02_v_zndrop.index,\n",
    "                             y=df_H2STK02_v_zndrop['Zn_drop'].dropna(how='any'),\n",
    "                             name='H2S_TK02 Sensitivity'),row=2, col=3)\n",
    "\n",
    "# Adds figure title\n",
    "figzndrop_PRED_TF.update_xaxes(title_text=\"DeZn_TK05_T (deg Celcius)\", row=2, col=1)# Set x-axis title\n",
    "figzndrop_PRED_TF.update_yaxes(title_text=\"Zndrop (%)\", row=2, col=1, secondary_y=False) # Set y-axes titles primary axis\n",
    "\n",
    "figzndrop_PRED_TF.update_xaxes(title_text=\"H2S_TK01_Flow (m3/h) \",  row=2, col=2) # Set x-axis title\n",
    "figzndrop_PRED_TF.update_yaxes(title_text=\"Zndrop (%)\", row=2, col=2, secondary_y=False) # Set y-axes titles primary axis\n",
    "\n",
    "figzndrop_PRED_TF.update_xaxes(title_text=\"H2S_TK02_Flow (m3/h)\", row=2, col=3) # Set x-axis title\n",
    "figzndrop_PRED_TF.update_yaxes(title_text=\"Zndrop (%)\", row=2, col=3, secondary_y=False) # Set y-axes titles primary axis\n",
    "\n",
    "figzndrop_PRED_TF.update_layout(annotations=[\n",
    "            go.layout.Annotation(\n",
    "                text='Latest H2S Factor: <b>{:.2f}</b>'.format(H2S_factor[0])+ '<br><br>' + recomm_text1+'<br><br>'+recomm_text2+'<br>'+\n",
    "                '<br> For the PLS on {}'.format(DF_test2.index[-1].strftime(\"%m/%d/%Y %H:%M:%S\"))+\"<br>\",\n",
    "                align='left',\n",
    "                showarrow=False,\n",
    "                xref='paper',\n",
    "                yref='paper',\n",
    "                x=.9,\n",
    "                y=.09,\n",
    "                #width=300,\n",
    "                #height=250,\n",
    "                #bgcolor= \"rgba(1,1,1,0.2)\",\n",
    "                bordercolor='gray',\n",
    "                borderwidth=0.5,\n",
    "                font=dict(\n",
    "        size=12)                 \n",
    "            )\n",
    "        ]\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------Code for adding and marking the plot with the latest value-------------------------------------------------------------------\n",
    "\n",
    "# arguments latest_val is the last good value in the dataset, df is the main dataset for the combined Pi and Laboratory Results\n",
    "def insert_temp_to_dfrange_get_index(latest_val, df):\n",
    "    for i in range(0, len(df.index)):\n",
    "        if (latest_val >= df.index[i]) & (latest_val <= df.index[i+1]):\n",
    "            xi = i\n",
    "            break;\n",
    "        else:\n",
    "            pass\n",
    "    return xi\n",
    "\n",
    "latest_temp_i = DF['DeZn_TK05_T'][-1]\n",
    "xi = insert_temp_to_dfrange_get_index(latest_temp_i, df_tk05temp_v_zndrop)\n",
    "\n",
    "latest_tk01_i = DF['H2S_tk01'][-1]\n",
    "x_tk01_i = insert_temp_to_dfrange_get_index(latest_tk01_i, df_H2STK01_v_zndrop)\n",
    "\n",
    "latest_tk02_i = DF['H2S_tk02'][-1]\n",
    "x_tk02_i = insert_temp_to_dfrange_get_index(latest_tk02_i, df_H2STK02_v_zndrop)\n",
    "\n",
    "figzndrop_PRED_TF.add_trace(go.Scatter(x=[df_tk05temp_v_zndrop.index[xi]],\n",
    "                             y=[df_tk05temp_v_zndrop['Zn_drop'].iloc[xi]],\n",
    "                             name='Latest DeZn TK05 Temp', marker=dict(size=10)),row=2, col=1)\n",
    "\n",
    "figzndrop_PRED_TF.add_trace(go.Scatter(x=[df_H2STK01_v_zndrop.index[x_tk01_i]],\n",
    "                             y=[df_H2STK01_v_zndrop['Zn_drop'].iloc[x_tk01_i]],\n",
    "                             name='Latest H2S_TK01_flow', marker=dict(size=10)),row=2, col=2)\n",
    "\n",
    "figzndrop_PRED_TF.add_trace(go.Scatter(x=[df_H2STK02_v_zndrop.index[x_tk02_i]],\n",
    "                             y=[df_H2STK02_v_zndrop['Zn_drop'].iloc[x_tk02_i]],\n",
    "                             name='Latest H2S_TK02_flow', marker=dict(size=10)),row=2, col=3)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------End---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot for the Zn drop predictions and the sensitivity to tk05 temp and H2Stk01 and tk02 flow\n",
    "#\n",
    "# DF prerequisite: DF, DF_TF\n",
    "\n",
    "#figznNi_axis_truefalse_list = [False, False, True, True]\n",
    "figznNi_PRED_TF = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    specs=[[{\"type\": \"scatter\"}]],\n",
    ")\n",
    "\n",
    "#generate_plot_from_df(figzn_PRED_TF, DF[['106_Zn']], True)\n",
    "\n",
    "figzn_PRED_TF.add_trace(go.Scatter(x=DF_TF.index, y=DF_TF['MSTK01_Zn_Pred'], name='MSTK01_Zn_Pred'), secondary_y=True,) #This code adds the TF_pred2 to the plot\n",
    "figzn_PRED_TF.add_trace(go.Scatter(x=DF_TF.index, y=DF_TF['MSTK01_Ni_Pred'], name='MSTK01_Ni_Pred'), secondary_y=False,) #This code adds the TF_pred2 to the plot\n",
    "\n",
    "figzn_PRED_TF.add_trace(go.Scatter(x=DF.index, y=DF['106_Zn'], name='106_Zn'), secondary_y=False,) #This code adds the TF_pred2 to the plot\n",
    "figzn_PRED_TF.add_trace(go.Scatter(x=DF.index, y=DF['106_Ni'], name='106_Ni'), secondary_y=False,) #This code adds the TF_pred2 to the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------End----------------------------------------------------------------------------\n",
    "plt.close()\n",
    "with open(r'C:\\Users\\v.t.flores\\Documents\\Zndrop_pred_mach2_b.html', 'w') as f:\n",
    "    f.write(figzndrop_PRED_TF.to_html(figzndrop_PRED_TF, full_html=True, include_plotlyjs='cdn'))\n",
    "    f.write(figzn_PRED_TF.to_html(figzn_PRED_TF, full_html=True, include_plotlyjs='cdn'))\n",
    "    \n",
    "#-------------------------------------------------------------------------End----------------------------------------------------------------------------\n",
    "plt.close()\n",
    "with open(r'\\\\THGOBNA002\\thpal\\05 Production\\150 DCS\\Miscellaneous\\Zndrop_pred_mach2_b.html', 'w') as f:\n",
    "    f.write(figzndrop_PRED_TF.to_html(figzndrop_PRED_TF, full_html=True, include_plotlyjs='cdn'))\n",
    "    f.write(figzn_PRED_TF.to_html(figzn_PRED_TF, full_html=True, include_plotlyjs='cdn'))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
