{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT THE LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exc1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from dateutil.parser import parse\n",
    "from itertools import compress\n",
    "from collections import OrderedDict\n",
    "from datetime import date\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD THE PI DATA and LABORATORY ANALYSIS RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for parsing dates that are valid\n",
    "def get_datesFromPi(df_col):\n",
    "    for i in range(1, len(df_col)):\n",
    "        try:\n",
    "            df_col[i] = parse(str(df_col[i]))\n",
    "        except ValueError:\n",
    "            break\n",
    "    return df_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sheet From Pi\n",
    "#Step1: Load the Data for the month\n",
    "df_JAN2020Pi = pd.read_excel(r'C:\\Users\\v.t.flores\\Documents\\Pressure Filter Inv_2019_2.xlsx', sheet_name='PiData_PressureFilter', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the row[1]\n",
    "df_JAN2020Pi = df_JAN2020Pi.drop(df_JAN2020Pi.index[0])\n",
    "df_JAN2020Pi['DATETIME'] = get_datesFromPi(df_JAN2020Pi['DATETIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_JAN2020Pi = df_JAN2020Pi[df_JAN2020Pi['DATETIME'] != str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATETIME</th>\n",
       "      <th>A_t_CWSH1</th>\n",
       "      <th>B_t_CWSH1</th>\n",
       "      <th>A_t_CWSH2</th>\n",
       "      <th>B_t_CWSH2</th>\n",
       "      <th>A_t_DRY</th>\n",
       "      <th>B_t_DRY</th>\n",
       "      <th>A_t_FEED</th>\n",
       "      <th>B_t_FEED</th>\n",
       "      <th>A_t_PRESS2</th>\n",
       "      <th>...</th>\n",
       "      <th>106FT02A_TOTWT</th>\n",
       "      <th>106PU26B_CUR</th>\n",
       "      <th>106PU27B_CUR</th>\n",
       "      <th>PU26B_OP</th>\n",
       "      <th>PU27B_OP</th>\n",
       "      <th>FT02B_MANFP</th>\n",
       "      <th>FT02B_CWP</th>\n",
       "      <th>FT02B_TOTWT</th>\n",
       "      <th>TH01_UF_DNST1</th>\n",
       "      <th>TH01_UF_DNST2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019-02-01 00:00:00</td>\n",
       "      <td>235</td>\n",
       "      <td>160</td>\n",
       "      <td>230</td>\n",
       "      <td>63</td>\n",
       "      <td>160</td>\n",
       "      <td>135</td>\n",
       "      <td>473</td>\n",
       "      <td>454</td>\n",
       "      <td>105</td>\n",
       "      <td>...</td>\n",
       "      <td>2.93823e+06</td>\n",
       "      <td>21.6828</td>\n",
       "      <td>22.4831</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>3.67461e+06</td>\n",
       "      <td>1099.84</td>\n",
       "      <td>1100.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019-02-01 00:01:00</td>\n",
       "      <td>235</td>\n",
       "      <td>160</td>\n",
       "      <td>230</td>\n",
       "      <td>114</td>\n",
       "      <td>160</td>\n",
       "      <td>135</td>\n",
       "      <td>473</td>\n",
       "      <td>454</td>\n",
       "      <td>105</td>\n",
       "      <td>...</td>\n",
       "      <td>2.93823e+06</td>\n",
       "      <td>21.7512</td>\n",
       "      <td>22.5925</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>3.67461e+06</td>\n",
       "      <td>1098.61</td>\n",
       "      <td>1100.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019-02-01 00:02:00</td>\n",
       "      <td>235</td>\n",
       "      <td>160</td>\n",
       "      <td>230</td>\n",
       "      <td>168</td>\n",
       "      <td>160</td>\n",
       "      <td>135</td>\n",
       "      <td>473</td>\n",
       "      <td>454</td>\n",
       "      <td>105</td>\n",
       "      <td>...</td>\n",
       "      <td>2.93823e+06</td>\n",
       "      <td>21.8675</td>\n",
       "      <td>22.572</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>3.67461e+06</td>\n",
       "      <td>1097.28</td>\n",
       "      <td>1100.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019-02-01 00:03:00</td>\n",
       "      <td>235</td>\n",
       "      <td>160</td>\n",
       "      <td>230</td>\n",
       "      <td>190</td>\n",
       "      <td>160</td>\n",
       "      <td>135</td>\n",
       "      <td>473</td>\n",
       "      <td>454</td>\n",
       "      <td>105</td>\n",
       "      <td>...</td>\n",
       "      <td>2.93823e+06</td>\n",
       "      <td>21.7649</td>\n",
       "      <td>22.5925</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>3.67461e+06</td>\n",
       "      <td>1096.79</td>\n",
       "      <td>1100.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2019-02-01 00:04:00</td>\n",
       "      <td>235</td>\n",
       "      <td>160</td>\n",
       "      <td>230</td>\n",
       "      <td>190</td>\n",
       "      <td>160</td>\n",
       "      <td>135</td>\n",
       "      <td>473</td>\n",
       "      <td>454</td>\n",
       "      <td>105</td>\n",
       "      <td>...</td>\n",
       "      <td>2.93823e+06</td>\n",
       "      <td>21.888</td>\n",
       "      <td>22.6609</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>3.67461e+06</td>\n",
       "      <td>1098.13</td>\n",
       "      <td>1100.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44636</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44637</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44638</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44639</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44640</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44640 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  DATETIME A_t_CWSH1 B_t_CWSH1 A_t_CWSH2 B_t_CWSH2 A_t_DRY  \\\n",
       "1      2019-02-01 00:00:00       235       160       230        63     160   \n",
       "2      2019-02-01 00:01:00       235       160       230       114     160   \n",
       "3      2019-02-01 00:02:00       235       160       230       168     160   \n",
       "4      2019-02-01 00:03:00       235       160       230       190     160   \n",
       "5      2019-02-01 00:04:00       235       160       230       190     160   \n",
       "...                    ...       ...       ...       ...       ...     ...   \n",
       "44636                                                                        \n",
       "44637                                                                        \n",
       "44638                                                                        \n",
       "44639                                                                        \n",
       "44640                                                                        \n",
       "\n",
       "      B_t_DRY A_t_FEED B_t_FEED A_t_PRESS2  ... 106FT02A_TOTWT 106PU26B_CUR  \\\n",
       "1         135      473      454        105  ...    2.93823e+06      21.6828   \n",
       "2         135      473      454        105  ...    2.93823e+06      21.7512   \n",
       "3         135      473      454        105  ...    2.93823e+06      21.8675   \n",
       "4         135      473      454        105  ...    2.93823e+06      21.7649   \n",
       "5         135      473      454        105  ...    2.93823e+06       21.888   \n",
       "...       ...      ...      ...        ...  ...            ...          ...   \n",
       "44636                                       ...                               \n",
       "44637                                       ...                               \n",
       "44638                                       ...                               \n",
       "44639                                       ...                               \n",
       "44640                                       ...                               \n",
       "\n",
       "      106PU27B_CUR PU26B_OP PU27B_OP FT02B_MANFP FT02B_CWP  FT02B_TOTWT  \\\n",
       "1          22.4831       30       30        0.01     -0.01  3.67461e+06   \n",
       "2          22.5925       30       30        0.01     -0.01  3.67461e+06   \n",
       "3           22.572       30       30        0.01     -0.01  3.67461e+06   \n",
       "4          22.5925       30       30       -0.01     -0.01  3.67461e+06   \n",
       "5          22.6609       30       30       -0.01     -0.01  3.67461e+06   \n",
       "...            ...      ...      ...         ...       ...          ...   \n",
       "44636                                                                     \n",
       "44637                                                                     \n",
       "44638                                                                     \n",
       "44639                                                                     \n",
       "44640                                                                     \n",
       "\n",
       "      TH01_UF_DNST1 TH01_UF_DNST2  \n",
       "1           1099.84       1100.37  \n",
       "2           1098.61       1100.64  \n",
       "3           1097.28       1100.64  \n",
       "4           1096.79       1100.73  \n",
       "5           1098.13       1100.73  \n",
       "...             ...           ...  \n",
       "44636                              \n",
       "44637                              \n",
       "44638                              \n",
       "44639                              \n",
       "44640                              \n",
       "\n",
       "[44640 rows x 29 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_JAN2020Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_JAN2020Pi.set_index('DATETIME', inplace=True)\n",
    "df_comb = df_JAN2020Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 2020-02-02 11:09:26.232815\n",
      "Done. Excel PiData files loaded as dataframes.  2020-02-02 11:09:26.232815\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "wb1 = openpyxl.load_workbook(r'\\\\thgobna001\\userdata\\THPAL\\Department\\TAGANITO\\SHARED\\AnalysisResult\\ANALYSIS RESULTS 2019\\2.) February 2019.xlsx')\n",
    "\n",
    "FT02wb = wb1['106FT02AB'] #####This block of code appends the Analysis Results\n",
    "\n",
    "currentmonthyr = datetime.date(2019,2,1) #update for the targetmonth\n",
    "\n",
    "print('Done', datetime.datetime.now())\n",
    "print('Done. Excel PiData files loaded as dataframes. ', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DECLARE THE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all functions declared 2020-02-02 11:09:33.479180\n"
     ]
    }
   ],
   "source": [
    "#---fcn defn--start\n",
    "#This function is used to replace the 'No good/ not enough data for calculation' value in the dataframe for data from PiData\n",
    "\n",
    "def det_no_data(value):\n",
    "    if type(value) == str:\n",
    "        reg = re.search(r'\\bCalculation\\b', value)\n",
    "        return reg[0]\n",
    "    else:\n",
    "        return value\n",
    "#------fcn defn end\n",
    "\n",
    "#fcn definition        \n",
    "#used in the laboratory analysis results data\n",
    "\n",
    "def filter_detrmin(cell_addrs):\n",
    "    PF_regex2 = re.compile(r'[a-z0-8]', re.IGNORECASE| re.VERBOSE|re.DOTALL)\n",
    "    filtername2 = PF_regex2.findall(str(cell_addrs))\n",
    "    for i in range(0, len(filtername2)):\n",
    "        if filtername2[i] != filtername2[-1]:\n",
    "            if filtername2[i]+filtername2[i+1] == '2A':\n",
    "                PFname = 'A'\n",
    "                return PFname\n",
    "            if filtername2[i]+filtername2[i+1] == '2B':\n",
    "                PFname = 'B'\n",
    "                return PFname      \n",
    "#-----------------end of function defn-----------        \n",
    "\n",
    "#fcn dfn removes the last character in a string if not an integer. does nothing otherwise\n",
    "def check_ending_char(str_var):\n",
    "    try:\n",
    "        if type(int(str_var[-1])) != int:\n",
    "                c = str_var[0:len(str_var)-1]\n",
    "        else:\n",
    "            c = str_var\n",
    "    except ValueError:\n",
    "        c = str_var[0:len(str_var)-1]\n",
    "    return c\n",
    "\n",
    "#fcn dfn removes the starting character in a string if not an integer. does nothing otherwise\n",
    "def check_starting_char(str_var):\n",
    "    try:\n",
    "        if type(int(str_var[0])) != int:\n",
    "            c = str_var[1:]\n",
    "        else:\n",
    "            c = str_var\n",
    "    except ValueError:\n",
    "        c = str_var[1:]\n",
    "    return c\n",
    "\n",
    "#fcn dfn\n",
    "#used in the analysis results data. cleans up the sampling time entry\n",
    "def time_cleaner(x):\n",
    "    time_regex = re.compile(r'[0-8a-zA-Z:]', re.IGNORECASE| re.VERBOSE|re.DOTALL)\n",
    "    timeact = time_regex.findall(x)\n",
    "    newtime = ''.join(timeact[0:5])\n",
    "    f_time = check_ending_char(newtime)\n",
    "    f_time1 = check_starting_char(f_time)\n",
    "    t = parse(f_time1)\n",
    "    new_str_time = str(t.hour)+':'+str(t.minute)+':'+str(t.second)\n",
    "    return new_str_time\n",
    "\n",
    "\n",
    "#fcn for formatting the dates from range str\n",
    "def datesplitter_start(x):\n",
    "    return parse(x.split('to')[0])\n",
    "def datesplitter_end(x):\n",
    "    return parse(x.split('to')[1])\n",
    "\n",
    "#------------------\n",
    "\n",
    "def remove_no_good_data_N2Dry_Feed(df): #removes the string value and fills it with the last good value\n",
    "    \n",
    "    df = pd.to_numeric(df, errors='coerce')\n",
    "    for x in range(0, len(df)):\n",
    "        if type(df[x]) == str:\n",
    "            df[x] = df[x-1]\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    return df\n",
    "\n",
    "#---------------------------\n",
    "def replace_no_good_data_with_LastGoodValues(df_col):\n",
    "    \n",
    "    df_col = pd.to_numeric(df_col, errors='coerce')\n",
    "    for x in range(0, len(df_col)):\n",
    "        if type(df_col[x]== str):\n",
    "            if det_no_data(df_col[x]) == 'Calculation':\n",
    "                df_col[x] = np.nan\n",
    "\n",
    "    df_col.fillna(method='ffill', inplace= True)\n",
    "    return df_col\n",
    "#--------------------------------------------\n",
    "def replace_no_good_data_with_Zero(df_col):\n",
    "\n",
    "    df_col = pd.to_numeric(df_col, errors='coerce')\n",
    "    for x in range(0, len(df_col)):\n",
    "        if type(df_col[x]) == str:\n",
    "            if det_no_data(df_col[x]) == 'Calculation':\n",
    "                df_col[x] = 0\n",
    "    df_col.fillna(value=0, inplace=True)\n",
    "    \n",
    "    return df_col\n",
    "#--------------------------------------------\n",
    "#FILTRATION TIME function\n",
    "#--------------------------------------------\n",
    "def generate_DF_comm(df_coltitle, str_colname):\n",
    "        \n",
    "    A_Filt_MaxList= list()\n",
    "    A_Filt_MaxList[:] = []\n",
    "    A_feedt_Val_List = list()\n",
    "    A_feedt_Val_List[:] = []\n",
    "    A_feedt_index_List = list()\n",
    "    A_feedt_index_List[:] = []\n",
    "    A_Pair_Filt_List = list()\n",
    "    A_Pair_Filt_List[:] = []\n",
    "    try:         \n",
    "        for i in range(0, len(keyslist)):\n",
    "            A_feedt_Val_List = list()\n",
    "            A_feedt_Val_List[:] = []\n",
    "            for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):\n",
    "                A_feedt = df_coltitle[j]\n",
    "                A_feedt_Val_List.append(A_feedt)\n",
    "            A_Filt_MaxList.append(sorted(A_feedt_Val_List)[-1])\n",
    "            A_feedt_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))  \n",
    "        A_Pair_Filt_List = list(zip(A_Filt_MaxList, A_feedt_index_List[0:]))      \n",
    "    except:\n",
    "        pass\n",
    "    df_A_Pair_Filt_t = pd.DataFrame(A_Pair_Filt_List)\n",
    "    df_A_Pair_Filt_t.columns = [str_colname, 'DATETIME']\n",
    "    df_A_Pair_Filt_t.set_index('DATETIME', inplace=True)  \n",
    "    return df_A_Pair_Filt_t\n",
    "#--------------------------------------------\n",
    "#finding peaks FT for A_filtration time\n",
    "#---function start---------\n",
    "def getTheLocalMinMax(df_col1, df_col2):\n",
    "    df_col1 = pd.to_numeric(df_col1, errors='coerce')\n",
    "    b = (np.diff(np.sign(np.diff(df_col1))) > 0).nonzero()[0] + 1         # local min\n",
    "   \n",
    "    df_col2 = pd.to_numeric(df_col2, errors='coerce')\n",
    "    f = (np.diff(np.sign(np.diff(df_col2))) < 0).nonzero()[0] + 1         # local max\n",
    "\n",
    "    try:\n",
    "        ft_cyc_lim = {}\n",
    "        for i in range(0, len(b)):\n",
    "            for j in range(0, len(f)):\n",
    "                if f[j] > b[i] and f[j] < b[i+1]:\n",
    "                    if f[j+1]<b[i+1]:\n",
    "                        cyc_start = b[i]\n",
    "                        cyc_end = f[j+1]\n",
    "                        ft_cyc_lim.setdefault(cyc_start,cyc_end)\n",
    "    except IndexError:\n",
    "        print('\\nDone')\n",
    "    return ft_cyc_lim\n",
    "\n",
    "#end of function----------------\n",
    "#code for getting the CWP for CakeWash1 and CakeWash2\n",
    "\n",
    "#----------------------------------------------------\n",
    "def generate_DF_CW1(df_col, str_colname):\n",
    "    A_CWP_1_indexList = list()\n",
    "    A_CWP_1_indexList[:] = []\n",
    "    A_CWP_1_inner = list()\n",
    "    A_CWP_1_inner[:] = []\n",
    "    A_Pair_CWP_1_List = list()\n",
    "    try:\n",
    "        for i in range(0, len(keyslist)):\n",
    "            A_CWPList_i = list()\n",
    "            A_CWPList_i[:] = []\n",
    "            d = list()\n",
    "            d[:] = []\n",
    "\n",
    "            A_CWPList_i = list(np.arange(keyslist[i],ft_cyc_lim[keyslist[i]]))\n",
    "            d = (np.diff(np.sign(np.diff(df_col.iloc[keyslist[i]:ft_cyc_lim[keyslist[i]]])))< 0).nonzero()[0] + 1\n",
    "            A_CWP_for_sort = list()\n",
    "            A_CWP_for_sort[:] = [] \n",
    "            A_CWP_1_indexList.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "\n",
    "            for i in range(0, len(d)):\n",
    "                CWP_val_shortlist = df_col.iloc[A_CWPList_i[d[i]]]\n",
    "                A_CWP_for_sort.append(CWP_val_shortlist)\n",
    "            A_CWP_1 = sorted(A_CWP_for_sort)[len(A_CWP_for_sort)-1]\n",
    "            A_CWP_1_inner.append(A_CWP_1)               \n",
    "\n",
    "    except IndexError:\n",
    "        print('IndexError')\n",
    "    A_Pair_CWP_1_List = list(zip(A_CWP_1_inner, A_CWP_1_indexList[0:]))\n",
    "    df_A_Pair_CWP_1 = pd.DataFrame(A_Pair_CWP_1_List)\n",
    "    df_A_Pair_CWP_1.columns = [str_colname, 'DATETIME']\n",
    "    df_A_Pair_CWP_1.set_index('DATETIME', inplace=True)\n",
    "    \n",
    "    return df_A_Pair_CWP_1\n",
    "#----------------------------------------------------\n",
    "\n",
    "#code for getting the CWP for CakeWash1 and CakeWash2\n",
    "#----------------------------------------------------\n",
    "def generate_DF_CW2(df_col, str_colname):\n",
    "    A_CWP_1_indexList = list()\n",
    "    A_CWP_1_indexList[:] = []\n",
    "    A_CWP_1_inner = list()\n",
    "    A_CWP_1_inner[:] = []\n",
    "    A_Pair_CWP_1_List = list()\n",
    "    try:\n",
    "        for i in range(0, len(keyslist)):\n",
    "            A_CWPList_i = list()\n",
    "            A_CWPList_i[:] = []\n",
    "            d = list()\n",
    "            d[:] = []\n",
    "\n",
    "            A_CWPList_i = list(np.arange(keyslist[i],ft_cyc_lim[keyslist[i]]))\n",
    "            d = (np.diff(np.sign(np.diff(df_col.iloc[keyslist[i]:ft_cyc_lim[keyslist[i]]])))< 0).nonzero()[0] + 1\n",
    "            A_CWP_for_sort = list()\n",
    "            A_CWP_for_sort[:] = [] \n",
    "            A_CWP_1_indexList.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "\n",
    "            for i in range(0, len(d)):\n",
    "                CWP_val_shortlist = df_col.iloc[A_CWPList_i[d[i]]]\n",
    "                A_CWP_for_sort.append(CWP_val_shortlist)\n",
    "            A_CWP_1 = sorted(A_CWP_for_sort)[len(A_CWP_for_sort)-2]\n",
    "            A_CWP_1_inner.append(A_CWP_1)               \n",
    "\n",
    "    except IndexError:\n",
    "        print('IndexError')\n",
    "    A_Pair_CWP_1_List = list(zip(A_CWP_1_inner, A_CWP_1_indexList[0:]))\n",
    "    df_A_Pair_CWP_1 = pd.DataFrame(A_Pair_CWP_1_List)\n",
    "    df_A_Pair_CWP_1.columns = [str_colname, 'DATETIME']\n",
    "    df_A_Pair_CWP_1.set_index('DATETIME', inplace=True)\n",
    "    \n",
    "    return df_A_Pair_CWP_1\n",
    "#----------------------------------------------------\n",
    "#---function to generate N2DRY DF\n",
    "def generate_DF_for_N2DRY(df_colN2, str_colname):\n",
    "    \n",
    "    A_Pair_N2_Dry_List = list()\n",
    "    A_Pair_N2_Dry_List[:] = []\n",
    "    A_t_N2_Dry_indexList = list()\n",
    "    A_t_N2_Dry_indexList[:] = []\n",
    "    A_t_N2_Dry_val = list()\n",
    "    A_t_N2_Dry_val[:] = []\n",
    "    \n",
    "    try:         \n",
    "        for i in range(0, len(keyslist)):   \n",
    "            A_t_N2_Dry = df_colN2[ft_cyc_lim[keyslist[i]]]\n",
    "            A_t_N2_Dry_val.append(A_t_N2_Dry)\n",
    "            A_t_N2_Dry_indexList.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]]))) \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    A_Pair_N2_Dry_List = list(zip(A_t_N2_Dry_val, A_t_N2_Dry_indexList[0:]))\n",
    "    df_A_Pair_N2DRY = pd.DataFrame(A_Pair_N2_Dry_List)\n",
    "    df_A_Pair_N2DRY.columns = [str_colname, 'DATETIME']\n",
    "    df_A_Pair_N2DRY.set_index('DATETIME', inplace=True)\n",
    "    \n",
    "    return df_A_Pair_N2DRY\n",
    "\n",
    "#------------------------------------------------\n",
    "#Functions for BRAVO FILTER Parameters\n",
    "\n",
    "def generate_dfB_comm(df_coltitle, str_colname):\n",
    "        \n",
    "    A_Filt_MaxList= list()\n",
    "    A_Filt_MaxList[:] = []\n",
    "    A_feedt_Val_List = list()\n",
    "    A_feedt_Val_List[:] = []\n",
    "    A_feedt_index_List = list()\n",
    "    A_feedt_index_List[:] = []\n",
    "    A_Pair_Filt_List = list()\n",
    "    A_Pair_Filt_List[:] = []\n",
    "    try:         \n",
    "        for i in range(0, len(keyslistB)):\n",
    "            A_feedt_Val_List = list()\n",
    "            A_feedt_Val_List[:] = []\n",
    "            for j in range(keyslistB[i], ft_cyc_limB[keyslistB[i]]):\n",
    "                A_feedt = df_coltitle[j]\n",
    "                A_feedt_Val_List.append(A_feedt)\n",
    "            A_Filt_MaxList.append(sorted(A_feedt_Val_List)[-1])\n",
    "            A_feedt_index_List.append((str(df_comb.index[keyslistB[i]])+\" to \"+str(df_comb.index[ft_cyc_limB[keyslistB[i]]])))  \n",
    "        A_Pair_Filt_List = list(zip(A_Filt_MaxList, A_feedt_index_List[0:]))      \n",
    "    except:\n",
    "        pass\n",
    "    df_A_Pair_Filt_t = pd.DataFrame(A_Pair_Filt_List)\n",
    "    df_A_Pair_Filt_t.columns = [str_colname, 'DATETIME']\n",
    "    df_A_Pair_Filt_t.set_index('DATETIME', inplace=True)  \n",
    "    return df_A_Pair_Filt_t\n",
    "#--------------------------------------------\n",
    "\n",
    "\n",
    "#code for getting the CWP for CakeWash1 and CakeWash2\n",
    "#----------------------------------------------------\n",
    "def generate_dfB_CW1(df_col, str_colname):\n",
    "    A_CWP_1_indexList = list()\n",
    "    A_CWP_1_indexList[:] = []\n",
    "    A_CWP_1_inner = list()\n",
    "    A_CWP_1_inner[:] = []\n",
    "    A_Pair_CWP_1_List = list()\n",
    "    try:\n",
    "        for i in range(0, len(keyslistB)):\n",
    "            A_CWPList_i = list()\n",
    "            A_CWPList_i[:] = []\n",
    "            d = list()\n",
    "            d[:] = []\n",
    "\n",
    "            A_CWPList_i = list(np.arange(keyslistB[i],ft_cyc_limB[keyslistB[i]]))\n",
    "            d = (np.diff(np.sign(np.diff(df_col.iloc[keyslistB[i]:ft_cyc_limB[keyslistB[i]]])))< 0).nonzero()[0] + 1\n",
    "            A_CWP_for_sort = list()\n",
    "            A_CWP_for_sort[:] = [] \n",
    "            A_CWP_1_indexList.append((str(df_comb.index[keyslistB[i]])+\" to \"+str(df_comb.index[ft_cyc_limB[keyslistB[i]]])))\n",
    "\n",
    "            for i in range(0, len(d)):\n",
    "                CWP_val_shortlist = df_col.iloc[A_CWPList_i[d[i]]]\n",
    "                A_CWP_for_sort.append(CWP_val_shortlist)\n",
    "            A_CWP_1 = sorted(A_CWP_for_sort)[len(A_CWP_for_sort)-1]\n",
    "            A_CWP_1_inner.append(A_CWP_1)               \n",
    "\n",
    "    except IndexError:\n",
    "        print('IndexError')\n",
    "    A_Pair_CWP_1_List = list(zip(A_CWP_1_inner, A_CWP_1_indexList[0:]))\n",
    "    df_A_Pair_CWP_1 = pd.DataFrame(A_Pair_CWP_1_List)\n",
    "    df_A_Pair_CWP_1.columns = [str_colname, 'DATETIME']\n",
    "    df_A_Pair_CWP_1.set_index('DATETIME', inplace=True)\n",
    "    \n",
    "    return df_A_Pair_CWP_1\n",
    "#----------------------------------------------------\n",
    "\n",
    "#code for getting the CWP for CakeWash1 and CakeWash2\n",
    "#----------------------------------------------------\n",
    "def generate_dfB_CW2(df_col, str_colname):\n",
    "    A_CWP_1_indexList = list()\n",
    "    A_CWP_1_indexList[:] = []\n",
    "    A_CWP_1_inner = list()\n",
    "    A_CWP_1_inner[:] = []\n",
    "    A_Pair_CWP_1_List = list()\n",
    "    try:\n",
    "        for i in range(0, len(keyslistB)):\n",
    "            A_CWPList_i = list()\n",
    "            A_CWPList_i[:] = []\n",
    "            d = list()\n",
    "            d[:] = []\n",
    "\n",
    "            A_CWPList_i = list(np.arange(keyslistB[i],ft_cyc_limB[keyslistB[i]]))\n",
    "            d = (np.diff(np.sign(np.diff(df_col.iloc[keyslistB[i]:ft_cyc_limB[keyslistB[i]]])))< 0).nonzero()[0] + 1\n",
    "            A_CWP_for_sort = list()\n",
    "            A_CWP_for_sort[:] = [] \n",
    "            A_CWP_1_indexList.append((str(df_comb.index[keyslistB[i]])+\" to \"+str(df_comb.index[ft_cyc_limB[keyslistB[i]]])))\n",
    "\n",
    "            for i in range(0, len(d)):\n",
    "                CWP_val_shortlist = df_col.iloc[A_CWPList_i[d[i]]]\n",
    "                A_CWP_for_sort.append(CWP_val_shortlist)\n",
    "            A_CWP_1 = sorted(A_CWP_for_sort)[len(A_CWP_for_sort)-2]\n",
    "            A_CWP_1_inner.append(A_CWP_1)               \n",
    "\n",
    "    except IndexError:\n",
    "        print('IndexError')\n",
    "    A_Pair_CWP_1_List = list(zip(A_CWP_1_inner, A_CWP_1_indexList[0:]))\n",
    "    df_A_Pair_CWP_1 = pd.DataFrame(A_Pair_CWP_1_List)\n",
    "    df_A_Pair_CWP_1.columns = [str_colname, 'DATETIME']\n",
    "    df_A_Pair_CWP_1.set_index('DATETIME', inplace=True)\n",
    "    \n",
    "    return df_A_Pair_CWP_1\n",
    "#----------------------------------------------------\n",
    "#---function to generate N2DRY DF\n",
    "def generate_dfB_for_N2DRY(df_colN2, str_colname):\n",
    "    \n",
    "    A_Pair_N2_Dry_List = list()\n",
    "    A_Pair_N2_Dry_List[:] = []\n",
    "    A_t_N2_Dry_indexList = list()\n",
    "    A_t_N2_Dry_indexList[:] = []\n",
    "    A_t_N2_Dry_val = list()\n",
    "    A_t_N2_Dry_val[:] = []\n",
    "    \n",
    "    try:         \n",
    "        for i in range(0, len(keyslistB)):   \n",
    "            A_t_N2_Dry = df_colN2[ft_cyc_limB[keyslistB[i]]]\n",
    "            A_t_N2_Dry_val.append(A_t_N2_Dry)\n",
    "            A_t_N2_Dry_indexList.append((str(df_comb.index[keyslistB[i]])+\" to \"+str(df_comb.index[ft_cyc_limB[keyslistB[i]]]))) \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    A_Pair_N2_Dry_List = list(zip(A_t_N2_Dry_val, A_t_N2_Dry_indexList[0:]))\n",
    "    df_A_Pair_N2DRY = pd.DataFrame(A_Pair_N2_Dry_List)\n",
    "    df_A_Pair_N2DRY.columns = [str_colname, 'DATETIME']\n",
    "    df_A_Pair_N2DRY.set_index('DATETIME', inplace=True)\n",
    "    \n",
    "    return df_A_Pair_N2DRY\n",
    "\n",
    "#------------------------------------------------\n",
    "\n",
    "print('all functions declared', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb['A_t_FEED'] = replace_no_good_data_with_LastGoodValues(df_comb['A_t_FEED'])\n",
    "df_comb['A_t_DRY'] = replace_no_good_data_with_LastGoodValues(df_comb['A_t_DRY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the Data Using the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   4729\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4730\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4731\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._maybe_get_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 12517",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-fdb523334621>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mdf_comb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'106PU27A_CUR'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreplace_no_good_data_with_Zero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_comb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'106PU27A_CUR'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mdf_comb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PU26A_OP'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreplace_no_good_data_with_Zero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_comb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PU26A_OP'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mdf_comb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PU27A_OP'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreplace_no_good_data_with_Zero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_comb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PU27A_OP'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-146-514dfe7498cd>\u001b[0m in \u001b[0;36mreplace_no_good_data_with_Zero\u001b[1;34m(df_col)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0mdf_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'coerce'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_col\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdet_no_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_col\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Calculation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[0mdf_col\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4729\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4730\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4731\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4732\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Remove the no good data for t_FEED and N2_DRY ------------------\n",
    "\n",
    "df_comb['A_t_FEED'] = replace_no_good_data_with_LastGoodValues(df_comb['A_t_FEED'])\n",
    "df_comb['A_t_DRY'] = replace_no_good_data_with_LastGoodValues(df_comb['A_t_DRY'])\n",
    "\n",
    "#Clean data from no good values\n",
    "df_comb['A_t_CWSH1'] = replace_no_good_data_with_LastGoodValues(df_comb['A_t_CWSH1'])\n",
    "df_comb['A_t_CWSH2'] = replace_no_good_data_with_LastGoodValues(df_comb['A_t_CWSH2'])\n",
    "df_comb['A_t_PRESS1'] = replace_no_good_data_with_LastGoodValues(df_comb['A_t_PRESS1'])\n",
    "df_comb['A_t_PRESS2'] = replace_no_good_data_with_LastGoodValues(df_comb['A_t_PRESS2'])\n",
    "\n",
    "#Clean data from no good values\n",
    "df_comb['106FT02A_MANFP'] = replace_no_good_data_with_Zero(df_comb['106FT02A_MANFP'])\n",
    "df_comb['106FT02A_CWP'] = replace_no_good_data_with_Zero(df_comb['106FT02A_CWP'])\n",
    "df_comb['106PU26A_CUR'] = replace_no_good_data_with_Zero(df_comb['106PU26A_CUR'])\n",
    "df_comb['106PU27A_CUR'] = replace_no_good_data_with_Zero(df_comb['106PU27A_CUR'])\n",
    "df_comb['PU26A_OP'] = replace_no_good_data_with_Zero(df_comb['PU26A_OP'])\n",
    "df_comb['PU27A_OP'] = replace_no_good_data_with_Zero(df_comb['PU27A_OP'])\n",
    "\n",
    "\n",
    "\n",
    "#Remove the no good data for t_FEED and N2_DRY for B dataframe------------------\n",
    "\n",
    "df_comb['B_t_FEED'] = replace_no_good_data_with_LastGoodValues(df_comb['B_t_FEED'])\n",
    "df_comb['B_t_DRY'] = replace_no_good_data_with_LastGoodValues(df_comb['B_t_DRY'])\n",
    "\n",
    "#Clean data from no good values\n",
    "df_comb['B_t_CWSH1'] = replace_no_good_data_with_LastGoodValues(df_comb['B_t_CWSH1'])\n",
    "df_comb['B_t_CWSH2'] = replace_no_good_data_with_LastGoodValues(df_comb['B_t_CWSH2'])\n",
    "df_comb['B_t_PRESS1'] = replace_no_good_data_with_LastGoodValues(df_comb['B_t_PRESS1'])\n",
    "df_comb['B_t_PRESS2'] = replace_no_good_data_with_LastGoodValues(df_comb['B_t_PRESS2'])\n",
    "\n",
    "#Clean data from no good values\n",
    "df_comb['FT02B_MANFP'] = replace_no_good_data_with_Zero(df_comb['FT02B_MANFP'])\n",
    "df_comb['FT02B_CWP'] = replace_no_good_data_with_Zero(df_comb['FT02B_CWP'])\n",
    "df_comb['106PU26B_CUR'] = replace_no_good_data_with_Zero(df_comb['106PU26B_CUR'])\n",
    "df_comb['106PU27B_CUR'] = replace_no_good_data_with_Zero(df_comb['106PU27B_CUR'])\n",
    "df_comb['PU26B_OP'] = replace_no_good_data_with_Zero(df_comb['PU26B_OP'])\n",
    "df_comb['PU27B_OP'] = replace_no_good_data_with_Zero(df_comb['PU27B_OP'])\n",
    "\n",
    "print('Done. All param_df_generated', datetime.datetime.now())\n",
    "#------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECT THE ALPHA PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the local min, max. Then generate the keys\n",
    "ft_cyc_lim = getTheLocalMinMax(df_comb['A_t_FEED'], df_comb['A_t_DRY'])\n",
    "keyslist = list(ft_cyc_lim.keys())\n",
    "#----\n",
    "print('Done. Filtration Cycle Time generated.', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the dataframes\n",
    "df_A_CWP_1 = generate_DF_CW1(df_comb['106FT02A_CWP'], 'A_CWP_1')\n",
    "df_A_CWP_2 = generate_DF_CW2(df_comb['106FT02A_CWP'], 'A_CWP_2')\n",
    "df_A_N2DRY = generate_DF_for_N2DRY(df_comb['A_t_DRY'], 'A_N2DRY')\n",
    "df_A_MANFP = generate_DF_comm(df_comb['106FT02A_MANFP'], 'A_MANF_P')\n",
    "df_A_t_FEED = generate_DF_comm(df_comb['A_t_FEED'], 'A_FILT_T')\n",
    "df_A_t_CWSH1 = generate_DF_comm(df_comb['A_t_CWSH1'], 'A_CWSH1')\n",
    "df_A_t_CWSH2 = generate_DF_comm(df_comb['A_t_CWSH2'], 'A_CWSH2')\n",
    "df_A_t_PRESS1 = generate_DF_comm(df_comb['A_t_PRESS1'], 'A_t_PRESS1')\n",
    "df_A_t_PRESS2 = generate_DF_comm(df_comb['A_t_PRESS2'], 'A_t_PRESS2')\n",
    "df_PU26A_OP = generate_DF_comm(df_comb['PU26A_OP'], 'A_PU26A_OP')\n",
    "df_PU27A_OP = generate_DF_comm(df_comb['PU27A_OP'], 'A_PU27A_OP')\n",
    "df_106PU26A_CUR = generate_DF_comm(df_comb['106PU26A_CUR'], 'A_PU26A_CUR')\n",
    "df_106PU27A_CUR = generate_DF_comm(df_comb['106PU27A_CUR'], 'A_PU27A_CUR')\n",
    "#-----\n",
    "print('Done. Generated dataframes of each parameter for pressure filter alpha.', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn = df_A_CWP_1.join(df_A_CWP_2, how='outer')\n",
    "dfn1 = df_A_N2DRY.join(df_A_MANFP, how='outer')\n",
    "dfn2 = df_A_t_FEED.join(df_A_t_CWSH1, how='outer')\n",
    "dfn3 = df_A_t_CWSH2.join(df_A_t_PRESS1, how='outer')\n",
    "dfn4 = df_A_t_PRESS2.join(df_PU26A_OP, how='outer')\n",
    "dfn5 = df_PU27A_OP.join(df_106PU26A_CUR, how='outer')\n",
    "dfn6 = df_106PU27A_CUR.join(dfn, how='outer')\n",
    "dfn7 = dfn1.join(dfn2, how='outer')\n",
    "dfn8 = dfn3.join(dfn4, how='outer')\n",
    "dfn9 = dfn5.join(dfn6, how='outer')\n",
    "dfn10 = dfn7.join(dfn8, how='outer')\n",
    "dfn11 = dfn9.join(dfn10, how='outer')\n",
    "\n",
    "df_PressureFilter_Param_A = dfn11\n",
    "\n",
    "#-----\n",
    "print('Done. Generated the merged dataframes of PiData parameters for pressure filter alpha.', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECT THE BRAVO PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the Data Using the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the local min, max. Then generate the keys\n",
    "ft_cyc_limB = getTheLocalMinMax(df_comb['B_t_FEED'], df_comb['B_t_DRY'])\n",
    "keyslistB = list(ft_cyc_limB.keys())\n",
    "#------\n",
    "print('Done. Filtration Cycle Time generated.', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the dataframes for B\n",
    "df_B_CWP_1 = generate_dfB_CW1(df_comb['FT02B_CWP'], 'B_CWP_1')\n",
    "df_B_CWP_2 = generate_dfB_CW2(df_comb['FT02B_CWP'], 'B_CWP_2')\n",
    "df_B_N2DRY = generate_dfB_for_N2DRY(df_comb['B_t_DRY'], 'B_N2DRY')\n",
    "df_B_MANFP = generate_dfB_comm(df_comb['FT02B_MANFP'], 'B_MANF_P')\n",
    "df_B_t_FEED = generate_dfB_comm(df_comb['B_t_FEED'], 'B_FILT_T')\n",
    "df_B_t_CWSH1 = generate_dfB_comm(df_comb['B_t_CWSH1'], 'B_CWSH1')\n",
    "df_B_t_CWSH2 = generate_dfB_comm(df_comb['B_t_CWSH2'], 'B_CWSH2')\n",
    "df_B_t_PRESS1 = generate_dfB_comm(df_comb['B_t_PRESS1'], 'B_t_PRESS1')\n",
    "df_B_t_PRESS2 = generate_dfB_comm(df_comb['B_t_PRESS2'], 'B_t_PRESS2')\n",
    "df_PU26B_OP = generate_dfB_comm(df_comb['PU26B_OP'], 'B_PU26B_OP')\n",
    "df_PU27B_OP = generate_dfB_comm(df_comb['PU27B_OP'], 'B_PU27B_OP')\n",
    "df_106PU26B_CUR = generate_dfB_comm(df_comb['106PU26B_CUR'], 'B_PU26B_CUR')\n",
    "df_106PU27B_CUR = generate_dfB_comm(df_comb['106PU27B_CUR'], 'B_PU27B_CUR')\n",
    "\n",
    "#-----\n",
    "print('Done. Generated dataframes of each parameter for pressure filter alpha.', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining the dataframes for bravo parameters\n",
    "dfnB = df_B_CWP_1.join(df_B_CWP_2, how='outer')\n",
    "dfn1B = df_B_N2DRY.join(df_B_MANFP, how='outer')\n",
    "dfn2B = df_B_t_FEED.join(df_B_t_CWSH1, how='outer')\n",
    "dfn3B = df_B_t_CWSH2.join(df_B_t_PRESS1, how='outer')\n",
    "dfn4B = df_B_t_PRESS2.join(df_PU26B_OP, how='outer')\n",
    "dfn5B = df_PU27B_OP.join(df_106PU26B_CUR, how='outer')\n",
    "dfn6B = df_106PU27B_CUR.join(dfnB, how='outer')\n",
    "dfn7B = dfn1B.join(dfn2B, how='outer')\n",
    "dfn8B = dfn3B.join(dfn4B, how='outer')\n",
    "dfn9B = dfn5B.join(dfn6B, how='outer')\n",
    "dfn10B = dfn7B.join(dfn8B, how='outer')\n",
    "dfn11B = dfn9B.join(dfn10B, how='outer')\n",
    "df_PressureFilter_Param_B = dfn11B\n",
    "\n",
    "#-----\n",
    "print('Done. Generated dataframes of each parameter for pressure filter alpha.', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABORATORY ANALYSIS RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------\n",
    "#collectst the index for the date.day the sample was taken and puts it in the list Smpl_day_index\n",
    "def get_analysis_samplingDate_indexlst(sheetname, columnletter):\n",
    "\n",
    "    Smpl_day_index = list()\n",
    "    Smpl_day_index[:] = []\n",
    "    for i in range (1, sheetname.max_row):\n",
    "        if type(sheetname[columnletter+ str(i)].value) == int:\n",
    "            Smpl_day_index.append(i)\n",
    "\n",
    "    #smpl_end_index is the last row index for the date.day the sample was taken\n",
    "    for i in range(1, sheetname.max_row):\n",
    "        if str(sheetname[columnletter+str(i)].value).split(' ')[0] == 'Daily':\n",
    "            smpl_end = i\n",
    "            break\n",
    "    return (Smpl_day_index, smpl_end)\n",
    "\n",
    "#--------------------------------------------------\n",
    "print('Done.', datetime.datetime.now())\n",
    "\n",
    "Smpl_day_index = get_analysis_samplingDate_indexlst(FT02wb, \"B\")[0]\n",
    "smpl_end_index = get_analysis_samplingDate_indexlst(FT02wb, \"B\")[1]\n",
    "#-------------------\n",
    "print('Done.', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------\n",
    "def generate_element_df(sheetname, elmnt_col_lttr, df_elementNameA, df_elementNameB):\n",
    "\n",
    "    #Code for creating a dataframe for MS_A analysis result for Element Pb\n",
    "    A_Pb_datelist = list()\n",
    "    A_Pb_datelist[:] = []\n",
    "    B_Pb_datelist = list()\n",
    "    B_Pb_datelist[:] = []\n",
    "\n",
    "    A_Pb_list = list()\n",
    "    A_Pb_list[:] = []\n",
    "    B_Pb_list = list()\n",
    "    B_Pb_list[:] = []\n",
    "\n",
    "    A_Pb_list_pair = list()\n",
    "    A_Pb_list_pair[:] = []\n",
    "    B_Pb_list_pair = list()\n",
    "    B_Pb_list_pair[:] = []\n",
    "\n",
    "    for y in range(0, len(Smpl_day_index)):   \n",
    "\n",
    "        if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "            for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "                if sheetname['D'+str(x)].value != None:    \n",
    "                    if filter_detrmin(str(sheetname['C'+str(x)].value)) == 'A':  \n",
    "\n",
    "                        if type(sheetname['D'+str(x)].value) == datetime.datetime:\n",
    "                            dt = sheetname['D'+str(x)].value\n",
    "                            nt = str(dt.hour)+':'+str(dt.minute)+':'+str(dt.second)\n",
    "                            A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(sheetname['B'+str(Smpl_day_index[y])].value))\\\n",
    "                            +' '+time_cleaner(nt)\n",
    "                        else:    \n",
    "                            A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(sheetname['B'+str(Smpl_day_index[y])].value))\\\n",
    "                            +' '+time_cleaner(str(sheetname['D'+str(x)].value))\n",
    "                        \n",
    "                        A_Pb_datelist.append(parse(A_date))  \n",
    "                        A_Pb = sheetname[elmnt_col_lttr+str(x)].value\n",
    "                        A_Pb_list.append(A_Pb)\n",
    "                    if filter_detrmin(str(sheetname['C'+str(x)].value)) == 'B':\n",
    "                        #print(sheetname['B'+str(Smpl_day_index[y])].value, 'B',(sheetname['D'+str(x)].value))\n",
    "                        \n",
    "                        if type(sheetname['D'+str(x)].value) == datetime.datetime:\n",
    "                            dt = sheetname['D'+str(x)].value\n",
    "                            nt = str(dt.hour)+':'+str(dt.minute)+':'+str(dt.second)\n",
    "                            B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(sheetname['B'+str(Smpl_day_index[y])].value))\\\n",
    "                            +' '+time_cleaner(nt)\n",
    "                        else:    \n",
    "                            B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(sheetname['B'+str(Smpl_day_index[y])].value))\\\n",
    "                            +' '+time_cleaner(str(sheetname['D'+str(x)].value))                   \n",
    "                    \n",
    "                        B_Pb_datelist.append(parse(B_date)) \n",
    "                        B_Pb = sheetname[elmnt_col_lttr+str(x)].value\n",
    "                        B_Pb_list.append(B_Pb)\n",
    "\n",
    "        if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "            for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "                if sheetname['D'+str(x)].value != None:    \n",
    "                    if filter_detrmin(str(sheetname['C'+str(x)].value)) == 'A':\n",
    "                        \n",
    "                        if type(sheetname['D'+str(x)].value) == datetime.datetime:\n",
    "                            dt = sheetname['D'+str(x)].value\n",
    "                            nt = str(dt.hour)+':'+str(dt.minute)+':'+str(dt.second)\n",
    "                            A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(sheetname['B'+str(Smpl_day_index[y])].value))\\\n",
    "                            +' '+time_cleaner(nt)\n",
    "                        else:    \n",
    "                            A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(sheetname['B'+str(Smpl_day_index[y])].value))\\\n",
    "                            +' '+time_cleaner(str(sheetname['D'+str(x)].value))                       \n",
    "              \n",
    "                        A_Pb_datelist.append(parse(A_date))\n",
    "                        A_Pb = sheetname[elmnt_col_lttr+str(x)].value\n",
    "                        A_Pb_list.append(A_Pb)\n",
    "\n",
    "                    if filter_detrmin(str(sheetname['C'+str(x)].value)) == 'B':\n",
    "                        \n",
    "                        if type(sheetname['D'+str(x)].value) == datetime.datetime:\n",
    "                            dt = sheetname['D'+str(x)].value\n",
    "                            nt = str(dt.hour)+':'+str(dt.minute)+':'+str(dt.second)\n",
    "                            B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(sheetname['B'+str(Smpl_day_index[y])].value))\\\n",
    "                            +' '+time_cleaner(nt)\n",
    "                        else:    \n",
    "                            B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(sheetname['B'+str(Smpl_day_index[y])].value))\\\n",
    "                            +' '+time_cleaner(str(sheetname['D'+str(x)].value))\n",
    "                        \n",
    "                        B_Pb_datelist.append(parse(B_date))\n",
    "                        B_Pb = sheetname[elmnt_col_lttr+str(x)].value\n",
    "                        B_Pb_list.append(B_Pb)\n",
    "                        \n",
    "    #this part creates the DataFrame after creating a list pair\n",
    "\n",
    "    A_Pb_list_pair = list(zip(A_Pb_list, A_Pb_datelist[0:]))\n",
    "    A_Pb_df = pd.DataFrame(A_Pb_list_pair)\n",
    "    A_Pb_df.columns = [df_elementNameA, 'DATETIME'] ##check if list title is updated\n",
    "    A_Pb_df.set_index('DATETIME', inplace=True)\n",
    "    A_Element_df = A_Pb_df.loc[~A_Pb_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "    B_Pb_list_pair = list(zip(B_Pb_list, B_Pb_datelist[0:]))\n",
    "    B_Pb_df = pd.DataFrame(B_Pb_list_pair)\n",
    "    B_Pb_df.columns = [df_elementNameB, 'DATETIME'] ##check if list title is updated\n",
    "\n",
    "    B_Pb_df.set_index('DATETIME', inplace=True)\n",
    "    B_Element_df = B_Pb_df.loc[~B_Pb_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "    return (A_Element_df, B_Element_df)\n",
    "#----------------------------------------------------------\n",
    "print('Done.', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Generate dataframes using the generate_element_df function. [0] index for alpha, [1] index for bravo\n",
    "df_Pb = generate_element_df(FT02wb, 'T', 'MS_A_Pb', 'MS_B_Pb')\n",
    "df_Zn = generate_element_df(FT02wb, 'U', 'MS_A_Zn', 'MS_B_Zn')\n",
    "df_Cu = generate_element_df(FT02wb, 'V', 'MS_A_Cu', 'MS_B_Cu')\n",
    "df_Ni = generate_element_df(FT02wb, 'W', 'MS_A_Ni', 'MS_B_Ni')\n",
    "df_Co = generate_element_df(FT02wb, 'X', 'MS_A_Co', 'MS_B_Co')\n",
    "df_Fe = generate_element_df(FT02wb, 'Y', 'MS_A_Fe', 'MS_B_Fe')\n",
    "df_Mn = generate_element_df(FT02wb, 'Z', 'MS_A_Mn', 'MS_B_Mn')\n",
    "df_Cr = generate_element_df(FT02wb, 'AA', 'MS_A_Cr', 'MS_B_Cr')\n",
    "df_Ca = generate_element_df(FT02wb, 'AB', 'MS_A_Ca', 'MS_B_Ca')\n",
    "df_Si = generate_element_df(FT02wb, 'AC', 'MS_A_Si', 'MS_B_Si')\n",
    "df_Al = generate_element_df(FT02wb, 'AD', 'MS_A_Al', 'MS_B_Al')\n",
    "df_Mg = generate_element_df(FT02wb, 'AE', 'MS_A_Mg', 'MS_B_Mg')\n",
    "df_S = generate_element_df(FT02wb, 'AF', 'MS_A_S', 'MS_B_S')\n",
    "\n",
    "df_moisture = generate_element_df(FT02wb, 'AP', 'MS_A_moisture', 'MS_B_moisture')\n",
    "df_PS_10D_spcl = generate_element_df(FT02wb, 'AQ', 'MS_A_PS_10D_spcl', 'MS_B_PS_10D_spcl')\n",
    "df_PS_50D_spcl = generate_element_df(FT02wb, 'AR', 'MS_A_PS_50D_spcl', 'MS_B_PS_50D_spcl')\n",
    "df_PS_90D_spcl = generate_element_df(FT02wb, 'AS', 'MS_A_PS_90D_spcl', 'MS_B_PS_90D_spcl')\n",
    "df_PS_mean_spcl = generate_element_df(FT02wb, 'AT', 'MS_A_PS_mean_v_spcl', 'MS_A_PS_mean_v_spcl')\n",
    "df_PS_stdev_spcl = generate_element_df(FT02wb, 'AU', 'MS_B_PS_stdev_spcl', 'MS_B_PS_stdev_spcl')\n",
    "\n",
    "#-----------------------\n",
    "print('Done.', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pressure Filter A lab results in merged dataframe\n",
    "df_A_PbZn = df_Pb[0].join(df_Zn[0], how='outer')\n",
    "df_A_CuNi = df_Cu[0].join(df_Ni[0], how='outer')\n",
    "df_A_CoFe = df_Co[0].join(df_Fe[0], how='outer')\n",
    "df_A_MnCr = df_Mn[0].join(df_Cr[0], how='outer')\n",
    "df_A_CaSi = df_Ca[0].join(df_Si[0], how='outer')\n",
    "df_A_AlMg = df_Al[0].join(df_Mg[0], how='outer')\n",
    "df_A_Sh2o = df_S[0].join(df_moisture[0], how='outer')\n",
    "df_A_PS_10D50D = df_PS_10D_spcl[0].join(df_PS_50D_spcl[0], how='outer')\n",
    "df_A_PS_90Dmean = df_PS_90D_spcl[0].join(df_PS_mean_spcl[0], how='outer')\n",
    "\n",
    "df_A_PbZnCuNi = df_A_PbZn.join(df_A_CuNi, how='outer')\n",
    "df_A_CoFeMnCr = df_A_CoFe.join(df_A_MnCr, how='outer')\n",
    "df_A_CaSiAlMg = df_A_CaSi.join(df_A_AlMg, how='outer')\n",
    "df_A_Sh2oPS10D50D = df_A_Sh2o.join(df_A_PS_10D50D, how='outer')\n",
    "df_A_PS_90Dmeanstdev = df_A_PS_90Dmean.join(df_PS_stdev_spcl[0], how='outer')\n",
    "\n",
    "df_A_PbZnCuNiCoFeMnCr = df_A_PbZnCuNi.join(df_A_CoFeMnCr, how='outer')\n",
    "df_A_CaSiAlMgSh2oPS10D50D = df_A_CaSiAlMg.join(df_A_Sh2oPS10D50D, how='outer')\n",
    "df_A_CaSiAlMgSh2oPS10D50D90Dmeanstdev = df_A_CaSiAlMgSh2oPS10D50D.join(df_A_PS_90Dmeanstdev, how='outer')\n",
    "df_A_PbZnCuNiCoFeMnCrCaSiAlMgSh2oPS10D50D90Dmeanstdev = df_A_PbZnCuNiCoFeMnCr.join(df_A_CaSiAlMgSh2oPS10D50D90Dmeanstdev, how='outer')\n",
    "df_A_merged_lab_res = df_A_PbZnCuNiCoFeMnCrCaSiAlMgSh2oPS10D50D90Dmeanstdev\n",
    "\n",
    "#-----------------------\n",
    "print('Done.', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_B_PbZn = df_Pb[1].join(df_Zn[1], how='outer')\n",
    "df_B_CuNi = df_Cu[1].join(df_Ni[1], how='outer')\n",
    "df_B_CoFe = df_Co[1].join(df_Fe[1], how='outer')\n",
    "df_B_MnCr = df_Mn[1].join(df_Cr[1], how='outer')\n",
    "df_B_CaSi = df_Ca[1].join(df_Si[1], how='outer')\n",
    "df_B_AlMg = df_Al[1].join(df_Mg[1], how='outer')\n",
    "df_B_Sh2o = df_S[1].join(df_moisture[1], how='outer')\n",
    "df_B_PS_10D50D = df_PS_10D_spcl[1].join(df_PS_50D_spcl[1], how='outer')\n",
    "df_B_PS_90Dmean = df_PS_90D_spcl[1].join(df_PS_mean_spcl[1], how='outer')\n",
    "\n",
    "df_B_PS_90Dmeanstdev = df_B_PS_90Dmean.join(df_PS_stdev_spcl[1], how='outer')\n",
    "df_B_PbZnCuNi = df_B_PbZn.join(df_B_CuNi, how='outer')\n",
    "df_B_CoFeMnCr = df_B_CoFe.join(df_B_MnCr, how='outer')\n",
    "df_B_CaSiAlMg = df_B_CaSi.join(df_B_AlMg, how='outer')\n",
    "df_B_Sh2o9D50D = df_B_Sh2o.join(df_B_PS_10D50D, how='outer')\n",
    "\n",
    "df_B_PbZnCuNiCoFeMnCr = df_B_PbZnCuNi.join(df_B_CoFeMnCr, how='outer')\n",
    "df_B_CaSiAlMgSh2o9D50D = df_B_CaSiAlMg.join(df_B_Sh2o9D50D, how='outer')\n",
    "\n",
    "df_B_PbZnCuNiCoFeMnCrCaSiAlMgSh2o9D50D = df_B_PbZnCuNiCoFeMnCr.join(df_B_CaSiAlMgSh2o9D50D, how='outer')\n",
    "df_B_PbZnCuNiCoFeMnCrCaSiAlMgSh2o9D50D90Dmeanstdev = df_B_PbZnCuNiCoFeMnCrCaSiAlMgSh2o9D50D.join(df_B_PS_90Dmeanstdev, how='outer')\n",
    "\n",
    "df_B_merged_lab_res = df_B_PbZnCuNiCoFeMnCrCaSiAlMgSh2o9D50D90Dmeanstdev\n",
    "\n",
    "#deletes the rows whose index has duplicates but keeps the last value\n",
    "df_A_merged_lab_res = df_A_merged_lab_res.loc[~df_A_merged_lab_res.index.duplicated(keep='last')]\n",
    "df_B_merged_lab_res = df_B_merged_lab_res.loc[~df_B_merged_lab_res.index.duplicated(keep='last')]\n",
    "\n",
    "#--------------------\n",
    "print('Done', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------Create new columns that separates the daterange written in string format then parses it to actual datetime\n",
    "df_PressureFilter_Param_A['newdt'] = df_PressureFilter_Param_A.index\n",
    "df_PressureFilter_Param_A['startdate'] = df_PressureFilter_Param_A['newdt'].apply(lambda x: datesplitter_start(x))\n",
    "df_PressureFilter_Param_A['enddate'] = df_PressureFilter_Param_A['newdt'].apply(lambda x: datesplitter_end(x))\n",
    "\n",
    "df_PressureFilter_Param_B['newdt'] = df_PressureFilter_Param_B.index\n",
    "df_PressureFilter_Param_B['startdate'] = df_PressureFilter_Param_B['newdt'].apply(lambda x: datesplitter_start(x))\n",
    "df_PressureFilter_Param_B['enddate'] = df_PressureFilter_Param_B['newdt'].apply(lambda x: datesplitter_end(x))\n",
    "\n",
    "#---------\n",
    "print('Done.', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_df(lst_index, lst_value, colname_output):\n",
    "    lst_index_batch_comp_wt = list(zip(lst_index,lst_value))\n",
    "    df_batch_comp_wt_index_time = pd.DataFrame(lst_index_batch_comp_wt)\n",
    "\n",
    "    df_batch_comp_wt_index_time.columns = ['DATETIME', colname_output]\n",
    "    df_bc_wt = df_batch_comp_wt_index_time\n",
    "    df_bc_wt.set_index('DATETIME', inplace= True)\n",
    "    \n",
    "    return df_bc_wt\n",
    "#---------------------\n",
    "def generate_cycle_time(df_of_parameter, main_df_containing_datetimerange):\n",
    "    w_n11_indexList = []\n",
    "    w_n11_bcwt_indexList = []\n",
    "\n",
    "    for i in range(0, len(df_of_parameter)):\n",
    "        for j in range(0, len(main_df_containing_datetimerange)):\n",
    "            if j < len(main_df_containing_datetimerange):\n",
    "                if main_df_containing_datetimerange['enddate'][j-1] <= df_of_parameter.index[i] and main_df_containing_datetimerange['enddate'][j] >= df_of_parameter.index[i]:\n",
    "                    w_n11_indexList.append(main_df_containing_datetimerange.index[j-1])\n",
    "                    w_n11_bcwt_indexList.append(df_of_parameter.index[i]) \n",
    "\n",
    "    df_parameter_cycTime = generate_df(w_n11_bcwt_indexList, w_n11_indexList[0:], 'FT_cycletimerange')\n",
    "    return df_parameter_cycTime\n",
    "\n",
    "#--------------------\n",
    "print('Done.', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMBINE PiDATA WITH LABORATORY RESULTS FOR ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for changing the index to Datetime in range format (from date1 to date2)\n",
    "\n",
    "A_column_names_laboratory_results = ['MS_A_Pb_spcl', 'MS_A_Zn_spcl', 'MS_A_Cu_spcl', 'MS_A_Ni_spcl', 'MS_A_Co_spcl', 'MS_A_Fe_spcl',\n",
    "       'MS_A_Mn_spcl', 'MS_A_Cr_spcl', 'MS_A_Ca_spcl', 'MS_A_Si_spcl', 'MS_A_Al_spcl', 'MS_A_Mg_spcl','MS_A_S_spcl', 'MS_A_moisture_spcl', 'MS_A_PS_10D_spcl', 'MS_A_PS_50D_spcl', 'MS_A_PS_90D_spcl',\n",
    "       'MS_A_PS_mean_v_spcl', 'MS_A_PS_stdev_spcl','DATETIME']\n",
    "\n",
    "A_column_names_merged_final = ['Cycle_Count','A_PU26A_CUR', 'A_PU27A_CUR', 'A_N2DRY', 'A_CWP_1', 'A_CWP_2',\n",
    "       'A_FILT_T', 'A_MANF_P', 'A_CWSH1', 'A_CWSH2', 'A_t_PRESS1',\n",
    "       'A_t_PRESS2', 'A_PU26A_OP', 'A_PU27A_OP', 'newdt', 'startdate',\n",
    "       'enddate', 'MS_A_Pb_spcl', 'MS_A_Zn_spcl', 'MS_A_Cu_spcl',\n",
    "       'MS_A_Ni_spcl', 'MS_A_Co_spcl', 'MS_A_Fe_spcl', 'MS_A_Mn_spcl',\n",
    "       'MS_A_Cr_spcl', 'MS_A_Ca_spcl', 'MS_A_Si_spcl', 'MS_A_Al_spcl',\n",
    "       'MS_A_Mg_spcl', 'MS_A_S_spcl', 'MS_A_moisture_spcl', 'MS_A_PS_10D_spcl',\n",
    "       'MS_A_PS_50D_spcl', 'MS_A_PS_90D_spcl', 'MS_A_PS_mean_v_spcl',\n",
    "       'MS_A_PS_stdev_spcl']\n",
    "\n",
    "dfEL4 = generate_cycle_time(df_A_merged_lab_res, df_PressureFilter_Param_A)\n",
    "dfEL5 = df_A_merged_lab_res.join(dfEL4, how='outer')\n",
    "dfEL5.columns = A_column_names_laboratory_results #Rename the column using the prepared list\n",
    "dfEL5.set_index('DATETIME', inplace=True)\n",
    "\n",
    "df_Cycle_LabRes = dfn11.join(dfEL5, how='outer')\n",
    "df_Cycle_LabRes.fillna(method='ffill')\n",
    "df_Cycle_LabRes['Cycle_Count'] = np.arange(1, len(df_Cycle_LabRes['A_FILT_T'])+1)\n",
    "df_Cycle_LabRes_2 = df_Cycle_LabRes.reindex(columns= A_column_names_merged_final)\n",
    "df_Cycle_LabRes_2_ffillna = df_Cycle_LabRes_2.fillna(method='ffill')\n",
    "\n",
    "A_df_JAN2020_cycLab = df_Cycle_LabRes_2_ffillna\n",
    "\n",
    "#---------dfEL5 is the is the analysis results data indexed by daterange format. readt for merging with dfn11(FT02A data from Pi)\n",
    "#---------\n",
    "print('Done.', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMBINE PiDATA WITH LABORATORY RESULTS FOR BRAVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------\n",
    "B_column_names_laboratory_results = ['MS_B_Pb_spcl', 'MS_B_Zn_spcl', 'MS_B_Cu_spcl', 'MS_B_Ni_spcl', 'MS_B_Co_spcl', 'MS_B_Fe_spcl',\n",
    "       'MS_B_Mn_spcl', 'MS_B_Cr_spcl', 'MS_B_Ca_spcl', 'MS_B_Si_spcl', 'MS_B_Al_spcl', 'MS_B_Mg_spcl','MS_B_S_spcl', 'MS_B_moisture_spcl', 'MS_B_PS_10D_spcl', 'MS_B_PS_50D_spcl', 'MS_B_PS_90D_spcl',\n",
    "       'MS_B_PS_mean_v_spcl', 'MS_B_PS_stdev_spcl','DATETIME']\n",
    "\n",
    "B_column_names_merged_final = ['Cycle_Count','B_PU26B_CUR', 'B_PU27B_CUR', 'B_N2DRY', 'B_CWP_1', 'B_CWP_2',\n",
    "       'B_FILT_T', 'B_MANF_P', 'B_CWSH1', 'B_CWSH2', 'B_t_PRESS1',\n",
    "       'B_t_PRESS2', 'B_PU26B_OP', 'B_PU27B_OP', 'newdt', 'startdate',\n",
    "       'enddate', 'MS_B_Pb_spcl', 'MS_B_Zn_spcl', 'MS_B_Cu_spcl',\n",
    "       'MS_B_Ni_spcl', 'MS_B_Co_spcl', 'MS_B_Fe_spcl', 'MS_B_Mn_spcl',\n",
    "       'MS_B_Cr_spcl', 'MS_B_Ca_spcl', 'MS_B_Si_spcl', 'MS_B_Al_spcl',\n",
    "       'MS_B_Mg_spcl', 'MS_B_S_spcl', 'MS_B_moisture_spcl', 'MS_B_PS_10D_spcl',\n",
    "       'MS_B_PS_50D_spcl', 'MS_B_PS_90D_spcl', 'MS_B_PS_mean_v_spcl',\n",
    "       'MS_B_PS_stdev_spcl']\n",
    "\n",
    "B_Lab_Res_formatted_index = generate_cycle_time(df_B_merged_lab_res, df_PressureFilter_Param_B) #the function creates an index in datetime range in string format the same with the index format of the main_df (dfn11 or dfn11B)\n",
    "\n",
    "B_df_merged_lab_res_fomatted_index = df_B_merged_lab_res.join(B_Lab_Res_formatted_index, how='outer')\n",
    "B_df_merged_lab_res_fomatted_index.columns = B_column_names_laboratory_results #Rename the column names\n",
    "\n",
    "B_df_merged_lab_res_fomatted_index.set_index('DATETIME', inplace=True)\n",
    "\n",
    "df_Cycle_LabResB = dfn11B.join(B_df_merged_lab_res_fomatted_index, how='outer')\n",
    "df_Cycle_LabResB.fillna(method='ffill')\n",
    "df_Cycle_LabResB['Cycle_Count'] = np.arange(1, len(df_Cycle_LabResB['B_FILT_T'])+1)\n",
    "df_Cycle_LabRes_2B = df_Cycle_LabResB.reindex(columns= B_column_names_merged_final)\n",
    "df_Cycle_LabRes_2B_ffillna = df_Cycle_LabRes_2B.fillna(method='ffill')\n",
    "\n",
    "B_df_JAN2020_Combined_LabRes_and_Filtration_Param = df_Cycle_LabRes_2B_ffillna\n",
    "\n",
    "#---------------B_df_merged_lab_res_fomatted_index is the is the analysis results data indexed by daterange format. readt for merging with dfn11(FT02B data from Pi)\n",
    "#---------\n",
    "print('Done.', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMBINE PiDATA WITH LABORATORY RESULTS FOR BRAVO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE THE DATAFRAME TO EXCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_df_JAN2020_cycLab.to_excel(r'C:\\Users\\v.t.flores\\Documents\\2019_PF_DATA\\106FT02A_data_FEB2019.xlsx')\n",
    "#A_df_JAN2020_cycLab.to_excel(r'C:\\Users\\v.t.flores\\Documents\\106FT02A_data_JAN2019.xlsx')\n",
    "#A_df_JAN2020_cycLab.to_excel(r'\\\\THGOBNA002\\thpal\\05 Production\\150 DCS\\00 DCS Monitoring & Records\\00 Active Files\\2020\\Pressure_Filter_Data_2019\\106FT02A_data_JAN2020.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "B_df_JAN2020_Combined_LabRes_and_Filtration_Param.to_excel(r'C:\\Users\\v.t.flores\\Documents\\2019_PF_DATA\\106FT02B_data_FEB2019.xlsx')\n",
    "#B_df_JAN2020_Combined_LabRes_and_Filtration_Param.to_excel(r'\\\\THGOBNA002\\thpal\\05 Production\\150 DCS\\00 DCS Monitoring & Records\\00 Active Files\\2020\\Pressure_Filter_Data_2019\\106FT02B_data_JAN2020.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
