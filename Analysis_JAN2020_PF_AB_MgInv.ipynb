{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT THE LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exc1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from dateutil.parser import parse\n",
    "from itertools import compress\n",
    "from collections import OrderedDict\n",
    "from datetime import date\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD THE PI DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sheet From Pi\n",
    "#Step1: Load the Data for the month\n",
    "df_JAN2019Pi = pd.read_excel(r'C:\\Users\\v.t.flores\\Documents\\Pressure_Filter_2020_PiData.xlsx', sheet_name='FromPi_JAN2020', \n",
    "                             index_col=False)\n",
    "\n",
    "#Drop the row[1]\n",
    "df_JAN2019Pi = df_JAN2019Pi.drop(df_JAN2019Pi.index[0])\n",
    "\n",
    "#Step2: Parse the DATETIME column\n",
    "df_JAN2019Pi['DATETIME'] = df_JAN2019Pi['DATETIME'].apply(lambda x: parse(str(x)))\n",
    "\n",
    "df_JAN2019Pi.set_index('DATETIME', inplace=True)\n",
    "df_comb = df_JAN2019Pi\n",
    "df_comb = df_comb.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 43200 entries, 2020-01-01 00:00:00 to 2020-01-30 23:59:00\n",
      "Data columns (total 28 columns):\n",
      "A_t_CWSH1         43200 non-null object\n",
      "B_t_CWSH1         43200 non-null object\n",
      "A_t_CWSH2         43200 non-null object\n",
      "B_t_CWSH2         43200 non-null object\n",
      "A_t_DRY           43200 non-null object\n",
      "B_t_DRY           43200 non-null object\n",
      "A_t_FEED          43200 non-null object\n",
      "B_t_FEED          43200 non-null object\n",
      "A_t_PRESS1        43200 non-null object\n",
      "A_t_PRESS2        43200 non-null object\n",
      "B_t_PRESS1        43200 non-null object\n",
      "B_t_PRESS2        43200 non-null object\n",
      "106PU26A_CUR      43200 non-null object\n",
      "PU26A_OP          43200 non-null object\n",
      "PU27A_OP          43200 non-null object\n",
      "106PU27A_CUR      43200 non-null object\n",
      "106FT02A_MANFP    43200 non-null object\n",
      "106FT02A_CWP      43200 non-null object\n",
      "106FT02A_TOTWT    43200 non-null object\n",
      "106PU26B_CUR      43200 non-null object\n",
      "106PU27B_CUR      43200 non-null object\n",
      "PU26B_OP          43200 non-null object\n",
      "PU27B_OP          43200 non-null object\n",
      "FT02B_MANFP       43200 non-null object\n",
      "FT02B_CWP         43200 non-null object\n",
      "FT02B_TOTWT       43200 non-null object\n",
      "TH01_UF_DNST1     43200 non-null object\n",
      "TH01_UF_DNST2     43200 non-null object\n",
      "dtypes: object(28)\n",
      "memory usage: 9.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_comb.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECT THE ALPHA PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in sign\n",
      "  \"\"\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in sign\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in greater\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in sign\n",
      "  import sys\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in less\n",
      "  import sys\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in sign\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in sign\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in greater\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in sign\n",
      "  app.launch_new_instance()\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in less\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done\n",
      "Done 2020-07-13 09:12:10.500675\n",
      "37930 37954\n"
     ]
    }
   ],
   "source": [
    "#exc7\n",
    "#finding peaks FT for A_filtration time\n",
    "\n",
    "df_comb['A_t_FEED'] = pd.to_numeric(df_comb['A_t_FEED'], errors='coerce')\n",
    "a = np.diff(np.sign(np.diff(df_comb['A_t_FEED']))).nonzero()[0] + 1               # local min & max\n",
    "b = (np.diff(np.sign(np.diff(df_comb['A_t_FEED']))) > 0).nonzero()[0] + 1         # local min\n",
    "c = (np.diff(np.sign(np.diff(df_comb['A_t_FEED']))) < 0).nonzero()[0] + 1         # local max\n",
    "# +1 due to the fact that diff reduces the original index number\n",
    "\n",
    "\n",
    "#finding peaks FT for A_N2 Dry time\n",
    "\n",
    "df_comb['A_t_DRY'] = pd.to_numeric(df_comb['A_t_DRY'], errors='coerce')\n",
    "d = np.diff(np.sign(np.diff(df_comb['A_t_DRY']))).nonzero()[0] + 1               # local min & max\n",
    "e = (np.diff(np.sign(np.diff(df_comb['A_t_DRY']))) > 0).nonzero()[0] + 1         # local min\n",
    "f = (np.diff(np.sign(np.diff(df_comb['A_t_DRY']))) < 0).nonzero()[0] + 1         # local max\n",
    "# +1 due to the fact that diff reduces the original index number\n",
    "\n",
    "#Use b as the starting index and f as the ending index for one full cylcle of Filtration to N2 Drying\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#This code here collects the index range for the each cycle\n",
    "try:\n",
    "    ft_cyc_lim = {}\n",
    "    for i in range(0, len(b)):\n",
    "        for j in range(0, len(f)):\n",
    "            if f[j] > b[i] and f[j] < b[i+1]:\n",
    "                if f[j+1]<b[i+1]:\n",
    "                    cyc_start = b[i]\n",
    "                    cyc_end = f[j+1]\n",
    "                    ft_cyc_lim.setdefault(cyc_start,cyc_end)\n",
    "except IndexError:\n",
    "    print('\\nDone')\n",
    "#------------------------------------------------------------  \n",
    "print('Done', str(datetime.datetime.now()))\n",
    "print(cyc_start, cyc_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 2020-07-13 09:20:11.130519\n"
     ]
    }
   ],
   "source": [
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "#Remove the 'No Good Data For Calculation' values using for loop\n",
    "for x in range(0, len(df_comb['106FT02A_CWP'])):\n",
    "    if df_comb['106FT02A_CWP'][x] =='[-11059] No Good Data For Calculation':\n",
    "            df_comb['106FT02A_CWP'][x] = 0\n",
    "\n",
    "\n",
    "#code for getting the CWP for CakeWash1 and CakeWash2\n",
    "\n",
    "A_CWP_1new = list()\n",
    "A_CWP_1new[:] = []\n",
    "\n",
    "A_CWP_1_indexList = list()\n",
    "A_CWP_1_indexList[:] = []\n",
    "\n",
    "A_CWP_2new = list()\n",
    "A_CWP_2new[:] = []\n",
    "\n",
    "A_CWP_2_indexList = list()\n",
    "A_CWP_2_indexList[:] = []\n",
    "\n",
    "A_CWP_1_inner = list()\n",
    "A_CWP_1_inner[:] = []\n",
    "\n",
    "A_CWP_2_inner = list()\n",
    "A_CWP_2_inner[:] = []\n",
    "\n",
    "A_Pair_CWP_1_List = list()\n",
    "A_Pair_CWP_2_List = list()\n",
    "\n",
    "try:\n",
    "    for i in range(0, len(keyslist)):\n",
    "        A_CWPList_i = list()\n",
    "        A_CWPList_i[:] = []\n",
    "        d = list()\n",
    "        d[:] = []\n",
    "        \n",
    "        A_CWPList_i = list(np.arange(keyslist[i],ft_cyc_lim[keyslist[i]]))\n",
    "        d = (np.diff(np.sign(np.diff(df_comb['106FT02A_CWP'].iloc[keyslist[i]:ft_cyc_lim[keyslist[i]]])))< 0).nonzero()[0] + 1\n",
    "        A_CWP_for_sort = list()\n",
    "        A_CWP_for_sort[:] = [] \n",
    "        \n",
    "        A_CWP_1_indexList.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "        A_CWP_2_indexList.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "        \n",
    "        for i in range(0, len(d)):\n",
    "            CWP_val_shortlist = df_comb['106FT02A_CWP'].iloc[A_CWPList_i[d[i]]]\n",
    "            A_CWP_for_sort.append(CWP_val_shortlist)\n",
    "                    \n",
    "        A_CWP_1 = sorted(A_CWP_for_sort)[len(A_CWP_for_sort)-1]\n",
    "        A_CWP_1_inner.append(A_CWP_1)\n",
    "        \n",
    "        A_CWP_2 = sorted(A_CWP_for_sort)[len(A_CWP_for_sort)-2]\n",
    "        A_CWP_2_inner.append(A_CWP_2)\n",
    "\n",
    "    A_Pair_CWP_1_List = list(zip(A_CWP_1_inner, A_CWP_1_indexList[0:]))\n",
    "    A_Pair_CWP_2_List = list(zip(A_CWP_2_inner, A_CWP_2_indexList[0:]))\n",
    "            \n",
    "    \n",
    "except IndexError:\n",
    "    print('IndexError')\n",
    "\n",
    "df_A_Pair_CWP_1 = pd.DataFrame(A_Pair_CWP_1_List)\n",
    "df_A_Pair_CWP_1.columns = ['A_CWP_1', 'DATETIME']\n",
    "df_A_Pair_CWP_1.set_index('DATETIME', inplace=True)\n",
    "    \n",
    "df_A_Pair_CWP_2 = pd.DataFrame(A_Pair_CWP_2_List)\n",
    "df_A_Pair_CWP_2.columns = ['A_CWP_2', 'DATETIME']\n",
    "df_A_Pair_CWP_2.set_index('DATETIME', inplace=True)\n",
    "\n",
    "print('Done', str(datetime.datetime.now()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 2020-07-13 09:20:13.926000\n"
     ]
    }
   ],
   "source": [
    "#MANIFOLD PRESSURE\n",
    "\n",
    "#Remove the 'No Good Data For Calculation' values using for loop\n",
    "for x in range(0, len(df_comb['106FT02A_MANFP'])):\n",
    "    if df_comb['106FT02A_MANFP'][x] =='[-11059] No Good Data For Calculation':\n",
    "            df_comb['106FT02A_MANFP'][x] = 0\n",
    "\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "#For loop Dictionary names \n",
    "A_MANFP_MaxList = list()\n",
    "A_MANFP_MaxList[:] = []\n",
    "A_Pair_MANFP_List =list() \n",
    "A_Pair_MANFP_List[:] = []\n",
    "A_MANFP_index_List=list()      \n",
    "A_MANFP_index_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        A_MANFP_val_List=list() \n",
    "        A_MANFP_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            #Gets A manifold pressure of the cycle\n",
    "            A_MANFP = df_comb['106FT02A_MANFP'][j]         \n",
    "            A_MANFP_val_List.append(A_MANFP)\n",
    "        A_MANFP_MaxList.append(sorted(A_MANFP_val_List)[-1])\n",
    "        A_MANFP_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    A_Pair_MANFP_List = list(zip(A_MANFP_MaxList, A_MANFP_index_List[0:]))      \n",
    "                        \n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_A_Pair_MANF_P = pd.DataFrame(A_Pair_MANFP_List)\n",
    "df_A_Pair_MANF_P.columns = ['A_MANF_P', 'DATETIME']\n",
    "df_A_Pair_MANF_P.set_index('DATETIME', inplace=True)\n",
    "\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 2020-07-13 09:51:52.562343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code will use the index range (or limits) to create a list of values: feedtime range, cake wash time range,\n",
    "# then selects the max value as the parameter value for that range\n",
    "\n",
    "\n",
    "#Remove the 'No Good Data For Calculation' values using for loop\n",
    "for x in range(0, len(df_comb['A_t_FEED'])):\n",
    "    if df_comb['A_t_FEED'][x] =='[-11059] No Good Data For Calculation':\n",
    "            df_comb['A_t_FEED'][x] = 0\n",
    "\n",
    "#FILTRATION TIME\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "#For loop Dictionary names \n",
    "A_Filt_MaxList= list()\n",
    "A_Filt_MaxList[:] = []\n",
    "A_feedt_Val_List = list()\n",
    "A_feedt_Val_List[:] = []\n",
    "A_feedt_index_List = list()\n",
    "A_feedt_index_List[:] = []\n",
    "A_Pair_Filt_List = list()\n",
    "A_Pair_Filt_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        A_feedt_Val_List = list()\n",
    "        A_feedt_Val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):\n",
    "            A_feedt = df_comb['A_t_FEED'][j]\n",
    "            A_feedt_Val_List.append(A_feedt)\n",
    "        A_Filt_MaxList.append(sorted(A_feedt_Val_List)[-1])\n",
    "        A_feedt_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))  \n",
    "    A_Pair_Filt_List = list(zip(A_Filt_MaxList, A_feedt_index_List[0:]))      \n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_A_Pair_Filt_t = pd.DataFrame(A_Pair_Filt_List)\n",
    "df_A_Pair_Filt_t.columns = ['A_FILT_T', 'DATETIME']\n",
    "df_A_Pair_Filt_t.set_index('DATETIME', inplace=True)\n",
    "\n",
    "print('Done', str(datetime.datetime.now()))\n",
    "df_comb['A_t_FEED'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 2020-07-13 09:20:16.034420\n"
     ]
    }
   ],
   "source": [
    "#Remove the 'No Good Data For Calculation' values using for loop\n",
    "for x in range(0, len(df_comb['A_t_CWSH1'])):\n",
    "    if df_comb['A_t_CWSH1'][x] =='[-11059] No Good Data For Calculation':\n",
    "            df_comb['A_t_CWSH1'][x] = 0\n",
    "\n",
    "\n",
    "#CWSH1\n",
    "##wORKING as of 2020.01.15\n",
    "\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "A_t_CWSH1_Maxlist = list()\n",
    "A_t_CWSH1_Maxlist[:] = []\n",
    "\n",
    "A_CWSH1_index_List=list()      \n",
    "A_CWSH1_index_List[:] = []\n",
    "\n",
    "A_Pair_CWSH1_List =list() \n",
    "A_Pair_CWSH1_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        A_t_CWSH1_val_List=list() \n",
    "        A_t_CWSH1_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            A_t_CWSH1 = df_comb['A_t_CWSH1'][j]         \n",
    "            A_t_CWSH1_val_List.append(A_t_CWSH1)\n",
    "        A_t_CWSH1_Maxlist.append(sorted(A_t_CWSH1_val_List)[-1])\n",
    "        A_CWSH1_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    A_Pair_CWSH1_List = list(zip(A_t_CWSH1_Maxlist, A_CWSH1_index_List[0:]))      \n",
    "                        \n",
    "except:\n",
    "    pass\n",
    "df_A_Pair_CWSH1 = pd.DataFrame(A_Pair_CWSH1_List)\n",
    "df_A_Pair_CWSH1.columns = ['A_CWSH1', 'DATETIME']\n",
    "df_A_Pair_CWSH1.set_index('DATETIME', inplace=True)\n",
    "\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 2020-07-13 09:20:18.958649\n"
     ]
    }
   ],
   "source": [
    "#CWSH2\n",
    "#Remove the 'No Good Data For Calculation' values using for loop\n",
    "for x in range(0, len(df_comb['A_t_CWSH2'])):\n",
    "    if df_comb['A_t_CWSH2'][x] =='[-11059] No Good Data For Calculation':\n",
    "            df_comb['A_t_CWSH2'][x] = 0\n",
    "\n",
    "##wORKING as of 2020.01.15\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "A_t_CWSH2_Maxlist = list()\n",
    "A_t_CWSH2_Maxlist[:] = []\n",
    "\n",
    "A_CWSH2_index_List=list()      \n",
    "A_CWSH2_index_List[:] = []\n",
    "\n",
    "A_Pair_CWSH2_List =list() \n",
    "A_Pair_CWSH2_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        A_t_CWSH2_val_List=list() \n",
    "        A_t_CWSH2_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            A_t_CWSH2 = df_comb['A_t_CWSH2'][j]         \n",
    "            A_t_CWSH2_val_List.append(A_t_CWSH2)\n",
    "        A_t_CWSH2_Maxlist.append(sorted(A_t_CWSH2_val_List)[-1])\n",
    "        A_CWSH2_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    A_Pair_CWSH2_List = list(zip(A_t_CWSH2_Maxlist, A_CWSH2_index_List[0:]))      \n",
    "                        \n",
    "except:\n",
    "    pass\n",
    "  \n",
    "df_A_Pair_CWSH2 = pd.DataFrame(A_Pair_CWSH2_List)\n",
    "df_A_Pair_CWSH2.columns = ['A_CWSH2', 'DATETIME']\n",
    "df_A_Pair_CWSH2.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 2020-07-13 09:20:21.363774\n"
     ]
    }
   ],
   "source": [
    "#Pressing_1\n",
    "\n",
    "#Remove the 'No Good Data For Calculation' values using for loop\n",
    "for x in range(0, len(df_comb['A_t_PRESS1'])):\n",
    "    if df_comb['A_t_PRESS1'][x] =='[-11059] No Good Data For Calculation':\n",
    "            df_comb['A_t_PRESS1'][x] = 0\n",
    "\n",
    "##wORKING as of 2020.01.15\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "A_t_Press1_Maxlist = list()\n",
    "A_t_Press1_Maxlist[:] = []\n",
    "\n",
    "A_t_Press1_index_List=list()      \n",
    "A_t_Press1_index_List[:] = []\n",
    "\n",
    "A_Pair_A_t_Press1_List =list() \n",
    "A_Pair_A_t_Press1_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        A_t_Press1_val_List=list() \n",
    "        A_t_Press1_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            A_t_Press1 = df_comb['A_t_PRESS1'][j]         \n",
    "            A_t_Press1_val_List.append(A_t_Press1)\n",
    "        A_t_Press1_Maxlist.append(sorted(A_t_Press1_val_List)[-1])\n",
    "        A_t_Press1_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    A_Pair_Press1_List = list(zip(A_t_Press1_Maxlist, A_t_Press1_index_List[0:]))      \n",
    "                        \n",
    "except:\n",
    "    pass\n",
    "df_A_Pair_PRESS1_t = pd.DataFrame(A_Pair_Press1_List)\n",
    "df_A_Pair_PRESS1_t.columns = ['A_PRESS1_t', 'DATETIME']\n",
    "df_A_Pair_PRESS1_t.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 2020-07-13 09:20:23.494339\n"
     ]
    }
   ],
   "source": [
    "#Pressing_2\n",
    "\n",
    "#Remove the 'No Good Data For Calculation' values using for loop\n",
    "for x in range(0, len(df_comb['A_t_PRESS2'])):\n",
    "    if df_comb['A_t_PRESS2'][x] =='[-11059] No Good Data For Calculation':\n",
    "            df_comb['A_t_PRESS2'][x] = 0\n",
    "\n",
    "##wORKING as of 2020.01.15\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "A_t_Press2_Maxlist = list()\n",
    "A_t_Press2_Maxlist[:] = []\n",
    "\n",
    "A_t_Press2_index_List=list()      \n",
    "A_t_Press2_index_List[:] = []\n",
    "\n",
    "A_Pair_Press2_List =list() \n",
    "A_Pair_Press2_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        A_t_Press2_val_List=list() \n",
    "        A_t_Press2_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            A_t_Press2 = df_comb['A_t_PRESS2'][j]         \n",
    "            A_t_Press2_val_List.append(A_t_Press2)\n",
    "        A_t_Press2_Maxlist.append(sorted(A_t_Press2_val_List)[-1])\n",
    "        A_t_Press2_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    A_Pair_Press2_List = list(zip(A_t_Press2_Maxlist, A_t_Press2_index_List[0:]))      \n",
    "                        \n",
    "except:\n",
    "    pass\n",
    "  \n",
    "df_A_Pair_PRESS2_t = pd.DataFrame(A_Pair_Press2_List)\n",
    "df_A_Pair_PRESS2_t.columns = ['A_PRESS2_t', 'DATETIME']\n",
    "df_A_Pair_PRESS2_t.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 2020-07-13 09:20:30.741612\n"
     ]
    }
   ],
   "source": [
    "#A_N2 Dry\n",
    "##Working as of 2020-01-15 17:26:09.736327\n",
    "A_Pair_N2_Dry_List = list()\n",
    "A_Pair_N2_Dry_List[:] = []\n",
    "A_t_N2_Dry_indexList = list()\n",
    "A_t_N2_Dry_indexList[:] = []\n",
    "A_t_N2_Dry_val = list()\n",
    "A_t_N2_Dry_val[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):   \n",
    "        A_t_N2_Dry = df_comb['A_t_DRY'][ft_cyc_lim[keyslist[i]]]\n",
    "        A_t_N2_Dry_val.append(A_t_N2_Dry)\n",
    "        A_t_N2_Dry_indexList.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    \n",
    "    A_Pair_N2_Dry_List = list(zip(A_t_N2_Dry_val, A_t_N2_Dry_indexList[0:]))          \n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_A_Pair_N2DRY = pd.DataFrame(A_Pair_N2_Dry_List)\n",
    "df_A_Pair_N2DRY.columns = ['A_N2DRY', 'DATETIME']\n",
    "df_A_Pair_N2DRY.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 2020-07-13 09:20:32.645770\n"
     ]
    }
   ],
   "source": [
    "dfn_CWP = df_A_Pair_CWP_1.join(df_A_Pair_CWP_2, how='outer')\n",
    "\n",
    "dfn2_CWSH = df_A_Pair_CWSH1.join(df_A_Pair_CWSH2, how='outer')\n",
    "\n",
    "\n",
    "dfn7 = dfn_CWP.join(dfn2_CWSH, how='outer')\n",
    "\n",
    "df_PressureFilter_Param = dfn7\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn7.to_excel(r'C:\\Users\\v.t.flores\\Documents\\106FT02A_MgInv_data_JAN2020.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_CWP_1</th>\n",
       "      <th>A_CWP_2</th>\n",
       "      <th>A_CWSH1</th>\n",
       "      <th>A_CWSH2</th>\n",
       "      <th>newdt</th>\n",
       "      <th>startdate</th>\n",
       "      <th>enddate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATETIME</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-01-01 00:21:00 to 2020-01-01 00:52:00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.79</td>\n",
       "      <td>155</td>\n",
       "      <td>185</td>\n",
       "      <td>2020-01-01 00:21:00 to 2020-01-01 00:52:00</td>\n",
       "      <td>2020-01-01 00:21:00</td>\n",
       "      <td>2020-01-01 00:52:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 00:54:00 to 2020-01-01 01:27:00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>155</td>\n",
       "      <td>185</td>\n",
       "      <td>2020-01-01 00:54:00 to 2020-01-01 01:27:00</td>\n",
       "      <td>2020-01-01 00:54:00</td>\n",
       "      <td>2020-01-01 01:27:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 01:30:00 to 2020-01-01 02:03:00</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.83</td>\n",
       "      <td>165</td>\n",
       "      <td>195</td>\n",
       "      <td>2020-01-01 01:30:00 to 2020-01-01 02:03:00</td>\n",
       "      <td>2020-01-01 01:30:00</td>\n",
       "      <td>2020-01-01 02:03:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 02:23:00 to 2020-01-01 02:54:00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>165</td>\n",
       "      <td>195</td>\n",
       "      <td>2020-01-01 02:23:00 to 2020-01-01 02:54:00</td>\n",
       "      <td>2020-01-01 02:23:00</td>\n",
       "      <td>2020-01-01 02:54:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 02:56:00 to 2020-01-01 03:27:00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>165</td>\n",
       "      <td>195</td>\n",
       "      <td>2020-01-01 02:56:00 to 2020-01-01 03:27:00</td>\n",
       "      <td>2020-01-01 02:56:00</td>\n",
       "      <td>2020-01-01 03:27:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            A_CWP_1  A_CWP_2  A_CWSH1  \\\n",
       "DATETIME                                                                \n",
       "2020-01-01 00:21:00 to 2020-01-01 00:52:00     0.83     0.79      155   \n",
       "2020-01-01 00:54:00 to 2020-01-01 01:27:00     0.83     0.83      155   \n",
       "2020-01-01 01:30:00 to 2020-01-01 02:03:00     0.87     0.83      165   \n",
       "2020-01-01 02:23:00 to 2020-01-01 02:54:00     0.83     0.82      165   \n",
       "2020-01-01 02:56:00 to 2020-01-01 03:27:00     0.83     0.83      165   \n",
       "\n",
       "                                            A_CWSH2  \\\n",
       "DATETIME                                              \n",
       "2020-01-01 00:21:00 to 2020-01-01 00:52:00      185   \n",
       "2020-01-01 00:54:00 to 2020-01-01 01:27:00      185   \n",
       "2020-01-01 01:30:00 to 2020-01-01 02:03:00      195   \n",
       "2020-01-01 02:23:00 to 2020-01-01 02:54:00      195   \n",
       "2020-01-01 02:56:00 to 2020-01-01 03:27:00      195   \n",
       "\n",
       "                                                                                 newdt  \\\n",
       "DATETIME                                                                                 \n",
       "2020-01-01 00:21:00 to 2020-01-01 00:52:00  2020-01-01 00:21:00 to 2020-01-01 00:52:00   \n",
       "2020-01-01 00:54:00 to 2020-01-01 01:27:00  2020-01-01 00:54:00 to 2020-01-01 01:27:00   \n",
       "2020-01-01 01:30:00 to 2020-01-01 02:03:00  2020-01-01 01:30:00 to 2020-01-01 02:03:00   \n",
       "2020-01-01 02:23:00 to 2020-01-01 02:54:00  2020-01-01 02:23:00 to 2020-01-01 02:54:00   \n",
       "2020-01-01 02:56:00 to 2020-01-01 03:27:00  2020-01-01 02:56:00 to 2020-01-01 03:27:00   \n",
       "\n",
       "                                                     startdate  \\\n",
       "DATETIME                                                         \n",
       "2020-01-01 00:21:00 to 2020-01-01 00:52:00 2020-01-01 00:21:00   \n",
       "2020-01-01 00:54:00 to 2020-01-01 01:27:00 2020-01-01 00:54:00   \n",
       "2020-01-01 01:30:00 to 2020-01-01 02:03:00 2020-01-01 01:30:00   \n",
       "2020-01-01 02:23:00 to 2020-01-01 02:54:00 2020-01-01 02:23:00   \n",
       "2020-01-01 02:56:00 to 2020-01-01 03:27:00 2020-01-01 02:56:00   \n",
       "\n",
       "                                                       enddate  \n",
       "DATETIME                                                        \n",
       "2020-01-01 00:21:00 to 2020-01-01 00:52:00 2020-01-01 00:52:00  \n",
       "2020-01-01 00:54:00 to 2020-01-01 01:27:00 2020-01-01 01:27:00  \n",
       "2020-01-01 01:30:00 to 2020-01-01 02:03:00 2020-01-01 02:03:00  \n",
       "2020-01-01 02:23:00 to 2020-01-01 02:54:00 2020-01-01 02:54:00  \n",
       "2020-01-01 02:56:00 to 2020-01-01 03:27:00 2020-01-01 03:27:00  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn7.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECT THE BRAVO PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in sign\n",
      "  \"\"\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in sign\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in greater\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in sign\n",
      "  import sys\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in less\n",
      "  import sys\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in sign\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in sign\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in greater\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in sign\n",
      "  app.launch_new_instance()\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in less\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done\n",
      "Done 2020-07-13 09:22:46.647433\n"
     ]
    }
   ],
   "source": [
    "#exc7\n",
    "#finding peaks FT for B_filtration time\n",
    "\n",
    "df_comb['B_t_FEED'] = pd.to_numeric(df_comb['B_t_FEED'], errors='coerce')\n",
    "a = np.diff(np.sign(np.diff(df_comb['B_t_FEED']))).nonzero()[0] + 1               # local min & max\n",
    "b = (np.diff(np.sign(np.diff(df_comb['B_t_FEED']))) > 0).nonzero()[0] + 1         # local min\n",
    "c = (np.diff(np.sign(np.diff(df_comb['B_t_FEED']))) < 0).nonzero()[0] + 1         # local max\n",
    "# +1 due to the fact that diff reduces the original index number\n",
    "\n",
    "\n",
    "#finding peaks FT for A_N2 Dry time\n",
    "\n",
    "df_comb['B_t_DRY'] = pd.to_numeric(df_comb['B_t_DRY'], errors='coerce')\n",
    "d = np.diff(np.sign(np.diff(df_comb['B_t_DRY']))).nonzero()[0] + 1               # local min & max\n",
    "e = (np.diff(np.sign(np.diff(df_comb['B_t_DRY']))) > 0).nonzero()[0] + 1         # local min\n",
    "f = (np.diff(np.sign(np.diff(df_comb['B_t_DRY']))) < 0).nonzero()[0] + 1         # local max\n",
    "# +1 due to the fact that diff reduces the original index number\n",
    "\n",
    "#Use b as the starting index and f as the ending index for one full cylcle of Filtration to N2 Drying\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#From the peaks computed in previous code, the index range for each cycle is stored in a dictionary called ft_cyc_lim\n",
    "try:\n",
    "    ft_cyc_lim = {}\n",
    "\n",
    "    for i in range(0, len(b)):\n",
    "        for j in range(0, len(f)):\n",
    "            if f[j] > b[i] and f[j] < b[i+1]:\n",
    "                if f[j+1]<b[i+1]:\n",
    "                    cyc_start = b[i]\n",
    "                    cyc_end = f[j+1]\n",
    "                    ft_cyc_lim.setdefault(cyc_start,cyc_end)\n",
    "\n",
    "except IndexError:\n",
    "    print('\\nDone')\n",
    "#------------------------------------------------------------     \n",
    "\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 2020-07-13 09:22:48.034688\n"
     ]
    }
   ],
   "source": [
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "#Remove the 'No Good Data For Calculation' values using for loop\n",
    "for x in range(0, len(df_comb['FT02B_CWP'])):\n",
    "    if df_comb['FT02B_CWP'][x] =='[-11059] No Good Data For Calculation':\n",
    "            df_comb['FT02B_CWP'][x] = 0\n",
    "\n",
    "#code for getting the CWP for CakeWash1 and CakeWash2\n",
    "##Working as of 1/15/2020\n",
    "B_CWP_1new = list()\n",
    "B_CWP_1new[:] = []\n",
    "\n",
    "B_CWP_1_indexList = list()\n",
    "B_CWP_1_indexList[:] = []\n",
    "\n",
    "B_CWP_2new = list()\n",
    "B_CWP_2new[:] = []\n",
    "\n",
    "B_CWP_2_indexList = list()\n",
    "B_CWP_2_indexList[:] = []\n",
    "\n",
    "B_CWP_1_inner = list()\n",
    "B_CWP_1_inner[:] = []\n",
    "\n",
    "B_CWP_2_inner = list()\n",
    "B_CWP_2_inner[:] = []\n",
    "\n",
    "B_Pair_CWP_1_List = list()\n",
    "B_Pair_CWP_2_List = list()\n",
    "\n",
    "try:\n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_CWPList_i = list()\n",
    "        B_CWPList_i[:] = []\n",
    "        d = list()\n",
    "        d[:] = []\n",
    "        \n",
    "        B_CWPList_i = list(np.arange(keyslist[i],ft_cyc_lim[keyslist[i]]))\n",
    "        d = (np.diff(np.sign(np.diff(df_comb['FT02B_CWP'].iloc[keyslist[i]:ft_cyc_lim[keyslist[i]]])))< 0).nonzero()[0] + 1\n",
    "        B_CWP_for_sort = list()\n",
    "        B_CWP_for_sort[:] = [] \n",
    "        \n",
    "        B_CWP_1_indexList.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "        B_CWP_2_indexList.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "        \n",
    "        if d.size != 0:\n",
    "            for i in range(0, len(d)):\n",
    "                CWP_val_shortlist = df_comb['FT02B_CWP'].iloc[B_CWPList_i[d[i]]]\n",
    "                B_CWP_for_sort.append(CWP_val_shortlist)\n",
    "\n",
    "            B_CWP_1 = sorted(B_CWP_for_sort)[len(B_CWP_for_sort)-1]\n",
    "            B_CWP_1_inner.append(B_CWP_1)\n",
    "\n",
    "            B_CWP_2 = sorted(B_CWP_for_sort)[len(B_CWP_for_sort)-2]\n",
    "            B_CWP_2_inner.append(B_CWP_2)\n",
    "        if d.size == 0:\n",
    "            B_CWP_1_inner.append(0)\n",
    "            B_CWP_2_inner.append(0)\n",
    "\n",
    "    B_Pair_CWP_1_List = list(zip(B_CWP_1_inner, B_CWP_1_indexList[0:]))\n",
    "    B_Pair_CWP_2_List = list(zip(B_CWP_2_inner, B_CWP_2_indexList[0:]))\n",
    "            \n",
    "    \n",
    "except IndexError:\n",
    "    print('IndexError')\n",
    "\n",
    "df_B_Pair_CWP_1 = pd.DataFrame(B_Pair_CWP_1_List)\n",
    "df_B_Pair_CWP_1.columns = ['B_CWP_1', 'DATETIME']\n",
    "df_B_Pair_CWP_1.set_index('DATETIME', inplace=True)\n",
    "    \n",
    "df_B_Pair_CWP_2 = pd.DataFrame(B_Pair_CWP_2_List)\n",
    "df_B_Pair_CWP_2.columns = ['B_CWP_2', 'DATETIME']\n",
    "df_B_Pair_CWP_2.set_index('DATETIME', inplace=True)\n",
    "   \n",
    "    \n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 2020-07-13 09:22:49.285044\n"
     ]
    }
   ],
   "source": [
    "# This code will use the index range (or limits) to create a list of values: feedtime range, cake wash time range,\n",
    "# then selects the max value as the parameter value for that range\n",
    "\n",
    "#FILTRATION TIME\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "#For loop Dictionary names \n",
    "B_Filt_MaxList= list()\n",
    "B_Filt_MaxList[:] = []\n",
    "B_feedt_Val_List = list()\n",
    "B_feedt_Val_List[:] = []\n",
    "B_feedt_index_List = list()\n",
    "B_feedt_index_List[:] = []\n",
    "B_Pair_Filt_List = list()\n",
    "B_Pair_Filt_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_feedt_Val_List = list()\n",
    "        B_feedt_Val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):\n",
    "            B_feedt = df_comb['B_t_FEED'][j]\n",
    "            B_feedt_Val_List.append(B_feedt)\n",
    "        B_Filt_MaxList.append(sorted(B_feedt_Val_List)[-1])\n",
    "        B_feedt_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))  \n",
    "    B_Pair_Filt_List = list(zip(B_Filt_MaxList, B_feedt_index_List[0:]))      \n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_B_Pair_Filt_t = pd.DataFrame(B_Pair_Filt_List)\n",
    "df_B_Pair_Filt_t.columns = ['B_FILT_T', 'DATETIME']\n",
    "df_B_Pair_Filt_t.set_index('DATETIME', inplace=True)\n",
    "\n",
    "print('Done', str(datetime.datetime.now()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 2020-07-13 09:22:51.563070\n"
     ]
    }
   ],
   "source": [
    "#MANIFOLD PRESSURE_B\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "#For loop Dictionary names \n",
    "B_MANFP_MaxList = list()\n",
    "B_MANFP_MaxList[:] = []\n",
    "B_Pair_MANFP_List =list() \n",
    "B_Pair_MANFP_List[:] = []\n",
    "B_MANFP_index_List=list()      \n",
    "B_MANFP_index_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_MANFP_val_List=list() \n",
    "        B_MANFP_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            #Gets A manifold pressure of the cycle\n",
    "            B_MANFP = df_comb['FT02B_MANFP'][j]         \n",
    "            B_MANFP_val_List.append(B_MANFP)\n",
    "        B_MANFP_MaxList.append(sorted(B_MANFP_val_List)[-1])\n",
    "        B_MANFP_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    B_Pair_MANFP_List = list(zip(B_MANFP_MaxList, B_MANFP_index_List[0:]))      \n",
    "                        \n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_B_Pair_MANF_P = pd.DataFrame(B_Pair_MANFP_List)\n",
    "df_B_Pair_MANF_P.columns = ['B_MANF_P', 'DATETIME']\n",
    "df_B_Pair_MANF_P.set_index('DATETIME', inplace=True)\n",
    "\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 2020-07-13 09:22:52.873695\n",
      "Done 2020-07-13 09:22:53.216278\n",
      "Done 2020-07-13 09:22:53.567306\n",
      "Done 2020-07-13 09:22:53.916374\n",
      "Done 2020-07-13 09:22:54.267968\n",
      "Done 2020-07-13 09:22:54.633974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:182: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 2020-07-13 09:22:56.180248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:216: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 2020-07-13 09:22:57.759770\n",
      "Done 2020-07-13 09:22:57.785731\n"
     ]
    }
   ],
   "source": [
    "#CWSH1\n",
    "##wORKING as of 2020.01.15\n",
    "\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "B_t_CWSH1_Maxlist = list()\n",
    "B_t_CWSH1_Maxlist[:] = []\n",
    "\n",
    "B_CWSH1_index_List=list()      \n",
    "B_CWSH1_index_List[:] = []\n",
    "\n",
    "B_Pair_CWSH1_List =list() \n",
    "B_Pair_CWSH1_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_t_CWSH1_val_List=list() \n",
    "        B_t_CWSH1_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            B_t_CWSH1 = df_comb['B_t_CWSH1'][j]         \n",
    "            B_t_CWSH1_val_List.append(B_t_CWSH1)\n",
    "        B_t_CWSH1_Maxlist.append(sorted(B_t_CWSH1_val_List)[-1])\n",
    "        B_CWSH1_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    B_Pair_CWSH1_List = list(zip(B_t_CWSH1_Maxlist, B_CWSH1_index_List[0:]))      \n",
    "                        \n",
    "except:\n",
    "    pass\n",
    "df_B_Pair_CWSH1 = pd.DataFrame(B_Pair_CWSH1_List)\n",
    "df_B_Pair_CWSH1.columns = ['B_CWSH1', 'DATETIME']\n",
    "df_B_Pair_CWSH1.set_index('DATETIME', inplace=True)\n",
    "\n",
    "print('Done', str(datetime.datetime.now()))\n",
    "#CWSH2\n",
    "##wORKING as of 2020.01.15\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "B_t_CWSH2_Maxlist = list()\n",
    "B_t_CWSH2_Maxlist[:] = []\n",
    "\n",
    "B_CWSH2_index_List=list()      \n",
    "B_CWSH2_index_List[:] = []\n",
    "\n",
    "B_Pair_CWSH2_List =list() \n",
    "B_Pair_CWSH2_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_t_CWSH2_val_List=list() \n",
    "        B_t_CWSH2_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            B_t_CWSH2 = df_comb['B_t_CWSH2'][j]         \n",
    "            B_t_CWSH2_val_List.append(B_t_CWSH2)\n",
    "        B_t_CWSH2_Maxlist.append(sorted(B_t_CWSH2_val_List)[-1])\n",
    "        B_CWSH2_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    B_Pair_CWSH2_List = list(zip(B_t_CWSH2_Maxlist, B_CWSH2_index_List[0:]))      \n",
    "                        \n",
    "except:\n",
    "    pass\n",
    "  \n",
    "df_B_Pair_CWSH2 = pd.DataFrame(B_Pair_CWSH2_List)\n",
    "df_B_Pair_CWSH2.columns = ['B_CWSH2', 'DATETIME']\n",
    "df_B_Pair_CWSH2.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))\n",
    "#Pressing_1\n",
    "##wORKING as of 2020.01.15\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "B_t_Press1_Maxlist = list()\n",
    "B_t_Press1_Maxlist[:] = []\n",
    "\n",
    "B_t_Press1_index_List=list()      \n",
    "B_t_Press1_index_List[:] = []\n",
    "\n",
    "B_Pair_B_t_Press1_List =list() \n",
    "B_Pair_B_t_Press1_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_t_Press1_val_List=list() \n",
    "        B_t_Press1_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            B_t_Press1 = df_comb['B_t_PRESS1'][j]         \n",
    "            B_t_Press1_val_List.append(B_t_Press1)\n",
    "        B_t_Press1_Maxlist.append(sorted(B_t_Press1_val_List)[-1])\n",
    "        B_t_Press1_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    B_Pair_Press1_List = list(zip(B_t_Press1_Maxlist, B_t_Press1_index_List[0:]))      \n",
    "                        \n",
    "except:\n",
    "    pass\n",
    "df_B_Pair_PRESS1_t = pd.DataFrame(B_Pair_Press1_List)\n",
    "df_B_Pair_PRESS1_t.columns = ['B_PRESS1_t', 'DATETIME']\n",
    "df_B_Pair_PRESS1_t.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))\n",
    "#Pressing_2\n",
    "##wORKING as of 2020.01.15\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "B_t_Press2_Maxlist = list()\n",
    "B_t_Press2_Maxlist[:] = []\n",
    "\n",
    "B_t_Press2_index_List=list()      \n",
    "B_t_Press2_index_List[:] = []\n",
    "\n",
    "B_Pair_Press2_List =list() \n",
    "B_Pair_Press2_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_t_Press2_val_List=list() \n",
    "        B_t_Press2_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            B_t_Press2 = df_comb['B_t_PRESS2'][j]         \n",
    "            B_t_Press2_val_List.append(B_t_Press2)\n",
    "        B_t_Press2_Maxlist.append(sorted(B_t_Press2_val_List)[-1])\n",
    "        B_t_Press2_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    B_Pair_Press2_List = list(zip(B_t_Press2_Maxlist, B_t_Press2_index_List[0:]))      \n",
    "                        \n",
    "except:\n",
    "    pass\n",
    "  \n",
    "df_B_Pair_PRESS2_t = pd.DataFrame(B_Pair_Press2_List)\n",
    "df_B_Pair_PRESS2_t.columns = ['B_PRESS2_t', 'DATETIME']\n",
    "df_B_Pair_PRESS2_t.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))\n",
    "\n",
    "#B_PU26B_OP\n",
    "##Working as of 2020.01.15\n",
    "B_PU26B_OP_Maxlist = list()\n",
    "B_PU26B_OP_Maxlist[:] = []\n",
    "\n",
    "B_PU26B_OP_index_List=list()      \n",
    "B_PU26B_OP_index_List[:] = []\n",
    "\n",
    "B_Pair_PU26B_OP_List =list() \n",
    "B_Pair_PU26B_OP_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_PU26B_OP_val_List=list() \n",
    "        B_PU26B_OP_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            B_PU26B_OP = df_comb['PU26B_OP'][j]      \n",
    "            B_PU26B_OP_val_List.append(B_PU26B_OP)\n",
    "        B_PU26B_OP_Maxlist.append(sorted(B_PU26B_OP_val_List)[-1])\n",
    "        B_PU26B_OP_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    B_Pair_PU26B_OP_List = list(zip(B_PU26B_OP_Maxlist, B_PU26B_OP_index_List[0:]))          \n",
    "except:\n",
    "    pass\n",
    "df_B_Pair_PU26B_OP = pd.DataFrame(B_Pair_PU26B_OP_List)\n",
    "df_B_Pair_PU26B_OP.columns = ['B_PU26B_OP', 'DATETIME']\n",
    "df_B_Pair_PU26B_OP.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))\n",
    "\n",
    "#B_PU27B_OP\n",
    "##Working as of 2020.01.15\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "B_PU27B_OP_Maxlist = list()\n",
    "B_PU27B_OP_Maxlist[:] = []\n",
    "\n",
    "B_PU27B_OP_index_List=list()      \n",
    "B_PU27B_OP_index_List[:] = []\n",
    "\n",
    "B_Pair_PU27B_OP_List =list() \n",
    "B_Pair_PU27B_OP_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_PU27B_OP_val_List=list() \n",
    "        B_PU27B_OP_val_List[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            B_PU27B_OP = df_comb['PU27B_OP'][j]      \n",
    "            B_PU27B_OP_val_List.append(B_PU27B_OP)\n",
    "        B_PU27B_OP_Maxlist.append(sorted(B_PU27B_OP_val_List)[-1])\n",
    "        B_PU27B_OP_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    B_Pair_PU27B_OP_List = list(zip(B_PU27B_OP_Maxlist, B_PU27B_OP_index_List[0:]))          \n",
    "except:\n",
    "    pass\n",
    "  \n",
    "df_B_Pair_PU27B_OP = pd.DataFrame(B_Pair_PU27B_OP_List)\n",
    "df_B_Pair_PU27B_OP.columns = ['B_PU27B_OP', 'DATETIME']\n",
    "df_B_Pair_PU27B_OP.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))\n",
    "\n",
    "#this code replaces the 'No Good Data' from 106PU26A_CUR column\n",
    "for x in range(0, len(df_comb['106PU26B_CUR'])):\n",
    "    if df_comb['106PU26B_CUR'][x] =='[-11059] No Good Data For Calculation':\n",
    "            df_comb['106PU26B_CUR'][x] = 0\n",
    "#B_PU26A_CUR\n",
    "##Working as of 2020.01.15\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "B_PU26B_CUR_Maxlist = list()\n",
    "B_PU26B_CUR_Maxlist[:] = []\n",
    "\n",
    "B_PU26B_CUR_index_List=list()      \n",
    "B_PU26B_CUR_index_List[:] = []\n",
    "\n",
    "B_Pair_PU26B_CUR_List =list() \n",
    "B_Pair_PU26B_CUR_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_PU26B_CUR_val=list() \n",
    "        B_PU26B_CUR_val[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            B_PU26B_CUR = df_comb['106PU26B_CUR'][j]      \n",
    "            B_PU26B_CUR_val.append(B_PU26B_CUR)\n",
    "        B_PU26B_CUR_Maxlist.append(sorted(B_PU26B_CUR_val)[-1])\n",
    "        B_PU26B_CUR_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    B_Pair_PU26B_CUR_List = list(zip(B_PU26B_CUR_Maxlist, B_PU26B_CUR_index_List[0:]))          \n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_B_Pair_PU26B_CUR = pd.DataFrame(B_Pair_PU26B_CUR_List)\n",
    "df_B_Pair_PU26B_CUR.columns = ['B_PU26B_CUR', 'DATETIME']\n",
    "df_B_Pair_PU26B_CUR.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))\n",
    "\n",
    "#this code replaces the 'No Good Data' from 106PU26A_CUR column\n",
    "for x in range(0, len(df_comb['106PU27B_CUR'])):\n",
    "    if df_comb['106PU27B_CUR'][x] =='[-11059] No Good Data For Calculation':\n",
    "            df_comb['106PU27B_CUR'][x] = 0\n",
    "#B_PU27B_CUR\n",
    "##Working as of 2020.01.15 after a million trials!!!!!!!!!! mabuak na akong utok!!!\n",
    "keyslist = list(ft_cyc_lim.keys()) #creates a list of the ft_cyc_lim keys so that we can iterate later\n",
    "\n",
    "B_PU27B_CUR_Maxlist = list()\n",
    "B_PU27B_CUR_Maxlist[:] = []\n",
    "\n",
    "B_PU27B_CUR_index_List=list()      \n",
    "B_PU27B_CUR_index_List[:] = []\n",
    "\n",
    "B_Pair_PU27B_CUR_List =list() \n",
    "B_Pair_PU27B_CUR_List[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):\n",
    "        B_PU27B_CUR_val=list() \n",
    "        B_PU27B_CUR_val[:] = []\n",
    "        for j in range(keyslist[i], ft_cyc_lim[keyslist[i]]):               \n",
    "            B_PU27B_CUR = df_comb['106PU27B_CUR'][j]      \n",
    "            B_PU27B_CUR_val.append(B_PU27B_CUR)\n",
    "        B_PU27B_CUR_Maxlist.append(sorted(B_PU27B_CUR_val)[-1])\n",
    "        B_PU27B_CUR_index_List.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    B_Pair_PU27B_CUR_List = list(zip(B_PU27B_CUR_Maxlist, B_PU27B_CUR_index_List[0:]))          \n",
    "except:\n",
    "    pass\n",
    "  \n",
    "df_B_Pair_PU27B_CUR = pd.DataFrame(B_Pair_PU27B_CUR_List)\n",
    "df_B_Pair_PU27B_CUR.columns = ['B_PU27B_CUR', 'DATETIME']\n",
    "df_B_Pair_PU27B_CUR.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))\n",
    "\n",
    "#A_N2 Dry\n",
    "##Working as of 2020-01-15 17:26:09.736327\n",
    "B_Pair_N2_Dry_List = list()\n",
    "B_Pair_N2_Dry_List[:] = []\n",
    "B_t_N2_Dry_indexList = list()\n",
    "B_t_N2_Dry_indexList[:] = []\n",
    "B_t_N2_Dry_val = list()\n",
    "B_t_N2_Dry_val[:] = []\n",
    "try:         \n",
    "    for i in range(0, len(keyslist)):   \n",
    "        B_t_N2_Dry = df_comb['B_t_DRY'][ft_cyc_lim[keyslist[i]]]\n",
    "        B_t_N2_Dry_val.append(B_t_N2_Dry)\n",
    "        B_t_N2_Dry_indexList.append((str(df_comb.index[keyslist[i]])+\" to \"+str(df_comb.index[ft_cyc_lim[keyslist[i]]])))\n",
    "    \n",
    "    B_Pair_N2_Dry_List = list(zip(B_t_N2_Dry_val, B_t_N2_Dry_indexList[0:]))          \n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_B_Pair_N2DRY = pd.DataFrame(B_Pair_N2_Dry_List)\n",
    "df_B_Pair_N2DRY.columns = ['B_N2DRY', 'DATETIME']\n",
    "df_B_Pair_N2DRY.set_index('DATETIME', inplace=True)\n",
    "print('Done', str(datetime.datetime.now()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 2020-07-13 09:24:11.742065\n"
     ]
    }
   ],
   "source": [
    "#Combining the dataframes for bravo parameters\n",
    "dfnB_CWP = df_B_Pair_CWP_1.join(df_B_Pair_CWP_2, how='outer')\n",
    "\n",
    "dfn2B_CWSH = df_B_Pair_CWSH1.join(df_B_Pair_CWSH2, how='outer')\n",
    "\n",
    "dfn7B = dfnB_CWP.join(dfn2B_CWSH, how='outer')\n",
    "\n",
    "df_PressureFilter_Param_B = dfn7B\n",
    "print('Done', str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn7B.to_excel(r'C:\\Users\\v.t.flores\\Documents\\106FT02B_data_MgInv_JAN2020.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABORATORY ANALYSIS RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "wb1 = openpyxl.load_workbook(r'\\\\thgobna001\\userdata\\THPAL\\Department\\TAGANITO\\SHARED\\AnalysisResult\\ANALYSIS RESULTS 2020\\JANUARY 2020\\1) JANUARY_2020.xlsx')\n",
    "#####This block of code appends the Analysis Results from December2019\n",
    "FT02wb = wb1['106FT02AB']\n",
    "#update for the targetmonth\n",
    "currentmonthyr = datetime.date(2020,1,1)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------\n",
    "#collectst the index for the date.day the sample was taken and puts it in the list Smpl_day_index\n",
    "Smpl_day_index = list()\n",
    "Smpl_day_index[:] = []\n",
    "for i in range (1, FT02wb.max_row):\n",
    "    if type(FT02wb['B'+ str(i)].value) == int:\n",
    "        #print(str(i)+':'+str(TH01_UF_sheet['B'+ str(i)].value))\n",
    "        Smpl_day_index.append(i)\n",
    "\n",
    "#smpl_end_index is the last row index for the date.day the sample was taken\n",
    "for i in range(1, FT02wb.max_row):\n",
    "    if str(FT02wb['B'+str(i)].value).split(' ')[0] == 'Daily':\n",
    "        smpl_end_index = i\n",
    "        break\n",
    "\n",
    "#fcn definition        \n",
    "def filter_detrmin(cell_addrs):\n",
    "    PF_regex2 = re.compile(r'[a-z0-8]', re.IGNORECASE| re.VERBOSE|re.DOTALL)\n",
    "    filtername2 = PF_regex2.findall(str(cell_addrs))\n",
    "    for i in range(0, len(filtername2)):\n",
    "        if filtername2[i] != filtername2[-1]:\n",
    "            if filtername2[i]+filtername2[i+1] == '2A':\n",
    "                PFname = 'A'\n",
    "                return PFname\n",
    "            if filtername2[i]+filtername2[i+1] == '2B':\n",
    "                PFname = 'B'\n",
    "                return PFname      \n",
    "#-----------------end of function defn-----------        \n",
    "\n",
    "#fcn dfn\n",
    "def time_cleaner(x):\n",
    "    time_regex = re.compile(r'[0-8a-zA-Z:]', re.IGNORECASE| re.VERBOSE|re.DOTALL)\n",
    "    timeact = time_regex.findall(x)\n",
    "    newtime = ''.join(timeact)\n",
    "    t = parse(x)\n",
    "    new_str_time = str(t.hour)+':'+str(t.minute)+':'+str(t.second)\n",
    "    return new_str_time\n",
    "#------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ni\n",
    "#Code for creating a dataframe for MS_A analysis result for Element Ni\n",
    "A_Ni_datelist = list()\n",
    "A_Ni_datelist[:] = []\n",
    "B_Ni_datelist = list()\n",
    "B_Ni_datelist[:] = []\n",
    "\n",
    "A_Ni_list = list()\n",
    "A_Ni_list[:] = []\n",
    "B_Ni_list = list()\n",
    "B_Ni_list[:] = []\n",
    "\n",
    "A_Ni_list_pair = list()\n",
    "A_Ni_list_pair[:] = []\n",
    "B_Ni_list_pair = list()\n",
    "B_Ni_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    A_Ni_datelist.append(parse(A_date))  \n",
    "                    A_Ni = FT02wb['W'+str(x)].value\n",
    "                    A_Ni_list.append(A_Ni)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                  \n",
    "                    B_Ni_datelist.append(parse(B_date)) \n",
    "                    B_Ni = FT02wb['W'+str(x)].value\n",
    "                    B_Ni_list.append(B_Ni)\n",
    "\n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    A_Ni_datelist.append(parse(A_date))\n",
    "                    A_Ni = FT02wb['W'+str(x)].value\n",
    "                    A_Ni_list.append(A_Ni)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    B_Ni_datelist.append(parse(B_date))\n",
    "                    B_Ni = FT02wb['W'+str(x)].value\n",
    "                    B_Ni_list.append(B_Ni)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_Ni_list_pair = list(zip(A_Ni_list, A_Ni_datelist[0:]))\n",
    "A_Ni_df = pd.DataFrame(A_Ni_list_pair)\n",
    "A_Ni_df.columns = ['MS_A_Ni', 'DATETIME'] ##check if list title is updated\n",
    "A_Ni_df.set_index('DATETIME', inplace=True)\n",
    "A_Ni_df = A_Ni_df.loc[~A_Ni_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "B_Ni_list_pair = list(zip(B_Ni_list, B_Ni_datelist[0:]))\n",
    "B_Ni_df = pd.DataFrame(B_Ni_list_pair)\n",
    "B_Ni_df.columns = ['MS_B_Ni', 'DATETIME'] ##check if list title is updated\n",
    "B_Ni_df.set_index('DATETIME', inplace=True)\n",
    "B_Ni_df = B_Ni_df.loc[~B_Ni_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "#-----End for Ni code\n",
    "\n",
    "#Mg\n",
    "#Mgde for creating a dataframe for MS_A analysis result for Element Mg\n",
    "A_Mg_datelist = list()\n",
    "A_Mg_datelist[:] = []\n",
    "B_Mg_datelist = list()\n",
    "B_Mg_datelist[:] = []\n",
    "\n",
    "A_Mg_list = list()\n",
    "A_Mg_list[:] = []\n",
    "B_Mg_list = list()\t\n",
    "B_Mg_list[:] = []\n",
    "\n",
    "A_Mg_list_pair = list()\n",
    "A_Mg_list_pair[:] = []\n",
    "B_Mg_list_pair = list()\n",
    "B_Mg_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(parse(A_date))\n",
    "                    A_Mg_datelist.append(parse(A_date))  \n",
    "                    A_Mg = FT02wb['AE'+str(x)].value\n",
    "                    A_Mg_list.append(A_Mg)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_datelist)\n",
    "                    B_Mg_datelist.append(parse(B_date)) \n",
    "                    B_Mg = FT02wb['AE'+str(x)].value\n",
    "                    B_Mg_list.append(B_Mg)\n",
    "\n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(A_date) \n",
    "                    A_Mg_datelist.append(parse(A_date))\n",
    "                    A_Mg = FT02wb['AE'+str(x)].value\n",
    "                    A_Mg_list.append(A_Mg)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_date)\n",
    "                    B_Mg_datelist.append(parse(B_date))\n",
    "                    B_Mg = FT02wb['AE'+str(x)].value\n",
    "                    B_Mg_list.append(B_Mg)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_Mg_list_pair = list(zip(A_Mg_list, A_Mg_datelist[0:]))\n",
    "A_Mg_df = pd.DataFrame(A_Mg_list_pair)\n",
    "A_Mg_df.columns = ['MS_A_Mg', 'DATETIME'] ##check if list title is updated\n",
    "A_Mg_df.set_index('DATETIME', inplace=True)\n",
    "A_Mg_df = A_Mg_df.loc[~A_Mg_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "\n",
    "B_Mg_list_pair = list(zip(B_Mg_list, B_Mg_datelist[0:]))\n",
    "B_Mg_df = pd.DataFrame(B_Mg_list_pair)\n",
    "B_Mg_df.columns = ['MS_B_Mg', 'DATETIME'] ##check if list title is updated\n",
    "B_Mg_df.set_index('DATETIME', inplace=True)\n",
    "B_Mg_df = B_Mg_df.loc[~B_Mg_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "#-----End for Mg code\n",
    "\n",
    "#PS_50D\n",
    "#PS_50Dde for creating a dataframe for MS_A analysis result for Element PS_50D\n",
    "A_PS_50D_datelist = list()\n",
    "A_PS_50D_datelist[:] = []\n",
    "B_PS_50D_datelist = list()\n",
    "B_PS_50D_datelist[:] = []\n",
    "\n",
    "A_PS_50D_list = list()\n",
    "A_PS_50D_list[:] = []\n",
    "B_PS_50D_list = list()\t\n",
    "B_PS_50D_list[:] = []\t\n",
    "\n",
    "A_PS_50D_list_pair = list()\n",
    "A_PS_50D_list_pair[:] = []\n",
    "B_PS_50D_list_pair = list()\n",
    "B_PS_50D_list_pair[:] = []\n",
    "\n",
    "for y in range(0, len(Smpl_day_index)):   \n",
    "    \n",
    "    if Smpl_day_index[y] != Smpl_day_index[-1]:      \n",
    "        for x in range(Smpl_day_index[y], Smpl_day_index[y+1]):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(parse(A_date))\n",
    "                    A_PS_50D_datelist.append(parse(A_date))  \n",
    "                    A_PS_50D = FT02wb['AR'+str(x)].value\n",
    "                    A_PS_50D_list.append(A_PS_50D)\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_datelist)\n",
    "                    B_PS_50D_datelist.append(parse(B_date)) \n",
    "                    B_PS_50D = FT02wb['AR'+str(x)].value\n",
    "                    B_PS_50D_list.append(B_PS_50D)\n",
    "             \n",
    "    if Smpl_day_index[y] == Smpl_day_index[-1]:\n",
    "        for x in range(Smpl_day_index[y], smpl_end_index):\n",
    "            if FT02wb['D'+str(x)].value != None:    \n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'A':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'A',(FT02wb['D'+str(x)].value))\n",
    "                    A_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(A_date) \n",
    "                    A_PS_50D_datelist.append(parse(A_date))\n",
    "                    A_PS_50D = FT02wb['AR'+str(x)].value\n",
    "                    A_PS_50D_list.append(A_PS_50D)\n",
    "\n",
    "                if filter_detrmin(str(FT02wb['C'+str(x)].value)) == 'B':\n",
    "                    #print(FT02wb['B'+str(Smpl_day_index[y])].value, 'B',(FT02wb['D'+str(x)].value))\n",
    "                    B_date = str((currentmonthyr)-datetime.timedelta(1)+datetime.timedelta(FT02wb['B'+str(Smpl_day_index[y])].value))\\\n",
    "                    +' '+time_cleaner(str(FT02wb['D'+str(x)].value))\n",
    "                    #print(B_date)\n",
    "                    B_PS_50D_datelist.append(parse(B_date))\n",
    "                    B_PS_50D = FT02wb['AR'+str(x)].value\n",
    "                    B_PS_50D_list.append(B_PS_50D)\n",
    "#this part creates the DataFrame after creating a list pair\n",
    "\n",
    "A_PS_50D_list_pair = list(zip(A_PS_50D_list, A_PS_50D_datelist[0:]))\n",
    "A_PS_50D_df = pd.DataFrame(A_PS_50D_list_pair)\n",
    "A_PS_50D_df.columns = ['MS_A_PS_50D', 'DATETIME'] ##check if list title is updated\n",
    "A_PS_50D_df.set_index('DATETIME', inplace=True)\n",
    "A_PS_50D_df = A_PS_50D_df.loc[~A_PS_50D_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "\n",
    "B_PS_50D_list_pair = list(zip(B_PS_50D_list, B_PS_50D_datelist[0:]))\n",
    "B_PS_50D_df = pd.DataFrame(B_PS_50D_list_pair)\n",
    "B_PS_50D_df.columns = ['MS_B_PS_50D', 'DATETIME'] ##check if list title is updated\n",
    "B_PS_50D_df.set_index('DATETIME', inplace=True)\n",
    "B_PS_50D_df = B_PS_50D_df.loc[~B_PS_50D_df.index.duplicated(keep = 'last')]\n",
    "\n",
    "#-----End for PS_50D code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pressure Filter A lab results in merged dataframe\n",
    "\n",
    "df_A_Mg_PS50 = A_PS_50D_df.join(A_Mg_df, how='outer')\n",
    "df_A_merged_lab_res = df_A_Mg_PS50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 2020-07-13 09:39:11.599476\n"
     ]
    }
   ],
   "source": [
    "#Pressure Filter B lab results in merged DF\n",
    "\n",
    "df_B_Mg_PS50 = B_PS_50D_df.join(B_Mg_df, how='outer')\n",
    "df_B_merged_lab_res = df_B_Mg_PS50\n",
    "\n",
    "#deletes the rows whose index has duplicates but keeps the last value\n",
    "df_A_merged_lab_res = df_A_merged_lab_res.loc[~df_A_merged_lab_res.index.duplicated(keep='last')]\n",
    "df_B_merged_lab_res = df_B_merged_lab_res.loc[~df_B_merged_lab_res.index.duplicated(keep='last')]\n",
    "\n",
    "print('Done', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MS_B_PS_50D</th>\n",
       "      <th>MS_B_Mg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATETIME</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-01-01 12:19:00</td>\n",
       "      <td>79.3725</td>\n",
       "      <td>0.0130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 21:26:00</td>\n",
       "      <td>79.5373</td>\n",
       "      <td>0.0095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 05:50:00</td>\n",
       "      <td>82.5196</td>\n",
       "      <td>0.0120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-02 10:30:00</td>\n",
       "      <td>84.4705</td>\n",
       "      <td>0.0131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-02 21:40:00</td>\n",
       "      <td>82.9174</td>\n",
       "      <td>0.0089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-29 05:33:00</td>\n",
       "      <td>83.4861</td>\n",
       "      <td>0.0059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-30 14:02:00</td>\n",
       "      <td>79.3649</td>\n",
       "      <td>0.0077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-30 20:05:00</td>\n",
       "      <td>75.5565</td>\n",
       "      <td>0.0081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-30 06:00:00</td>\n",
       "      <td>73.3627</td>\n",
       "      <td>0.0086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-31 21:42:00</td>\n",
       "      <td>77.4201</td>\n",
       "      <td>0.0099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     MS_B_PS_50D  MS_B_Mg\n",
       "DATETIME                                 \n",
       "2020-01-01 12:19:00      79.3725   0.0130\n",
       "2020-01-01 21:26:00      79.5373   0.0095\n",
       "2020-01-01 05:50:00      82.5196   0.0120\n",
       "2020-01-02 10:30:00      84.4705   0.0131\n",
       "2020-01-02 21:40:00      82.9174   0.0089\n",
       "...                          ...      ...\n",
       "2020-01-29 05:33:00      83.4861   0.0059\n",
       "2020-01-30 14:02:00      79.3649   0.0077\n",
       "2020-01-30 20:05:00      75.5565   0.0081\n",
       "2020-01-30 06:00:00      73.3627   0.0086\n",
       "2020-01-31 21:42:00      77.4201   0.0099\n",
       "\n",
       "[101 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_B_merged_lab_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMBINE PiDATA WITH LABORATORY RESULTS FOR ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndfEL5.set_index('DATETIME', inplace=True)\\ndf_Cycle_LabRes = dfn7.join(dfEL5, how='outer')\\ndf_ffillna = df_Cycle_LabRes.fillna(method='ffill')\\ndf_Cycle_LabRes['Cycle_Count'] = np.arange(1, len(df_Cycle_LabRes['A_FILT_T'])+1)\\ndf_Cycle_LabRes_2=df_Cycle_LabRes.reindex(columns= ['Cycle_Count','A_PU26A_CUR', 'A_PU27A_CUR', 'A_N2DRY', 'A_CWP_1', 'A_CWP_2',\\n       'A_FILT_T', 'A_MANF_P', 'A_CWSH1', 'A_CWSH2', 'A_PRESS1_t',\\n       'A_PRESS2_t', 'A_PU26A_OP', 'A_PU27A_OP', 'newdt', 'startdate',\\n       'enddate', 'MS_A_Pb_spcl', 'MS_A_Zn_spcl', 'MS_A_Cu_spcl',\\n       'MS_A_Ni_spcl', 'MS_A_Co_spcl', 'MS_A_Fe_spcl', 'MS_A_Mn_spcl',\\n       'MS_A_Cr_spcl', 'MS_A_Ca_spcl', 'MS_A_Si_spcl', 'MS_A_Al_spcl',\\n       'MS_A_Mg_spcl', 'MS_A_S_spcl', 'MS_A_moisture_spcl', 'MS_A_PS_10D_spcl',\\n       'MS_A_PS_50D_spcl', 'MS_A_PS_90D_spcl', 'MS_A_PS_mean_v_spcl',\\n       'MS_A_PS_stdev_spcl'])\\ndf_Cycle_LabRes_2_ffillna = df_Cycle_LabRes_2.fillna(method='ffill')\\ndf_JAN2020_cycLab_A= df_Cycle_LabRes_2_ffillna\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fcn for formatting the dates from range str\n",
    "def datesplitter_start(x):\n",
    "    return parse(x.split('to')[0])\n",
    "def datesplitter_end(x):\n",
    "    return parse(x.split('to')[1])\n",
    "#---------\n",
    "df_PressureFilter_Param['newdt'] = df_PressureFilter_Param.index\n",
    "df_PressureFilter_Param['startdate'] = df_PressureFilter_Param['newdt'].apply(lambda x: datesplitter_start(x))\n",
    "df_PressureFilter_Param['enddate'] = df_PressureFilter_Param['newdt'].apply(lambda x: datesplitter_end(x))\n",
    "\n",
    "#code for changing the index to Datetime in range format (from date1 to date2)\n",
    "df_anl_indexList = list()\n",
    "df_anl_indexList[:] = []\n",
    "df_n7_indexList = list()\n",
    "df_n7_indexList[:] = []\n",
    "try:\n",
    "    for i in range(0, len(df_A_merged_lab_res)):\n",
    "        for j in range(0, len(dfn7)):\n",
    "            if dfn7['enddate'][j-1] <= df_A_merged_lab_res.index[i] and dfn7['enddate'][j] >= df_A_merged_lab_res.index[i]:\n",
    "                df_n7_indexList.append(dfn7.index[j])\n",
    "                df_anl_indexList.append(df_A_merged_lab_res.index[i]) \n",
    "except IndexError:\n",
    "    print('IndexError')  \n",
    "Analys_indexList_Pair = list(zip(df_anl_indexList, df_n7_indexList[0:]))\n",
    "dfEL4 = pd.DataFrame(Analys_indexList_Pair)\n",
    "dfEL4.columns = ['DATETIME', 'FT_cycletimerange']\n",
    "dfEL4.set_index('DATETIME', inplace=True)\n",
    "\n",
    "dfEL5 = df_A_merged_lab_res.join(dfEL4, how='outer')\n",
    "#dfEL5.columns = ['MS_A_Pb_spcl', 'MS_A_Zn_spcl', 'MS_A_Cu_spcl', 'MS_A_Ni_spcl', 'MS_A_Co_spcl', 'MS_A_Fe_spcl',\n",
    "#       'MS_A_Mn_spcl', 'MS_A_Cr_spcl', 'MS_A_Ca_spcl', 'MS_A_Si_spcl', 'MS_A_Al_spcl', 'MS_A_Mg_spcl','MS_A_S_spcl', 'MS_A_moisture_spcl', 'MS_A_PS_10D_spcl', 'MS_A_PS_50D_spcl', 'MS_A_PS_90D_spcl',\n",
    "#       'MS_A_PS_mean_v_spcl', 'MS_A_PS_stdev_spcl','DATETIME']\n",
    "\n",
    "\"\"\"\n",
    "dfEL5.set_index('DATETIME', inplace=True)\n",
    "df_Cycle_LabRes = dfn7.join(dfEL5, how='outer')\n",
    "df_ffillna = df_Cycle_LabRes.fillna(method='ffill')\n",
    "df_Cycle_LabRes['Cycle_Count'] = np.arange(1, len(df_Cycle_LabRes['A_FILT_T'])+1)\n",
    "df_Cycle_LabRes_2=df_Cycle_LabRes.reindex(columns= ['Cycle_Count','A_PU26A_CUR', 'A_PU27A_CUR', 'A_N2DRY', 'A_CWP_1', 'A_CWP_2',\n",
    "       'A_FILT_T', 'A_MANF_P', 'A_CWSH1', 'A_CWSH2', 'A_PRESS1_t',\n",
    "       'A_PRESS2_t', 'A_PU26A_OP', 'A_PU27A_OP', 'newdt', 'startdate',\n",
    "       'enddate', 'MS_A_Pb_spcl', 'MS_A_Zn_spcl', 'MS_A_Cu_spcl',\n",
    "       'MS_A_Ni_spcl', 'MS_A_Co_spcl', 'MS_A_Fe_spcl', 'MS_A_Mn_spcl',\n",
    "       'MS_A_Cr_spcl', 'MS_A_Ca_spcl', 'MS_A_Si_spcl', 'MS_A_Al_spcl',\n",
    "       'MS_A_Mg_spcl', 'MS_A_S_spcl', 'MS_A_moisture_spcl', 'MS_A_PS_10D_spcl',\n",
    "       'MS_A_PS_50D_spcl', 'MS_A_PS_90D_spcl', 'MS_A_PS_mean_v_spcl',\n",
    "       'MS_A_PS_stdev_spcl'])\n",
    "df_Cycle_LabRes_2_ffillna = df_Cycle_LabRes_2.fillna(method='ffill')\n",
    "df_JAN2020_cycLab_A= df_Cycle_LabRes_2_ffillna\n",
    "\"\"\"\n",
    "#---------------dfEL5 is the is the analysis results data indexed by daterange format. readt for merging with dfn11(FT02A data from Pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MS_A_PS_50D</th>\n",
       "      <th>MS_A_Mg</th>\n",
       "      <th>FT_cycletimerange</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATETIME</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-01-01 02:58:00</td>\n",
       "      <td>82.9663</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>2020-01-01 02:56:00 to 2020-01-01 03:27:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 10:52:00</td>\n",
       "      <td>83.8041</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>2020-01-01 13:55:00 to 2020-01-01 14:32:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 22:00:00</td>\n",
       "      <td>81.3093</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>2020-01-02 06:45:00 to 2020-01-02 07:22:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-02 04:40:00</td>\n",
       "      <td>82.5332</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>2020-01-02 06:45:00 to 2020-01-02 07:22:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-02 09:08:00</td>\n",
       "      <td>87.7388</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>2020-01-03 03:51:00 to 2020-01-03 04:28:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-30 06:19:00</td>\n",
       "      <td>74.4257</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-30 13:42:00</td>\n",
       "      <td>81.2719</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-30 21:53:00</td>\n",
       "      <td>77.7506</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-31 11:05:00</td>\n",
       "      <td>80.1904</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-31 20:12:00</td>\n",
       "      <td>74.9485</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     MS_A_PS_50D  MS_A_Mg  \\\n",
       "DATETIME                                    \n",
       "2020-01-01 02:58:00      82.9663   0.0125   \n",
       "2020-01-01 10:52:00      83.8041   0.0155   \n",
       "2020-01-01 22:00:00      81.3093   0.0124   \n",
       "2020-01-02 04:40:00      82.5332   0.0129   \n",
       "2020-01-02 09:08:00      87.7388   0.0393   \n",
       "...                          ...      ...   \n",
       "2020-01-30 06:19:00      74.4257   0.0118   \n",
       "2020-01-30 13:42:00      81.2719   0.0114   \n",
       "2020-01-30 21:53:00      77.7506   0.0079   \n",
       "2020-01-31 11:05:00      80.1904   0.0088   \n",
       "2020-01-31 20:12:00      74.9485   0.0143   \n",
       "\n",
       "                                              FT_cycletimerange  \n",
       "DATETIME                                                         \n",
       "2020-01-01 02:58:00  2020-01-01 02:56:00 to 2020-01-01 03:27:00  \n",
       "2020-01-01 10:52:00  2020-01-01 13:55:00 to 2020-01-01 14:32:00  \n",
       "2020-01-01 22:00:00  2020-01-02 06:45:00 to 2020-01-02 07:22:00  \n",
       "2020-01-02 04:40:00  2020-01-02 06:45:00 to 2020-01-02 07:22:00  \n",
       "2020-01-02 09:08:00  2020-01-03 03:51:00 to 2020-01-03 04:28:00  \n",
       "...                                                         ...  \n",
       "2020-01-30 06:19:00                                         NaN  \n",
       "2020-01-30 13:42:00                                         NaN  \n",
       "2020-01-30 21:53:00                                         NaN  \n",
       "2020-01-31 11:05:00                                         NaN  \n",
       "2020-01-31 20:12:00                                         NaN  \n",
       "\n",
       "[104 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfEL5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMBINE PiDATA WITH LABORATORY RESULTS FOR BRAVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fcn for formatting the dates from range str\n",
    "def datesplitter_start(x):\n",
    "    return parse(x.split('to')[0])\n",
    "def datesplitter_end(x):\n",
    "    return parse(x.split('to')[1])\n",
    "#---------\n",
    "df_PressureFilter_Param_B['newdt'] = df_PressureFilter_Param_B.index\n",
    "df_PressureFilter_Param_B['startdate'] = df_PressureFilter_Param_B['newdt'].apply(lambda x: datesplitter_start(x))\n",
    "df_PressureFilter_Param_B['enddate'] = df_PressureFilter_Param_B['newdt'].apply(lambda x: datesplitter_end(x))\n",
    "\n",
    "#code for changing the index to Datetime in range format (from date1 to date2)\n",
    "df_anlB_indexList = list()\n",
    "df_anlB_indexList[:] = []\n",
    "df_n11B_indexList = list()\n",
    "df_n11B_indexList[:] = []\n",
    "try:\n",
    "    for i in range(0, len(df_B_merged_lab_res)):\n",
    "        for j in range(0, len(dfn11B)):\n",
    "            if dfn11B['enddate'][j-1] <= df_B_merged_lab_res.index[i] and dfn11B['enddate'][j] >= df_B_merged_lab_res.index[i]:\n",
    "                df_n11B_indexList.append(dfn11B.index[j])\n",
    "                df_anlB_indexList.append(df_B_merged_lab_res.index[i]) \n",
    "except IndexError:\n",
    "    print('IndexError')  \n",
    "Analys_indexList_Pair = list(zip(df_anlB_indexList, df_n11B_indexList[0:]))\n",
    "dfEL4B = pd.DataFrame(Analys_indexList_Pair)\n",
    "dfEL4B.columns = ['DATETIME', 'FT_cycletimerange']\n",
    "dfEL4B.set_index('DATETIME', inplace=True)\n",
    "\n",
    "dfEL5B = df_B_merged_lab_res.join(dfEL4B, how='outer')\n",
    "dfEL5B.columns = ['MS_B_Pb_spcl', 'MS_B_Zn_spcl', 'MS_B_Cu_spcl', 'MS_B_Ni_spcl', 'MS_B_Co_spcl', 'MS_B_Fe_spcl',\n",
    "       'MS_B_Mn_spcl', 'MS_B_Cr_spcl', 'MS_B_Ca_spcl', 'MS_B_Si_spcl', 'MS_B_Al_spcl', 'MS_B_Mg_spcl','MS_B_S_spcl', 'MS_B_moisture_spcl', 'MS_B_PS_10D_spcl', 'MS_B_PS_50D_spcl', 'MS_B_PS_90D_spcl',\n",
    "       'MS_B_PS_mean_v_spcl', 'MS_B_PS_stdev_spcl','DATETIME']\n",
    "\n",
    "dfEL5B.set_index('DATETIME', inplace=True)\n",
    "df_Cycle_LabResB = dfn11B.join(dfEL5B, how='outer')\n",
    "df_ffillnaB = df_Cycle_LabResB.fillna(method='ffill')\n",
    "df_Cycle_LabResB['Cycle_Count'] = np.arange(1, len(df_Cycle_LabResB['B_FILT_T'])+1)\n",
    "df_Cycle_LabRes_2B=df_Cycle_LabResB.reindex(columns= ['Cycle_Count','B_PU26B_CUR', 'B_PU27B_CUR', 'B_N2DRY', 'B_CWP_1', 'B_CWP_2',\n",
    "       'B_FILT_T', 'B_MANF_P', 'B_CWSH1', 'B_CWSH2', 'B_PRESS1_t',\n",
    "       'B_PRESS2_t', 'B_PU26B_OP', 'B_PU27B_OP', 'newdt', 'startdate',\n",
    "       'enddate', 'MS_B_Pb_spcl', 'MS_B_Zn_spcl', 'MS_B_Cu_spcl',\n",
    "       'MS_B_Ni_spcl', 'MS_B_Co_spcl', 'MS_B_Fe_spcl', 'MS_B_Mn_spcl',\n",
    "       'MS_B_Cr_spcl', 'MS_B_Ca_spcl', 'MS_B_Si_spcl', 'MS_B_Al_spcl',\n",
    "       'MS_B_Mg_spcl', 'MS_B_S_spcl', 'MS_B_moisture_spcl', 'MS_B_PS_10D_spcl',\n",
    "       'MS_B_PS_50D_spcl', 'MS_B_PS_90D_spcl', 'MS_B_PS_mean_v_spcl',\n",
    "       'MS_B_PS_stdev_spcl'])\n",
    "df_Cycle_LabRes_2B_ffillna = df_Cycle_LabRes_2B.fillna(method='ffill')\n",
    "df_JAN2020_cycLab_B = df_Cycle_LabRes_2B_ffillna\n",
    "\n",
    "#---------------dfEL5 is the is the analysis results data indexed by daterange format. readt for merging with dfn11(FT02A data from Pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE THE DATAFRAME TO EXCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_JAN2020_cycLab_A.to_excel(r'C:\\Users\\v.t.flores\\Documents\\106FT02A_data_JAN2020.xlsx')\n",
    "df_JAN2020_cycLab_A.to_excel(r'\\\\THGOBNA002\\thpal\\05 Production\\150 DCS\\00 DCS Monitoring & Records\\00 Active Files\\2020\\Pressure_Filter_Data_2019\\106FT02A_data_JAN2020.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_JAN2020_cycLab_B.to_excel(r'C:\\Users\\v.t.flores\\Documents\\106FT02B_data_JAN2020.xlsx')\n",
    "df_JAN2020_cycLab_B.to_excel(r'\\\\THGOBNA002\\thpal\\05 Production\\150 DCS\\00 DCS Monitoring & Records\\00 Active Files\\2020\\Pressure_Filter_Data_2019\\106FT02B_data_JAN2020.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
